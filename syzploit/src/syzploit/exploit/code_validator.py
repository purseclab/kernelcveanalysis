"""
exploit.code_validator — Static analysis of LLM-generated exploit C code.

Catches common LLM failure patterns BEFORE compilation:
  - Stub functions (return 0 / empty body / (void)arg)
  - "Thinking out loud" comments (alternative approaches, TODO blocks)
  - Multiple competing exploit strategies in one file
  - Invalid kernel headers that won't exist in NDK
  - Functions declared but never implemented
  - Excessive comment-to-code ratio
  - Missing intermediate validation checks

These are GENERIC patterns — they apply to any exploit type
(binder, io_uring, pipe, msg_msg, etc.), not just a specific CVE.
"""

from __future__ import annotations

import re
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional, Tuple

from ..core.log import console


class Severity(Enum):
    """Validation issue severity."""
    ERROR = "error"         # Must fix — exploit will definitely not work
    WARNING = "warning"     # Likely broken — should fix before compiling
    INFO = "info"           # Suspicious but might be intentional


@dataclass
class ValidationIssue:
    """A single issue found in the generated code."""
    severity: Severity
    category: str           # e.g. "stub_function", "invalid_header"
    message: str            # Human-readable description
    line_number: int = 0    # Approximate line in the source
    suggestion: str = ""    # What the LLM should do instead


@dataclass
class ValidationResult:
    """Aggregate result of code validation."""
    issues: List[ValidationIssue] = field(default_factory=list)
    code_lines: int = 0
    comment_lines: int = 0
    blank_lines: int = 0
    function_count: int = 0
    stub_count: int = 0

    @property
    def has_errors(self) -> bool:
        return any(i.severity == Severity.ERROR for i in self.issues)

    @property
    def has_warnings(self) -> bool:
        return any(i.severity == Severity.WARNING for i in self.issues)

    @property
    def error_count(self) -> int:
        return sum(1 for i in self.issues if i.severity == Severity.ERROR)

    @property
    def warning_count(self) -> int:
        return sum(1 for i in self.issues if i.severity == Severity.WARNING)

    @property
    def comment_ratio(self) -> float:
        """Fraction of non-blank lines that are comments."""
        total = self.code_lines + self.comment_lines
        if total == 0:
            return 0.0
        return self.comment_lines / total

    def format_for_llm(self) -> str:
        """Format issues as structured feedback for LLM regeneration."""
        if not self.issues:
            return ""

        lines = [
            "=== CODE QUALITY ISSUES DETECTED ===",
            f"Errors: {self.error_count}, Warnings: {self.warning_count}",
            f"Comment ratio: {self.comment_ratio:.0%} "
            f"({self.comment_lines} comment lines / "
            f"{self.code_lines + self.comment_lines} total non-blank)",
            "",
            "FIX ALL of these issues in the regenerated code:",
            "",
        ]

        for i, issue in enumerate(self.issues, 1):
            prefix = "❌" if issue.severity == Severity.ERROR else "⚠️"
            lines.append(f"{prefix} {i}. [{issue.category}] {issue.message}")
            if issue.line_number:
                lines.append(f"   (around line {issue.line_number})")
            if issue.suggestion:
                lines.append(f"   → Fix: {issue.suggestion}")
            lines.append("")

        return "\n".join(lines)

    def summary(self) -> str:
        """One-line summary for console output."""
        return (
            f"Validation: {self.error_count} errors, "
            f"{self.warning_count} warnings, "
            f"comment ratio {self.comment_ratio:.0%}, "
            f"{self.stub_count} stubs"
        )


# ═════════════════════════════════════════════════════════════════════
# Individual check functions
# ═════════════════════════════════════════════════════════════════════

# Headers that exist in the kernel tree but NOT in the NDK sysroot.
_INVALID_NDK_HEADERS = re.compile(
    r'#\s*include\s*<linux/'
    r'(?:binder|io_uring|sched|cred|mm_types|fs|namei|dcache|'
    r'pipe_fs_i|msg|ipc|key|security|selinux|audit|ptrace|elf|'
    r'module|kprobes|ftrace|bpf|perf_event).*\.h>'
)

# Patterns indicating "thinking out loud" comment blocks
_THINKING_PATTERNS = [
    re.compile(r'(?:alternatively|alternative approach)', re.IGNORECASE),
    re.compile(r"let'?s try (?:a |another )", re.IGNORECASE),
    re.compile(r'we (?:could|might|should|can) (?:also |instead )', re.IGNORECASE),
    re.compile(r'(?:for simplicity|for reliability|to be safe)', re.IGNORECASE),
    re.compile(r'(?:option \d|approach \d|strategy \d|method \d)', re.IGNORECASE),
    re.compile(r'(?:more direct|simpler|easier) approach', re.IGNORECASE),
    re.compile(r'since (?:the |this |we )', re.IGNORECASE),
    re.compile(r'(?:instead of|rather than) (?:using|implementing)', re.IGNORECASE),
    re.compile(r'(?:NOTE|IMPORTANT|CAVEAT|WARNING):', re.IGNORECASE),
    re.compile(r'the (?:idea|concept|theory|principle) (?:is|here)', re.IGNORECASE),
]

# Pattern for competing strategy markers
_STRATEGY_MARKERS = re.compile(
    r'(?:attempt|strategy|approach|method|technique)\s*'
    r'(?:#?\d+|one|two|three|1|2|3)',
    re.IGNORECASE,
)


def _count_lines(code: str) -> Tuple[int, int, int]:
    """Count code, comment, and blank lines. Returns (code, comment, blank)."""
    code_count = 0
    comment_count = 0
    blank_count = 0
    in_block_comment = False

    for line in code.split("\n"):
        stripped = line.strip()

        if not stripped:
            blank_count += 1
            continue

        if in_block_comment:
            comment_count += 1
            if "*/" in stripped:
                in_block_comment = False
            continue

        if stripped.startswith("/*"):
            comment_count += 1
            if "*/" not in stripped:
                in_block_comment = True
            continue

        if stripped.startswith("//"):
            comment_count += 1
            continue

        code_count += 1

    return code_count, comment_count, blank_count


def _find_stub_functions(code: str) -> List[ValidationIssue]:
    """
    Detect stub functions — functions with trivially empty bodies.

    Patterns caught:
      - { return 0; }
      - { (void)arg; return 0; }
      - { /* TODO */ return -1; }
      - { } (empty body)
    """
    issues = []

    # Match function definitions and extract their bodies
    # This regex captures: return_type func_name(params) { body }
    func_pattern = re.compile(
        r'(?:static\s+)?'                      # optional static
        r'(?:inline\s+)?'                       # optional inline
        r'(?:unsigned\s+)?'                     # optional unsigned
        r'(?:long|int|void|uint64_t|uint32_t|char\s*\*|ssize_t|size_t)\s+'  # return type
        r'(\w+)\s*'                             # function name (captured)
        r'\([^)]*\)\s*\{'                       # params + opening brace
    )

    lines = code.split("\n")
    for i, line in enumerate(lines):
        m = func_pattern.search(line)
        if not m:
            continue

        func_name = m.group(1)

        # Skip main, check_uid_before/after, and ipc helpers
        if func_name in ("main", "check_uid_before", "check_uid_after",
                         "ipc_send", "ipc_wait"):
            continue

        # Extract the function body (find matching closing brace)
        brace_depth = 0
        body_lines = []
        started = False
        first_brace_line = True
        for j in range(i, min(i + 200, len(lines))):
            for ch in lines[j]:
                if ch == "{":
                    brace_depth += 1
                    started = True
                elif ch == "}":
                    brace_depth -= 1
            if started:
                if first_brace_line:
                    # Skip the function signature line — only keep
                    # text after the opening brace
                    brace_pos = lines[j].find("{")
                    after_brace = lines[j][brace_pos + 1:] if brace_pos >= 0 else ""
                    if after_brace.strip():
                        body_lines.append(after_brace)
                    first_brace_line = False
                else:
                    body_lines.append(lines[j])
            if started and brace_depth == 0:
                break

        if not body_lines:
            continue

        # Analyze the body
        body = "\n".join(body_lines)
        # Strip the enclosing braces and comments
        body_stripped = re.sub(r'/\*.*?\*/', '', body, flags=re.DOTALL)
        body_stripped = re.sub(r'//.*$', '', body_stripped, flags=re.MULTILINE)
        body_stripped = re.sub(r'[{}]', '', body_stripped).strip()

        # Remove (void)arg; casts
        body_stripped = re.sub(r'\(void\)\w+\s*;', '', body_stripped).strip()

        # Check if it's a stub
        is_stub = False
        if not body_stripped:
            is_stub = True
        elif body_stripped in ("return 0;", "return -1;", "return;",
                               "return NULL;", "return 0"):
            is_stub = True
        elif re.match(r'^return\s+0\s*;?\s*$', body_stripped):
            is_stub = True

        if is_stub:
            issues.append(ValidationIssue(
                severity=Severity.ERROR,
                category="stub_function",
                message=(
                    f"Function '{func_name}()' is a stub — it has no real "
                    f"implementation (body is just '{body_stripped or '<empty>'}')."
                ),
                line_number=i + 1,
                suggestion=(
                    f"Implement '{func_name}()' fully. If it's a R/W primitive, "
                    f"it must use the actual kernel exploit mechanism (e.g. "
                    f"corrupted object fields, ioctl, pipe read/write) to "
                    f"read/write kernel memory."
                ),
            ))

    return issues


def _check_comment_ratio(code: str, code_lines: int, comment_lines: int) -> List[ValidationIssue]:
    """Flag excessive comment-to-code ratio (>40% = likely 'thinking out loud')."""
    issues = []
    total = code_lines + comment_lines
    if total == 0:
        return issues

    ratio = comment_lines / total
    if ratio > 0.50:
        issues.append(ValidationIssue(
            severity=Severity.ERROR,
            category="excessive_comments",
            message=(
                f"Code is {ratio:.0%} comments ({comment_lines} comment lines "
                f"vs {code_lines} code lines). This usually means the LLM is "
                f"'thinking out loud' instead of writing working code."
            ),
            suggestion=(
                "Remove all comments that discuss alternative approaches, "
                "explain design decisions, or describe what 'could' be done. "
                "Keep only brief inline comments that explain non-obvious code."
            ),
        ))
    elif ratio > 0.35:
        issues.append(ValidationIssue(
            severity=Severity.WARNING,
            category="high_comment_ratio",
            message=(
                f"Code has a high comment ratio ({ratio:.0%}). "
                f"Ensure comments describe what the code DOES, not "
                f"alternatives or design discussions."
            ),
            suggestion="Remove comments about alternative approaches.",
        ))

    return issues


def _check_thinking_comments(code: str) -> List[ValidationIssue]:
    """Detect 'thinking out loud' comment blocks."""
    issues = []
    lines = code.split("\n")
    thinking_count = 0
    thinking_examples = []

    for i, line in enumerate(lines):
        stripped = line.strip()
        if not (stripped.startswith("//") or stripped.startswith("*") or
                stripped.startswith("/*")):
            continue

        for pattern in _THINKING_PATTERNS:
            if pattern.search(stripped):
                thinking_count += 1
                if len(thinking_examples) < 3:
                    # Truncate long comments
                    example = stripped[:100].rstrip()
                    thinking_examples.append(f"  line {i+1}: {example}")
                break

    if thinking_count >= 5:
        issues.append(ValidationIssue(
            severity=Severity.ERROR,
            category="thinking_out_loud",
            message=(
                f"Found {thinking_count} 'thinking out loud' comments that "
                f"discuss alternatives instead of implementing code. Examples:\n"
                + "\n".join(thinking_examples)
            ),
            suggestion=(
                "Delete ALL comments that discuss alternatives, explain what "
                "'could' be done, or compare approaches. The exploit must "
                "commit to ONE approach and implement it completely."
            ),
        ))
    elif thinking_count >= 3:
        issues.append(ValidationIssue(
            severity=Severity.WARNING,
            category="thinking_out_loud",
            message=(
                f"Found {thinking_count} comments discussing alternatives. "
                f"Exploit should commit to one approach."
            ),
            suggestion="Remove comments about alternative approaches.",
        ))

    return issues


def _check_invalid_headers(code: str) -> List[ValidationIssue]:
    """Detect #include directives for kernel headers not in NDK."""
    issues = []
    for i, line in enumerate(code.split("\n"), 1):
        if _INVALID_NDK_HEADERS.search(line):
            header = line.strip()
            issues.append(ValidationIssue(
                severity=Severity.ERROR,
                category="invalid_header",
                message=(
                    f"Invalid header for NDK compilation: '{header}'. "
                    f"Kernel internal headers don't exist in the NDK sysroot."
                ),
                line_number=i,
                suggestion=(
                    "Define the needed kernel structs, constants, and ioctl "
                    "numbers INLINE in the source file instead of including "
                    "kernel headers. Use _IO/_IOR/_IOW/_IOWR macros from "
                    "<sys/ioctl.h> for ioctl numbers."
                ),
            ))

    return issues


def _check_competing_strategies(code: str) -> List[ValidationIssue]:
    """Detect multiple competing exploit strategies in the same file."""
    issues = []
    matches = _STRATEGY_MARKERS.findall(code)

    if len(matches) >= 3:
        issues.append(ValidationIssue(
            severity=Severity.ERROR,
            category="multiple_strategies",
            message=(
                f"Found {len(matches)} references to numbered strategies/approaches: "
                f"{', '.join(m.strip() for m in matches[:5])}. "
                f"The exploit should implement ONE strategy, not multiple."
            ),
            suggestion=(
                "Choose the BEST exploitation strategy based on the "
                "vulnerability details and implement it fully. Remove all "
                "code for alternative strategies."
            ),
        ))

    return issues


def _check_placeholder_markers(code: str) -> List[ValidationIssue]:
    """Detect TODO, FIXME, PLACEHOLDER, and similar markers."""
    issues = []
    placeholder_re = re.compile(
        r'\b(TODO|FIXME|PLACEHOLDER|HACK|XXX|STUB|NOT\s+IMPLEMENTED)\b',
        re.IGNORECASE,
    )

    count = 0
    examples = []
    for i, line in enumerate(code.split("\n"), 1):
        m = placeholder_re.search(line)
        if m:
            count += 1
            if len(examples) < 3:
                examples.append(f"  line {i}: {line.strip()[:80]}")

    if count >= 1:
        severity = Severity.ERROR if count >= 3 else Severity.WARNING
        issues.append(ValidationIssue(
            severity=severity,
            category="placeholder_markers",
            message=(
                f"Found {count} placeholder markers (TODO/FIXME/etc.):\n"
                + "\n".join(examples)
            ),
            suggestion=(
                "Replace ALL TODO/FIXME markers with actual implementation. "
                "Every function and code path must be complete."
            ),
        ))

    return issues


def _check_missing_validation(code: str) -> List[ValidationIssue]:
    """
    Check that the exploit validates intermediate steps.

    Real exploits check: trigger worked, reclaim succeeded, leak is valid,
    R/W primitive returns sensible values, cred overwrite succeeded.
    At minimum, exploit should have validation printf + conditional checks.
    """
    issues = []

    # Check for common exploit phases and whether they have validation
    phase_patterns = [
        ("vulnerability trigger", r'(?:trigger|vuln|exploit|bug)', r'(?:trigger|vuln).*(?:fail|error|success|ok|done)'),
        ("heap spray/reclaim", r'(?:spray|reclaim|alloc|slab)', r'(?:spray|reclaim).*(?:fail|success|done|count)'),
        ("R/W primitive", r'(?:read64|write64|kread|kwrite|arb.*read|arb.*write)', r'(?:read|write).*(?:0x|fail|invalid|success)'),
    ]

    validation_count = 0
    for phase_name, impl_re, check_re in phase_patterns:
        has_impl = bool(re.search(impl_re, code, re.IGNORECASE))
        has_check = bool(re.search(check_re, code, re.IGNORECASE))
        if has_impl and has_check:
            validation_count += 1

    # Also count explicit validation patterns
    explicit_checks = len(re.findall(
        r'(?:printf|fprintf).*(?:\[-\]|\[\+\]|fail|success|error|done|ok)',
        code, re.IGNORECASE
    ))

    if explicit_checks < 3:
        issues.append(ValidationIssue(
            severity=Severity.WARNING,
            category="missing_validation",
            message=(
                f"Only found {explicit_checks} validation/status prints. "
                f"Real exploits print status at each phase: trigger done, "
                f"spray count, leak value, R/W test, cred overwrite result."
            ),
            suggestion=(
                "Add printf() status messages after each exploit phase: "
                "'[+] Trigger succeeded', '[+] Spray: allocated N objects', "
                "'[+] Leak: kernel base = 0x%lx', '[+] R/W primitive ready', "
                "'[+] Cred overwrite done'. Also add error checks with "
                "'[-] Phase failed' messages and appropriate error handling."
            ),
        ))

    return issues


def _check_wrong_cve(code: str, target_cve: Optional[str] = None) -> List[ValidationIssue]:
    """
    Check if the code references a DIFFERENT CVE than the target.

    A common LLM failure: the model confuses similar CVEs and implements
    the wrong vulnerability (e.g. CVE-2019-2215 instead of CVE-2023-20938).
    """
    issues = []
    if not target_cve:
        return issues

    # Find all CVE references in the code
    cve_refs = set(re.findall(r'CVE-\d{4}-\d+', code, re.IGNORECASE))
    target_normalized = target_cve.upper()
    wrong_cves = {c.upper() for c in cve_refs if c.upper() != target_normalized}

    if wrong_cves:
        issues.append(ValidationIssue(
            severity=Severity.ERROR,
            category="wrong_cve",
            message=(
                f"Code references WRONG CVE(s): {', '.join(sorted(wrong_cves))}. "
                f"Target CVE is {target_cve}. The LLM may have confused this "
                f"vulnerability with a different one."
            ),
            suggestion=(
                f"Remove ALL code and references related to {', '.join(sorted(wrong_cves))}. "
                f"Implement ONLY the vulnerability described by {target_cve}."
            ),
        ))

    return issues


def _check_slab_consistency(
    code: str,
    target_slab: Optional[str] = None,
) -> List[ValidationIssue]:
    """
    Check that spray objects target the correct slab cache.

    Common LLM mistake: spraying pipe_buffer (kmalloc-1024) when the
    target object is in kmalloc-128, or using msg_msg for a different
    slab entirely.
    """
    issues = []
    if not target_slab:
        return issues

    # Extract slab size from target (e.g. "kmalloc-128" → 128)
    slab_match = re.search(r'kmalloc-(\d+)', target_slab)
    if not slab_match:
        return issues
    target_size = int(slab_match.group(1))

    # Known spray object sizes
    spray_objects = {
        "pipe_buffer": 1024,
        "msg_msg": 4096,        # variable, but header is large
        "seq_operations": 32,
        "timerfd_ctx": 256,
        "epitem": 128,
        "signalfd_ctx": 128,
        "sk_buff": 256,         # variable
        "setxattr": -1,         # variable, user-controlled
        "add_key": -1,          # variable, user-controlled
        "sendmsg": -1,          # variable via cmsg, can target any slab
    }

    # Check which spray objects are used in the code
    used_sprays = []
    for obj, size in spray_objects.items():
        # Look for references to the spray object
        if re.search(rf'\b{obj}\b', code, re.IGNORECASE):
            used_sprays.append((obj, size))

    for obj, size in used_sprays:
        if size > 0 and size != target_size and abs(size - target_size) > 32:
            issues.append(ValidationIssue(
                severity=Severity.WARNING,
                category="slab_mismatch",
                message=(
                    f"Spray object '{obj}' is typically in kmalloc-{size}, "
                    f"but target slab is {target_slab}. Size mismatch may "
                    f"prevent cross-cache reclaim."
                ),
                suggestion=(
                    f"Use a spray object that targets {target_slab}. "
                    f"For kmalloc-128: epitem (epoll_ctl), signalfd_ctx. "
                    f"For kmalloc-256: sk_buff (sendmsg with cmsg). "
                    f"For kmalloc-1024: pipe_buffer. "
                    f"For variable sizes: setxattr, add_key, sendmsg cmsg."
                ),
            ))

    return issues


# ═════════════════════════════════════════════════════════════════════
# Main validation entry point
# ═════════════════════════════════════════════════════════════════════

def validate_exploit_code(
    code: str,
    *,
    target_cve: Optional[str] = None,
    target_slab: Optional[str] = None,
) -> ValidationResult:
    """
    Validate generated exploit C code for common LLM failure patterns.

    Parameters
    ----------
    code : str
        The generated C source code.
    target_cve : str, optional
        The CVE being exploited (e.g. "CVE-2023-20938").
        Used to detect wrong-CVE confusion.
    target_slab : str, optional
        Target slab cache (e.g. "kmalloc-128").
        Used to validate spray object consistency.

    Returns
    -------
    ValidationResult
        Contains all issues found, line counts, and formatting
        methods for LLM feedback.
    """
    code_lines, comment_lines, blank_lines = _count_lines(code)

    result = ValidationResult(
        code_lines=code_lines,
        comment_lines=comment_lines,
        blank_lines=blank_lines,
    )

    # Run all checks
    stub_issues = _find_stub_functions(code)
    result.stub_count = len(stub_issues)
    result.issues.extend(stub_issues)

    result.issues.extend(_check_comment_ratio(code, code_lines, comment_lines))
    result.issues.extend(_check_thinking_comments(code))
    result.issues.extend(_check_invalid_headers(code))
    result.issues.extend(_check_competing_strategies(code))
    result.issues.extend(_check_placeholder_markers(code))
    result.issues.extend(_check_missing_validation(code))
    result.issues.extend(_check_wrong_cve(code, target_cve))
    result.issues.extend(_check_slab_consistency(code, target_slab))

    # Count functions (rough)
    func_defs = re.findall(
        r'(?:static\s+)?(?:inline\s+)?(?:unsigned\s+)?'
        r'(?:long|int|void|uint64_t|uint32_t|char\s*\*|ssize_t|size_t)\s+'
        r'\w+\s*\([^)]*\)\s*\{',
        code,
    )
    result.function_count = len(func_defs)

    # Log summary
    severity_str = "[green]PASS[/]" if not result.has_errors else "[red]FAIL[/]"
    if result.has_warnings and not result.has_errors:
        severity_str = "[yellow]WARN[/]"
    console.print(f"  Code validation: {severity_str} — {result.summary()}")

    return result
