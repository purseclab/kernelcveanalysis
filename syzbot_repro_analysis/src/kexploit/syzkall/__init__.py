# Pulls buts from syzkaller syzbot and filters for interesting looking ones

from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn, MofNCompleteColumn
import csv
import json
import shutil
import os
import re
import sys
import traceback
import time

from .bug_db import SyzkallBugDatabase
from .scrape import pull_bugs
from .run_bug import test_repro_crashes, wait_for_connection, test_repro_crashes_qemu
import json as _json

# Optional: integrate local analysis package if available. Import is non-fatal so
# the harness can run even when the analyzer or its heavy dependencies are not
# installed in the environment.
from .analysis import crash_analyzer as precondition_analyzer  # type: ignore

def pull(kernel_name: str):
    db = SyzkallBugDatabase(kernel_name)

    try:
        pull_bugs(db, kernel_name)
    finally:
        db.close()

def query(kernel_name: str):
    db = SyzkallBugDatabase(kernel_name)
    bugs = db.get_all_bugs()

    for bug in bugs:
        # print(bug.description)
        # print(bug.kernel_name)
        # if 'android' not in bug.kernel_name:
        #     continue
        print(f'{bug.bug_id}: {bug.title} ({bug.crash_time})')


def collect_stats(kernel_name: str, outfile: Path):
    db = SyzkallBugDatabase(kernel_name)
    console = Console()
    table = Table(title="Syzkaller Bug Test Results")
    table.add_column("Bug ID", style="cyan", no_wrap=True)
    table.add_column("Reported Crash", style="magenta")
    table.add_column("Crash Type", style="magenta")
    table.add_column("Syzcall", justify="center")
    table.add_column("File", justify="center")
    table.add_column("Function", justify="center", style="red")
    table.add_column("Line Number", justify="center", style="red")

    bugs = db.get_all_bugs()
    if not bugs:
        console.print("[red]No bugs found in the database.[/red]")
        return
    outfile_csv_name = outfile + '.csv'
    outfile_csv = []
    outfile_json_name = outfile + '.json'
    outfile_json = []

    for bug in bugs:
        metadata = db.get_bug_metadata(bug.bug_id)
        if metadata is None:
            table.add_row(bug.bug_id, "Invalid ID", "âŒ", "âŒ", "âŒ", "âŒ", "âŒ")
            outfile_json.append({"bug_id": bug.bug_id, "error": "Invalid ID"})
            outfile_csv.append([bug.bug_id, "Invalid ID", "âŒ", "âŒ", "âŒ", "âŒ", "âŒ"])
            continue

        description = metadata.description

        # parse the crash message to get the crash type, file, function, and syscall
        result = parse_crash_log(metadata.crash_report)
        repro_path = metadata.save_syz_repro()
        try:
            all_syscalls, last_syscall = extract_last_syscall(repro_path)
        except:
            last_syscall = "unknown"

        bug_url = f"https://syzkaller.appspot.com/bug?id={bug.bug_id}"
        table.add_row(str(bug.bug_id), bug_url, result["crash_type"], last_syscall, result["file"], result["function"], str(result["line_number"]))
        outfile_csv.append([bug.bug_id, bug_url, result["crash_type"], last_syscall, result["file"], result["function"], str(result["line_number"])])
        outfile_json.append({
            "bug_id": bug.bug_id,
            "bug_url": bug_url,
            "crash_type": result["crash_type"],
            "syscall": last_syscall,
            "file": result["file"],
            "function": result["function"],
            "line_number": result["line_number"]})

    console.print(table)
    # === WRITE CSV RESULTS ===
    with open(outfile_csv_name, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Bug ID", "Report URL", "Crash Type", "Syscall", "File", "Function", "Line Number"])
        writer.writerows(outfile_csv)
    # === WRITE JSON RESULTS ===
    with open(outfile_json_name, mode="w", encoding="utf-8") as f:
        json.dump(outfile_json, f, indent=2)

    console.print(f"[green]Results saved to:[/green]")
    console.print(f"  [bold]{outfile_csv_name}[/bold]")
    console.print(f"  [bold]{outfile_json_name}[/bold]")



def test_all(local: bool, arch: str, kernel_name: str, qemu: bool, root: bool, source_image: Path, source_disk: Path, source: bool, outdir_name: str):
    db = SyzkallBugDatabase(kernel_name)
    console = Console()
    table = Table(title="Syzkaller Bug Test Results")
    table.add_column("Bug ID", style="cyan", no_wrap=True)
    table.add_column("Description", style="magenta")
    table.add_column("Compiled", justify="center")
    table.add_column("Crash Occurred", justify="center", style="red")
    table.add_column("Non-Root", justify="center", style="red")
    table.add_column("Used Assets", justify="center", style="red")

    bugs = db.get_all_bugs()
    if not bugs:
        console.print("[red]No bugs found in the database.[/red]")
        return

    results = []
    crashing_bugs = []
    unique_outcomes = []
    crash_summary = {}
    crash_type_summary = {}
    syscall_summary = {}
    exceptions = []

    base_dir = os.path.join(os.getcwd(), outdir_name)
    os.makedirs(base_dir, exist_ok=True)
    if root:
        rootstr = "_root"
    else:
        rootstr = ""
    crashes_dir = os.path.join(base_dir, f"{kernel_name}{rootstr}_crashes")
    os.makedirs(crashes_dir, exist_ok=True)
    if source:
        sources_dir = os.path.join(base_dir, f"{kernel_name}_sources")
        os.makedirs(sources_dir, exist_ok=True)


    # summary_path = os.path.join(base_dir, f"{kernel_name}_summary.txt")
    crash_report_path = os.path.join(base_dir, f"{kernel_name}{rootstr}_crash_analysis.json")
    unique_report_path = os.path.join(base_dir, f"{kernel_name}{rootstr}_unique_outcomes.json")
    crash_summary_path = os.path.join(base_dir, f"{kernel_name}{rootstr}_crash_summary.json")
    # make a log folder if it doesn't exist at the base dir
    log_dir = os.path.join(base_dir, f"logs{rootstr}")
    os.makedirs(log_dir, exist_ok=True)

    compiled_count = 0
    crashed_root_count = 0
    crashed_unroot_count = 0
    unique_outcomes_root_count = 0
    unique_outcomes_unroot_count = 0
    last_run = "âŒ"
    start_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    previous_source_image = None
    previous_source_disk = None
    download_succeeded = True
    used_assets = "no"

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TimeRemainingColumn(),
        transient=True
    ) as progress:
        task = progress.add_task("Testing bugs...", total=len(bugs))

        for idx, bug in enumerate(bugs, 1):
            metadata = db.get_bug_metadata(bug.bug_id)
            if metadata is None:
                row = [str(bug.bug_id), "Invalid ID", "âŒ", "N/A", "yes" if not root else "no", "N/A"]
                table.add_row(*row)
                results.append(row)
                progress.advance(task)
                continue

            description = metadata.description
            progress.update(
                task,
                description=f"[{idx}/{len(bugs)}] Testing Bug ID {bug.bug_id}: {description[:50]} {last_run}"
            )

            try:
                repro_path = metadata.compile_repro(arch)
                compiled = "âœ…"
                compiled_count += 1
            except Exception as e:
                compiled = "âŒ"
                repro_path = None
                print(f"Error compiling repro for bug {bug.bug_id}: {e}")

            crashed = "âŒ"
            crash_log_path = None
            if repro_path:
                progress.update(
                    task,
                    description=f"[{idx}/{len(bugs)}] Running repro for Bug ID {bug.bug_id}: {description[:50]}"
                )
                try:
                    if qemu:
                        if source:
                            source_disk, source_image = bug.download_artifacts(sources_dir)
                            if source_disk is None or source_image is None:
                                download_succeeded = False
                                if previous_source_disk and previous_source_image:
                                    source_disk = previous_source_disk
                                    source_image = previous_source_image
                                    used_assets = "no (used previous)"
                            else:
                                download_succeeded = True
                                used_assets = "yes"
                        v, t = test_repro_crashes_qemu(repro_path, local, bug.bug_id, log_dir, root, source_image, source_disk)
                    else:
                        v, t = test_repro_crashes(repro_path, local, bug.bug_id, log_dir, root)
                    if v and (t==1 or t==3):
                        crashed = "ðŸ’¥"+str(t)
                        if root:
                            crashed_root_count += 1
                        else:
                            crashed_unroot_count += 1
                        crash_subdir = os.path.join(crashes_dir, f"bug_{bug.bug_id}")
                        os.makedirs(crash_subdir, exist_ok=True)

                        os.system(f'cp {os.path.dirname(repro_path)}/* {crash_subdir}')
                        try:
                            crashing_file, crashing_line, crash_type = extract_crash_locations(metadata.crash_report)
                        except Exception as e:
                            crashing_file = "unknown"
                            crash_type = "unknown"
                            crashing_line = 0
                            exceptions.append(f"Error extracting crash locations for bug {bug.bug_id}: {e}")
                        try:
                            all_syscalls, last_syscall = extract_last_syscall(os.path.join(os.path.dirname(repro_path),"repro.syz"))
                        except:
                            last_syscall = "unknown"

                        crash_type_summary[crash_type] = crash_type_summary.get(crash_type, 0) + 1
                        syscall_summary[last_syscall] = syscall_summary.get(last_syscall, 0) + 1

                        print("ADDING CRASH",  bug.bug_id, crashed)
                        crashing_bugs.append({
                            "bug_id": bug.bug_id,
                            "description": description,
                            "repro_path": os.path.join(crash_subdir, os.path.basename(repro_path)),
                            "file": crashing_file,
                            "line" : crashing_line,
                            "crash_type": crash_type,
                            "rooted": root,
                            "syscall": last_syscall,
                            "used_assets": used_assets
                        })
                        # Attempt to run precondition analysis (non-fatal)
                        try:
                            # analyze() returns dict with parsed/snippets/evidence/classification/exploitability/llm_analysis
                            analysis = precondition_analyzer.analyze(metadata.crash_report, None)
                            outname = os.path.join(crash_subdir, f'precondition_analysis_{bug.bug_id}.json')
                            os.makedirs(os.path.dirname(outname), exist_ok=True)
                            with open(outname, 'w', encoding='utf-8') as pf:
                                _json.dump(analysis, pf, indent=2)
                        except Exception as e:
                            exceptions.append(f"precondition analysis failed for {bug.bug_id}: {e}")
                        print("crashed happened, waiting for connection...")
                        if not qemu:
                            wait_for_connection()
                        print("crashed happened, done")
                    elif not v and t==2:
                        crashed = "âš ï¸"+str(t)
                        print("WARNING: No crash, but repro may not work as expected",  bug.bug_id, crashed)
                    # elif v and t==2 and not root:
                    #     crashed = "âš ï¸"+str(t)+(" root" if root else "")
                    #     if root:
                    #         unique_outcomes_root_count += 1
                    #     else:
                    #         unique_outcomes_unroot_count += 1
                    #     crash_subdir = os.path.join(crashes_dir, f"bug_{bug.bug_id}")
                    #     os.makedirs(crash_subdir, exist_ok=True)
                    #     os.system(f'cp {os.path.dirname(repro_path)}/* {crash_subdir}')
                    #     try:
                    #         crashing_file, crashing_line, crash_type = extract_crash_locations(metadata.crash_report)
                    #     except Exception as e:
                    #         crashing_file = "unknown"
                    #         crash_type = "unknown"
                    #         crashing_line = 0
                    #         exceptions.append(f"Error extracting crash locations for bug {bug.bug_id}: {e}")
                    #     try:
                    #         all_syscalls, last_syscall = extract_last_syscall(os.path.join(os.path.dirname(repro_path),"repro.syz"))
                    #     except:
                    #         last_syscall = "unknown"

                    #     crash_type_summary[crash_type] = crash_type_summary.get(crash_type, 0) + 1
                    #     syscall_summary[last_syscall] = syscall_summary.get(last_syscall, 0) + 1
                    #     print("ADDING UNIQUE OUTCOME",  bug.bug_id, crashed)
                    #     unique_outcomes.append({
                    #         "bug_id": bug.bug_id,
                    #         "description": description,
                    #         "repro_path": os.path.join(crash_subdir, os.path.basename(repro_path)),
                    #         "file": crashing_file,
                    #         "line" : crashing_line,
                    #         "crash_type": crash_type,
                    #         "rooted": root,
                    #         "syscall": last_syscall
                    #     })

                    #     print("unqique outcome happened, waiting for connection...")
                    #     wait_for_connection()
                    #     print("unique outcome happened, done")
                    #     break

                except Exception as e:
                    # Print full traceback (with filenames & line numbers) to stdout
                    print("âŒ Exception caught, full details below:", file=sys.stdout)
                    traceback.print_exc(file=sys.stdout)
                    crashed = "âŒ"
            last_run = crashed
            if download_succeeded:
                previous_source_disk = source_disk
                previous_source_image = source_image
            row = [str(bug.bug_id), description, compiled, crashed, "yes" if not root else "no", used_assets]
            table.add_row(*row)
            results.append(row)
            progress.advance(task)

    console.print(table)


    end_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    run_time = time.mktime(time.strptime(end_time, "%Y-%m-%d %H:%M:%S")) - time.mktime(time.strptime(start_time, "%Y-%m-%d %H:%M:%S"))

    # === WRITE CRASH ANALYSIS ===
    crash_summary = {
        "kernel_name": kernel_name,
        "total_bugs": len(results),
        "compiled_count": compiled_count,
        "crashed_root_count": crashed_root_count,
        "crashed_unroot_count": crashed_unroot_count,
        "unique_outcomes_root_count": unique_outcomes_root_count,
        "unique_outcomes_unroot_count": unique_outcomes_unroot_count,
        "runtime": run_time,
        "crash_type_summary": crash_type_summary,
        "syscall_summary": syscall_summary
    }
    with open(crash_summary_path, 'w', encoding='utf-8') as f:
        json.dump(crash_summary, f, indent=2)

    with open(unique_report_path, 'w', encoding='utf-8') as f:
        json.dump(unique_outcomes, f, indent=2)

    with open(crash_report_path, 'w', encoding='utf-8') as f:
        json.dump(crashing_bugs, f, indent=2)

    # === WRITE CSV RESULTS ===
    csv_path = os.path.join(base_dir, f"{kernel_name}{rootstr}_results.csv")
    with open(csv_path, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Bug ID", "Description", "Compiled", "Crash Occurred", "Non-Root", "Used Assets"])
        writer.writerows(results)

    # === WRITE JSON RESULTS ===
    json_path = os.path.join(base_dir, f"{kernel_name}{rootstr}_results.json")
    json_data = [
        {
            "bug_id": bug_id,
            "description": desc,
            "compiled": compiled,
            "crashed": crashed,
            "rooted": root,
            "used_assets": used_assets
        }
        for bug_id, desc, compiled, crashed, root, used_assets in results
    ]
    with open(json_path, mode="w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=2)

    console.print(f"[green]Results saved to:[/green]")
    console.print(f"  [bold]{csv_path}[/bold]")
    console.print(f"  [bold]{json_path}[/bold]")
    console.print(f"  [bold]{crash_summary_path}[/bold]")
    console.print(f"  [bold]{crash_report_path}[/bold]")
    console.print(f'[green]Runtime: {run_time} seconds[/green]')
    print(exceptions)



def extract_last_syscall(syz_file_path: str):
    syscalls = []

    with open(syz_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            match = re.match(r'^([a-zA-Z0-9_]+)(\$[a-zA-Z0-9_]+)?\s*\(', line)
            if match:
                base_syscall = match.group(1)
                syscalls.append(base_syscall)

    last_syscall = syscalls[-1] if syscalls else None
    return syscalls, last_syscall


def extract_crash_type(crash_line: str, idx: int = 0) -> str:
    crash_type = ""
    for j in range(idx + 1, len(crash_line)):
        if crash_line[j] == 'in':
            break
        if crash_type:
            crash_type += ' '
        crash_type += crash_line[j].strip()
    return crash_type.strip()



"""
general protection fault, probably for non-canonical address 0xdffffc0000000001: 0000 [#1] PREEMPT SMP KASAN
KASAN: null-ptr-deref in range [0x0000000000000008-0x000000000000000f]
CPU: 0 PID: 289 Comm: syz-executor840 Not tainted 5.10.234-syzkaller-00157-ge0b88ee5f09c #0
Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 02/12/2025
RIP: 0010:dir_rename fs/incfs/vfs.c:1394 [inline]
RIP: 0010:dir_rename_wrap+0xf7/0x570 fs/incfs/vfs.c:85
===================== OR =====================
BUG: KASAN: slab-out-of-bounds in ext4_read_inline_data fs/ext4/inline.c:209 [inline]
BUG: KASAN: slab-out-of-bounds in ext4_read_inline_dir+0x435/0xd90 fs/ext4/inline.c:1515
Read of size 68 at addr ffff88810f5446d3 by task syz-executor/355

CPU: 0 PID: 355 Comm: syz-executor Not tainted 5.10.239-syzkaller #0
Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 05/07/2025
Call Trace:

"""

def parse_crash_log(log: str):
    result = {
        "crash_type": "",
        "function": "",
        "file": "",
        "line_number": ""
    }

    # Case 1: BUG: style crash
    bug_re = re.compile(
        r'(?m)^(?!.*\[inline\]).*BUG:\s*(KASAN|UBSAN|KMSAN):\s*([^\n]+?)\s+in\s+([^\s]+)\s+([^:]+):(\d+)',
        re.MULTILINE
    )

    m = bug_re.search(log)
    if m:
        result["crash_type"] = f"{m.group(2)}".strip().split(' in ')[0]
        result["function"]   = m.group(3).strip().split('+')[0]
        result["file"]       = m.group(4).strip()
        result["line_number"] = int(m.group(5))
        return result

    # Case 2: KASAN: â€¦ with RIP fallback
    m = re.search(r'KASAN:\s*([^\n]+)', log)
    if m:
        result["crash_type"] = m.group(1).strip().split(' in ')[0]
    else:
        m = re.search(r'^(.*fault.*)', log, re.MULTILINE)
        if m:
            result["crash_type"] = m.group(1).strip()

    # RIP: function+offset file:line
    bug2_re = re.compile(r'RIP:.*?([a-zA-Z0-9_]+)\+.*? ([^ ]+):(\d+)(?!\s*\[inline\])', re.MULTILINE)
    m = bug2_re.search(log)
    if m:
        result["function"] = m.group(1).split('+')[0]
        result["file"] = m.group(2)
        result["line_number"] = int(m.group(3))

    return result


def extract_crash_locations(log_text: str):
    """
    Extract the first N crash entries after a line of ====.
    Looks for lines like:
    'BUG: KASAN: ... in <func> <file>:<line> [inline]'
    """
    log_lines = log_text.split("\n")
    start = 0
    end = -1
    for idx, line in enumerate(log_lines):
        line = line.strip()
        if line.startswith('====') and start == 0:
            start = idx
        if line == '':
            end = idx - 1
        if line.startswith("CPU") and end == -1:
            end = idx - 2
    if end < start or end >= len(log_lines):
        return "", 0, "end index out of range"
    # entries = []
    crashing_file = ""
    crash_type = ""
    crashing_line = 0
    for i in range(start +1, end):
        line = log_lines[i].split(' ')
        if line[-1] == '[inline]':
            continue
        else:
            file_line1 = next((word for word in line if '.c:' in word), None)
            file1, line1_num = (file_line1.split(':') if file_line1 else (None, None))
            crashing_file = file1
            crashing_line = line1_num
            for idx, word in enumerate(line):
                # TODO: make this more robust
                # currently this just looks at memory bugs from KASAN, UBSAN, and KMSAN
                # it then determines the type of the crash from all words starting after the colon afer the bug type
                # all the way until the word 'in'
                if 'KASAN:' in word:
                    crash_type = extract_crash_type(line, idx)
                elif 'UBSAN:' in word:
                    crash_type = extract_crash_type(line, idx)
                elif 'KMSAN:' in word:
                    crash_type = extract_crash_type(line, idx)
            break

    return crashing_file, crashing_line, crash_type

def test(id: str, local: bool, arch: str, root: bool, kernel_name: str, qemu: bool, source_disk: Path, source_image: Path, outdir_name: str):
    db = SyzkallBugDatabase(kernel_name)
    metadata = db.get_bug_metadata(id)
    if metadata is None:
        print('Invalid bug id supplied')
        return
    
    print(f'Compiling {metadata.description}...')
    repro_path = metadata.compile_repro(arch)
    if qemu:
        v, t = test_repro_crashes_qemu(repro_path, local, id, outdir_name, root, source_image, source_disk)
    else:
        v, t = test_repro_crashes(repro_path, local, id, outdir_name, root)
    if v:
        print('Crash occured')
    else:
        print('No crash occured')
    # Run precondition analysis and save output (non-fatal)
    try:
        analysis = precondition_analyzer.analyze(metadata.crash_report, None)
        outname = os.path.join(outdir_name, f"precondition_analysis_{id}.json")
        os.makedirs(os.path.dirname(outname), exist_ok=True)
        with open(outname, 'w', encoding='utf-8') as of:
            _json.dump(analysis, of, indent=2)
        print(f"[+] Precondition analysis written to: {outname}")
    except Exception as e:
        print(f"[!] Precondition analysis failed: {e}")