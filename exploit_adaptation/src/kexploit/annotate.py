from pathlib import Path
from typing import Optional
from enum import Enum
from dataclasses import dataclass
import os

from openai import OpenAI
from pydantic import BaseModel

from .kernel_image import KernelImage, Kernel
from .parse import Literal, Token, LexedCode, ReplaceResult, ExploitFileGroup, DiffMode, is_annotated
from .tui import console
from .annotations import Annotation, RopAddress, KernelAddress, SrcMetadata, KernelData

SYSTEM_PROMPT = '''
You are a model which analyses exploits for the linux kernel. These exploits depend on addresses, offsets, and struct field offsets in the linux kernel to work.
You should find all these addresses, and output JSON data containing a list of these addresses, offsets, and struct field offsets?

Addresses are 64 bit absolute virtual kernel addresses, typically with many leading 0xffffff bits, and have a type of `address`.
Offsets are offsets relative to the base of the kernel executable, and have a type of `offset`.
Struct field offsets are the offset from the start of a struct to a field in the struct. They are typically much smaller then offsets relative to the kernel base, typically less than 2048.
Struct field offsets have a type of `struct_offset`.

Report addresses and offsets exactly as they appear in the code. Don't try to compute offsets from the addresses present in the code, just report the address constants in the JSON data.

For example, for the given input code:
File main.c:
```
#define A 0xffffffc010020f58
#define B_OFFSET 0x120

int main() {
    size_t c = 0xffffffc092004cd0;
    size_t d_offset = 0x34980;
    size_t z_offset = A - 0xffffffc088084000;
}
```
File exploit.h:
```
#define M_PTR 0x68
#define TIMERFD_FOPS_OFFSET 0x149828
```

The corresponding JSON data is:
```
[
    {
        "file": "main.c",
        "type": "address",
        "value": "0xffffffc010020f58"
    },
    {
        "file": "main.c",
        "type": "struct_offset",
        "value": "0x120"
    },
    {
        "file": "main.c",
        "type": "address",
        "value": "0xffffffc092004cd0"
    },
    {
        "file": "main.c",
        "type": "offset",
        "value": "0x34980"
    },
    {
        "file": "main.c",
        "type": "address",
        "value": "0xffffffc088084000"
    },
    {
        "file": "exploit.h",
        "type": "struct_offset",
        "value": "0x68"
    },
    {
        "file": "exploit.h",
        "type": "offset",
        "value": "0x149828"
    }
]
```

Be careful not to report integers constants which are used for other purposes (loop iteration, syscall arguments, etc.) as kernel addresses or offsets.

You will be given C code from several files, and should output only the JSON data.
'''
# MODEL = 'gpt-4.1-mini'
MODEL = 'gpt-5-mini'

class InitialAnnotationType(Enum):
    NONE = 0
    ADDRESS = 1
    OFFSET = 2
    VALUE = 3

class AnnotationCanidate:
    '''
    Scores how likely an integer is to be a specific type of annotation
    '''

    scores: list[tuple[float, Optional[Annotation]]]

    def __init__(self):
        self.scores = [(0.0, None) for _ in range(4)]
    
    def set_score(self, annotation_type: InitialAnnotationType, score: float, annotation: Optional[Annotation] = None):
        self.scores[annotation_type.value] = (score, annotation)
    
    def get_annotation(self) -> tuple[InitialAnnotationType, Optional[Annotation]]:
        max_score = 0.0
        annotation_type = InitialAnnotationType.NONE

        for i, (score, _) in enumerate(self.scores):
            if score > max_score:
                max_score = score
                annotation_type = InitialAnnotationType(i)
        
        return annotation_type, self.scores[annotation_type.value][1]

class AnnotateContext:
    kernel_image: KernelImage
    files: ExploitFileGroup
    llm_annotate: bool

    def __init__(self, files: list[Path], kernel_name: Kernel, llm_annotate: bool):
        # mapping from filename to file lexed file content
        self.files = ExploitFileGroup(files)
        self.kernel_image = KernelImage(kernel_name)
        self.llm_annotate = llm_annotate

        if llm_annotate:
            self.annotation_map = get_chatgpt_completions(self)
        else:
            self.annotation_map = {}
    
    def get_chatgpt_annotation(self, file: LexedCode, value: int) -> Optional[Annotation]:
        assert self.llm_annotate

        annotation_type = self.annotation_map.get((os.path.basename(file.filename), value))
        if annotation_type is None:
            return None
        
        is_relative = self.kernel_image.is_kernel_offset(value)
        is_valid = is_relative or self.kernel_image.is_kernel_address(value)
        
        # we don't care what chatgpt says for address or offset, just do it based on kernel image
        if (annotation_type == InitialAnnotationType.ADDRESS or annotation_type == InitialAnnotationType.OFFSET) and not is_valid:
            # value is not address or offset
            return ReplaceResult.skip()
        
        return KernelAddress(address=value, is_relative=is_relative)
    
    def get_non_chatgpt_annotation(self, value: int) -> Optional[Annotation]:
        score = score_annotations_for_integer(self, value)
        _, annotation = score.get_annotation()
        return annotation
    
    def get_annotation(self, file: LexedCode, value: int) -> Optional[Annotation]:
        if self.llm_annotate:
            annotation = self.get_chatgpt_annotation(file, value)
        else:
            annotation = self.get_non_chatgpt_annotation(value)
        
        rop_gadget = None
        if self.kernel_image.arch_info.rop_translation_supported() and isinstance(annotation, KernelAddress):
            if annotation.is_relative:
                prev_symbol = self.kernel_image.find_previous_symbol_offset(value)
                on_symbol = prev_symbol is not None and value == prev_symbol.offset
                rop_gadget = self.kernel_image.get_rop_chain_instructions_offset(value)
            else:
                prev_symbol = self.kernel_image.find_previous_symbol_absolute(value)
                on_symbol = prev_symbol is not None and value == prev_symbol.address
                rop_gadget = self.kernel_image.get_rop_chain_instructions(value)

            if rop_gadget is not None and not on_symbol:
                return RopAddress(gadget=rop_gadget, is_relative=annotation.is_relative, original_value=value)
        
        return annotation

class KernelOffset(BaseModel):
    file: str
    # either `address`, `offset`, or `struct_offset`
    type: str
    value: str

class AnnotationData(BaseModel):
    data: list[KernelOffset]

def get_chatgpt_completions(context: AnnotateContext) -> dict[tuple[str, int], InitialAnnotationType]:
    prompt = ''
    for file in context.files.lexed_code():
        prompt += f'File: {os.path.basename(file.filename)}\n`{file.code}`\n'
    
    # TODO: use json schema
    response = OpenAI().responses.parse(
        model=MODEL,
        instructions=SYSTEM_PROMPT,
        input=prompt,
        text_format=AnnotationData,
    )

    # make type checker happy
    assert response.output_parsed is not None

    annotation_map = {}
    for offset in response.output_parsed.data:
        try:
            n = int(offset.value, 0)
        except:
            # only consider things chatgpt reports which are just integer constants for now
            continue

        match offset.type:
            case 'address':
                annotation = InitialAnnotationType.ADDRESS
            case 'offset':
                annotation = InitialAnnotationType.OFFSET
            case 'struct_offset':
                # ignore struct offsets for now
                continue
                # annotation = KEXPLOIT_STRUCT_OFFSET
            case _:
                # if invalid type ignore
                print(f'Warning: invalid offset type received from LLM: {offset.type}')
                continue

        annotation_map[(offset.file, n)] = annotation
    
    return annotation_map

MAX_SCORE = 1.0

def lerp(min_x: float, max_x: float, min_y: float, max_y: float, x: float):
    x = max(min_x, min(x, max_x))
    return ((max_y - min_y) / (max_x - min_x)) * (x - min_x) + min_y

def score_kernel_address(context: AnnotateContext, canidate: AnnotationCanidate, n: int):
    annotation = KernelAddress(address=n, is_relative=False)

    if context.kernel_image.is_kernel_address(n):
        base_score = 0.85

        previous_symbol = context.kernel_image.find_previous_symbol_absolute(n)
        if previous_symbol is None:
            return base_score
        
        score_loss = lerp(0, 0x40, 0.0, MAX_SCORE - base_score, n - previous_symbol.address)

        canidate.set_score(InitialAnnotationType.ADDRESS, MAX_SCORE - score_loss, annotation)    

def score_kernel_offset(context: AnnotateContext, canidate: AnnotationCanidate, n: int):
    annotation = KernelAddress(address=n, is_relative=True)

    if context.kernel_image.is_kernel_offset(n):
        base_score = 0.0
        away_from_0_score = lerp(0, 0x8000, 0.0, 0.2, n)
        hex_digits_score = lerp(0, 4, 0.0, 0.3, len(set(hex(n)[2:])))
        decimal_digits_score = lerp(0, 4, 0.0, 0.3, len(set(str(n))))

        close_to_symbol_score = 0.0
        previous_symbol = context.kernel_image.find_previous_symbol_absolute(n)
        if previous_symbol is not None:
            close_to_symbol_score = lerp(0, 0x10, 0.2, 0.0, n - previous_symbol.address)
        
        score = base_score + away_from_0_score + hex_digits_score + decimal_digits_score + close_to_symbol_score
        canidate.set_score(InitialAnnotationType.OFFSET, score, annotation)

# FIXME: this assumes integers are 8 byte annotations
def score_kernel_value(context: AnnotateContext, canidate: AnnotationCanidate, n: int):
    n_bytes = context.kernel_image.arch_info.pack_usize(n)
    count = 0
    max_count = 10

    closest_addr = 0
    distance = 1 << 64

    for address in context.kernel_image.find_bytes(n_bytes):
        count += 1
        # if we exceed the max count, return
        if count > max_count:
            return
        
        previous_symbol = context.kernel_image.find_previous_symbol_absolute(address)
        current_distance = address - previous_symbol.address
        if current_distance < distance:
            distance = current_distance
            closest_addr = address
    
    if n == 0x149abfff91005a4d:
        print(count)

    if count == 0:
        return
    
    hex_digit_score = lerp(1, 16, 0.0, 0.15, len(hex(n)) - 2)
    matching_count_score = lerp(1, 10, 0.5, 0.0, count)
    distance_score = lerp(0, 0x10, 0.3, 0.0, distance)

    score = hex_digit_score + matching_count_score + distance_score
    annotation = KernelData(address=closest_addr, num_bytes=8, original_value=n)
    canidate.set_score(InitialAnnotationType.VALUE, score, annotation)

def score_annotations_for_integer(context: AnnotateContext, n: int) -> AnnotationCanidate:
    score = AnnotationCanidate()

    # threshhold that needs to be cleared to have a type for an annotation
    score.set_score(InitialAnnotationType.NONE, 0.75)
    score_kernel_address(context, score, n)
    score_kernel_offset(context, score, n)
    score_kernel_value(context, score, n)

    return score

def annotate(exploit_files: list[Path], kernel: Kernel, llm_annotate: bool, diff_output: Optional[Path], diff_mode: DiffMode):
    context = AnnotateContext(exploit_files, kernel, llm_annotate)
    metadata = SrcMetadata(
        original_kernel_name=kernel.name,
        current_kernel_name=kernel.name,
    )

    def rewrite_file(lexed_file: LexedCode) -> Optional[str]:
        if lexed_file.is_annotated():
            console.error(f'{lexed_file.filename} is already annotated')
            return None
        
        def do_annotate(tokens: list[Token], index: int) -> ReplaceResult:
            match tokens[index].data:
                case Literal(int(value)):
                    # don't annotate if already annotated
                    if is_annotated(tokens, index):
                        return ReplaceResult.skip()
                    
                    annotation = context.get_annotation(lexed_file, value)
                    if annotation is None:
                        return ReplaceResult.skip()
                    else:
                        return ReplaceResult.replace(1, str(annotation))

            return ReplaceResult.skip()
        
        annotated_code, errors = lexed_file.replace_tokens(do_annotate)

        annotated_code = f'{metadata}\n{annotated_code}'
        console.print_errors(errors)

        return annotated_code
    
    context.files.rewrite_files(rewrite_file, diff_output, diff_mode)