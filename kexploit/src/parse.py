from dataclasses import dataclass
from typing import List, Optional, Callable, Tuple

# Utility functions for `parsing` C code of exploits
# Not currently using a real parser to parse exploits, but may in future
# only some lexing taking place

@dataclass
class Span:
    start_index: int
    # exclusive
    end_index: int

@dataclass
class Code:
    content: str

@dataclass
class Literal:
    value: int

@dataclass
class Ident:
    name: str

# currently only `(` or `)`
@dataclass
class Delim:
    delim: str

TokenData = Code | Literal | Ident | Delim

@dataclass
class Token:
    span: Span
    data: TokenData

def is_delim(token: Token, delim_type: str):
    return token.data == Delim(delim_type)

def find_before(tokens: List[Token], index: int, f: Callable[[Token], bool]) -> Optional[int]:
    return next((index - i - 1 for i, token in enumerate(reversed(tokens[:index])) if f(token)), None)

def find_after(tokens: List[Token], index: int, f: Callable[[Token], bool]) -> Optional[int]:
    return next((i + index for i, token in enumerate(tokens[index:]) if f(token)), None)

@dataclass
class LexedCode:
    filename: str
    code: str
    tokens: List[Token]

    # takes in a callback
    # Callback gets index in token stream to check
    # if it wants to replace code corresponding to tokens with different code,
    # return a tupole representing amount of tokens to replace and code to replace them with
    def replace_tokens(self, f: Callable[[List[Token], int], Optional[Tuple[int, str]]]) -> str:
        output = ""
        last_index = 0
        token_index = 0

        while token_index < len(self.tokens):
            token = self.tokens[token_index]
            # put all characters from last processed token end to this tokens end into output
            output += self.code[last_index:token.span.start_index]
            last_index = token.span.start_index

            match f(self.tokens, token_index):
                case int(skip_count), str(code):
                    output += code
                    token_index += skip_count
                    last_index = self.tokens[token_index - 1].span.end_index
                case _:
                    output += self.code[last_index:token.span.end_index]
                    last_index = token.span.end_index
                    token_index += 1
        
        # put remaining code in output
        output += self.code[last_index:]

        return output

def lex(filename: str, code: str) -> LexedCode:
    i = 0
    tokens = []
    
    def matches(pattern: str) -> bool:
        return i + len(pattern) <= len(code) and code[i:i + len(pattern)] == pattern

    def next_match(pattern: str) -> Optional[int]:
        try:
            return code[i:].index(pattern) + i
        except:
            return None
    
    def span_until(index: int) -> Span:
        return Span(
            start_index=i,
            end_index=index,
        )
    
    def take_whitespace() -> bool:
        nonlocal i

        if code[i].isspace():
            i += 1
            return True
        else:
            return False
    
    def take_comment() -> bool:
        nonlocal i

        if matches('//'):
            end_index = next_match('\n')
            if end_index is None:
                return False
            
            i = end_index + 1
            return True
        elif matches('/*'):
            end_index = next_match('*/')
            if end_index is None:
                return False
            
            i = end_index + 1
            return True
        else:
            return False

    def take_int_literal() -> Optional[Token]:
        nonlocal i

        if matches('0x') or matches('0X'):
            # hex number
            next_i = i + 2
            n_str = ''

            # TODO: convert to ascii and us lt gt to check if hex digit
            while next_i < len(code) and code[next_i] in '0123456789abcdefABCDEF':
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 16)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal
        elif matches('0b') or matches('0B'):
            # binary number
            next_i = i + 2
            n_str = ''

            while next_i < len(code) and code[next_i] in '01':
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 2)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal
        elif matches('0'):
            # octal number
            next_i = i + 1
            n_str = ''

            while next_i < len(code) and code[next_i] in '01234567':
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 8)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal
        else:
            # decimal number
            next_i = i
            n_str = ''

            while next_i < len(code) and code[next_i].isdigit():
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 10)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal

    def take_literal() -> Optional[Token]:
        return take_int_literal()

    def take_ident() -> Optional[Token]:
        nonlocal i

        if code[i].isalpha() or code[i] == '_':
            ident = code[i]
            next_i = i + 1

            while next_i < len(code) and (code[next_i].isalnum() or code[next_i] == '_'):
                ident += code[next_i]
                next_i += 1
            
            ident = Token(
                span=span_until(next_i),
                data=Ident(ident),
            )

            i = next_i
            return ident
        else:
            return None
    
    def take_delim() -> Optional[Token]:
        nonlocal i

        if code[i] == '(':
            delim = Token(span_until(i + 1), Delim('('))
            i += 1
            return delim
        elif code[i] == ')':
            delim = Token(span_until(i + 1), Delim(')'))
            i += 1
            return delim
        else:
            return None

    while i < len(code):
        if take_whitespace() or take_comment():
            pass
        elif (ident := take_ident()) is not None:
            tokens.append(ident)
        elif (literal := take_literal()) is not None:
            tokens.append(literal)
        elif (delim := take_delim()) is not None:
            tokens.append(delim)
        else:
            i += 1
    
    return LexedCode(
        filename=filename,
        code=code,
        tokens=tokens,
    )