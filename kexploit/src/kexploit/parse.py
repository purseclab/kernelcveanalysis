from dataclasses import dataclass
from typing import Optional, Callable, Self, Iterator
from enum import StrEnum, Enum
from pathlib import Path
from difflib import unified_diff
from pydantic import BaseModel
import bisect
import os

from .tui import SrcCodeError, console

# Utility functions for `parsing` C code of exploits
# Not currently using a real parser to parse exploits, but may in future
# only some lexing taking place

@dataclass
class Span:
    start_index: int
    # exclusive
    end_index: int

@dataclass
class Code:
    content: str

@dataclass
class Literal:
    value: str | int

@dataclass
class Ident:
    name: str

# currently only `(` or `)`
@dataclass
class Delim:
    delim: str

TokenData = Code | Literal | Ident | Delim

@dataclass
class Token:
    span: Span
    data: TokenData

    def is_literal(self) -> bool:
        match self.data:
            case Literal(_):
                return True
            case _:
                return False

def is_delim(token: Token, delim_type: str):
    return token.data == Delim(delim_type)

def find_before(tokens: list[Token], index: int, f: Callable[[Token], bool]) -> Optional[int]:
    return next((index - i - 1 for i, token in enumerate(reversed(tokens[:index])) if f(token)), None)

def find_after(tokens: list[Token], index: int, f: Callable[[Token], bool]) -> Optional[int]:
    return next((i + index for i, token in enumerate(tokens[index:]) if f(token)), None)

class AnnotationType(StrEnum):
    KEXPLOIT_META = "__kexploit_src_metadata"
    KEXPLOIT_ADDRESS = "__kexploit_kernel_address"
    KEXPLOIT_OFFSET = "__kexploit_kernel_offset"
    KEXPLOIT_STRUCT_OFFSET = "__kexploit_struct_offset"
    KEXPLOIT_ROP_ADDRESS = "__kexploit_rop_address"
    KEXPLOIT_ROP_OFFSET = "__kexploit_rop_offset"

rule_names = set(str(annotation) for annotation in AnnotationType)
def is_rule_token(token: Token) -> bool:
    match token.data:
        case Ident(name) if name in rule_names:
            return True
        case _:
            return False

RewriteValue = int | str

def parse_rewrite_value(token: Token) -> Optional[RewriteValue]:
    match token.data:
        case Literal(value):
            return value
        case _:
            return None

def parse_rewrite_values(tokens: list[Token]) -> Optional[list[RewriteValue]]:
    values = [parse_rewrite_value(token) for token in tokens]

    if any(value is None for value in values):
        return None
    else:
        return values


def rewrite_value_to_string(value: RewriteValue) -> str:
    match value:
        case int(x):
            return hex(x)
        case str(x):
            return f'"{x.replace('\\', '\\\\').replace('"', '\\"')}"'

def rewrite_values_to_string(values: list[RewriteValue]) -> str:
    return ', '.join(rewrite_value_to_string(value) for value in values)

@dataclass
class Annotation:
    annotation_name: AnnotationType
    args: list[RewriteValue]

    def token_count(self) -> int:
        # 3 extra for both parens and annottion name
        return len(self.args) + 3
    
    def __str__(self) -> str:
        return f'{self.annotation_name}({rewrite_values_to_string(self.args)})'
    
    def with_args(self, new_args: list[RewriteValue]) -> 'Annotation':
        return Annotation(
            annotation_name=self.annotation_name,
            args=new_args,
        )

    @classmethod
    def parse(cls, tokens: list[Token], token_index: int) -> Optional[Self]:
        # need at least ident and 2 delims
        if token_index + 3 <= len(tokens):
            match tokens[token_index].data, tokens[token_index + 1].data:
                case Ident(name), Delim(delim='('):
                    try:
                        name = AnnotationType(name)
                    except ValueError:
                        return None
                    
                    # skip paren and ident
                    arg_index = token_index + 2

                    # find index of closing paren
                    end_index = find_after(tokens, token_index, lambda token: is_delim(token, ')'))
                    if end_index is None:
                        return None

                    args = parse_rewrite_values(tokens[arg_index:end_index])
                    if args is None:
                        return None

                    return cls(
                        annotation_name=name,
                        args=args,
                    )
        
        return None

class SrcMetadata(BaseModel):
    # kernel original exploit was written for
    original_kernel_name: str
    # kernel exploit is currently adapted to
    current_kernel_name: str
    
    def to_annotation(self) -> Annotation:
        return Annotation(
            annotation_name=AnnotationType.KEXPLOIT_META,
            args=[self.model_dump_json()],
        )

@dataclass
class LineInfo:
    line: str
    line_number: int
    line_start_index: int

@dataclass
class DoReplace:
    replace_token_count: int
    replacement_text: str

# returned by replace token callable to indicate error
@dataclass
class ReplaceError:
    message: str
    error_token: Token

class ReplaceResult:
    result: DoReplace | ReplaceError | None

    def __init__(self, result: DoReplace | ReplaceError | None):
        self.result = result

    @classmethod
    def replace(cls, replace_token_count: int, replacement_text: str) -> Self:
        return cls(DoReplace(
            replace_token_count=replace_token_count,
            replacement_text=replacement_text,
        ))
    
    @classmethod
    def skip(cls) -> Self:
        return cls(None)
    
    @classmethod
    def error(cls, message: str, token: Token) -> Self:
        return cls(ReplaceError(
            message=message,
            error_token=token,
        ))

# Both C and Python are supported for annotating
class CodeType(Enum):
    C = 1
    PYTHON = 2

class LexedCode:
    filename: Path
    code_type: CodeType
    code: str
    tokens: list[Token]
    line_endings: list[int]
    annotations: list[Annotation]

    def __init__(self, filename: Path, code_type: CodeType, code: str, tokens: list[Token]):
        self.filename = filename
        self.code_type = code_type
        self.code = code
        self.tokens = tokens
        self.calculate_line_endings()

        self.annotations = []
        for index in range(len(self.tokens)):
            annotation = Annotation.parse(self.tokens, index)
            if annotation is None:
                continue

            self.annotations.append(annotation)
    
    def get_annotations(self, type: Optional[AnnotationType] = None) -> list[Annotation]:
        if type is None:
            return self.annotations
        else:
            return [annotation for annotation in self.annotations if annotation.annotation_name == type]
    
    def get_metadata(self) -> Optional[SrcMetadata]:
        annotations = self.get_annotations(type=AnnotationType.KEXPLOIT_META)
        if len(annotations) != 1:
            return None
        
        args = annotations[0].args
        # TODO: better error message
        assert len(args) == 1 and type(args[0]) == str

        return SrcMetadata.model_validate_json(args[0])
    
    def is_annotated(self) -> bool:
        return len(self.get_annotations()) > 0

    # takes in a callback
    # Callback gets index in token stream to check
    # if it wants to replace code corresponding to tokens with different code,
    # returns a ReplaceResult which indicates what to replace if anything
    def replace_tokens(self, f: Callable[[list[Token], int], ReplaceResult]) -> tuple[str, list[SrcCodeError]]:
        output = ""
        last_index = 0
        token_index = 0
        errors = []

        while token_index < len(self.tokens):
            token = self.tokens[token_index]
            # put all characters from last processed token end to this tokens end into output
            output += self.code[last_index:token.span.start_index]
            last_index = token.span.start_index

            match f(self.tokens, token_index).result:
                case DoReplace(skip_count, code):
                    # replace tokens
                    output += code
                    token_index += skip_count
                    last_index = self.tokens[token_index - 1].span.end_index
                case ReplaceError(message, error_token):
                    # skip replace tokens with error
                    output += self.code[last_index:token.span.end_index]
                    last_index = token.span.end_index
                    token_index += 1

                    errors.append(SrcCodeError(
                        message=message,
                        span=error_token.span,
                        code=self,
                    ))
                case None:
                    # slip replace tokens
                    output += self.code[last_index:token.span.end_index]
                    last_index = token.span.end_index
                    token_index += 1
        
        # put remaining code in output
        output += self.code[last_index:]

        return output, errors
    
    def calculate_line_endings(self):
        self.line_endings = [index for index, character in enumerate(self.code) if character == '\n']
    
    # returns tuple of line number, line
    def get_line(self, index: int) -> LineInfo:
        line_end_index = bisect.bisect_left(self.line_endings, index)

        prev_index = 0 if line_end_index == 0 else self.line_endings[line_end_index - 1] + 1
        next_index = len(self.code) if line_end_index >= len(self.line_endings) else self.line_endings[line_end_index]

        return LineInfo(
            line=self.code[prev_index:next_index],
            line_number=line_end_index + 1,
            line_start_index=prev_index,
        )

def lex(filename: Path, code: str) -> Optional[LexedCode]:
    i = 0
    tokens = []

    filename_end = os.path.basename(filename)

    if filename_end.endswith('.c') or filename_end.endswith('.h'):
        code_type = CodeType.C
    elif filename_end.endswith('.py'):
        code_type = CodeType.PYTHON
    else:
        console.error('Unsupported language')
        return None
    
    def matches(pattern: str) -> bool:
        return i + len(pattern) <= len(code) and code[i:i + len(pattern)] == pattern

    def next_match(pattern: str) -> Optional[int]:
        try:
            return code[i:].index(pattern) + i
        except:
            return None
    
    def span_until(index: int) -> Span:
        return Span(
            start_index=i,
            end_index=index,
        )
    
    def take_whitespace() -> bool:
        nonlocal i

        if code[i].isspace():
            i += 1
            return True
        else:
            return False
    
    def take_comment() -> bool:
        nonlocal i

        if code_type == CodeType.C:
            comment_delims = [('//', '\n'), ('/*', '*/')]
        elif code_type == CodeType.PYTHON:
            comment_delims = [('#', '\n')]
        
        for start_delim, end_delim in comment_delims:
            if matches(start_delim):
                end_index = next_match(end_delim)
                if end_index is None:
                    return False
                
                i = end_index + 1
                return True
        
        return False

    def take_int_literal() -> Optional[Token]:
        nonlocal i

        if matches('0x') or matches('0X'):
            # hex number
            next_i = i + 2
            n_str = ''

            # TODO: convert to ascii and us lt gt to check if hex digit
            while next_i < len(code) and code[next_i] in '0123456789abcdefABCDEF':
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 16)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal
        elif matches('0b') or matches('0B'):
            # binary number
            next_i = i + 2
            n_str = ''

            while next_i < len(code) and code[next_i] in '01':
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 2)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal
        elif matches('0'):
            # octal number
            next_i = i + 1
            n_str = ''

            while next_i < len(code) and code[next_i] in '01234567':
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 8)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal
        else:
            # decimal number
            next_i = i
            n_str = ''

            while next_i < len(code) and code[next_i].isdigit():
                n_str += code[next_i]
                next_i += 1
            
            if len(n_str) == 0:
                return None
            
            try:
                n = int(n_str, 10)
            except:
                return None
            
            literal = Token(
                span=span_until(next_i),
                data=Literal(n)
            )

            i = next_i
            return literal
    
    def take_string_literal() -> Optional[Token]:
        nonlocal i

        if matches('"'):
            index = i + 1
            string_data = ''

            in_escape = False
            found_closing_quote = False
            while index < len(code):
                if not in_escape:
                    if code[index] == '\\':
                        in_escape = True
                    elif code[index] == '"':
                        found_closing_quote = True
                        break
                    else:
                        string_data += code[index]
                else:
                    string_data += code[index]
                    in_escape = False
                
                index += 1
            
            if not found_closing_quote:
                return None

            next_index = index + 1
            
            literal = Token(
                span=span_until(next_index),
                data=Literal(string_data),
            )

            i = next_index

            return literal
        else:
            return None

    def take_literal() -> Optional[Token]:
        n = take_int_literal()
        if n is not None:
            return n
        
        value = take_string_literal()
        if value is not None:
            return value
        
        return None

    def take_ident() -> Optional[Token]:
        nonlocal i

        if code[i].isalpha() or code[i] == '_':
            ident = code[i]
            next_i = i + 1

            while next_i < len(code) and (code[next_i].isalnum() or code[next_i] == '_'):
                ident += code[next_i]
                next_i += 1
            
            token = Token(
                span=span_until(next_i),
                data=Ident(ident),
            )

            i = next_i
            return token
        else:
            return None
    
    def take_delim() -> Optional[Token]:
        nonlocal i

        if code[i] == '(':
            delim = Token(span_until(i + 1), Delim('('))
            i += 1
            return delim
        elif code[i] == ')':
            delim = Token(span_until(i + 1), Delim(')'))
            i += 1
            return delim
        else:
            return None

    while i < len(code):
        if take_whitespace() or take_comment():
            pass
        elif (ident := take_ident()) is not None:
            tokens.append(ident)
        elif (literal := take_literal()) is not None:
            tokens.append(literal)
        elif (delim := take_delim()) is not None:
            tokens.append(delim)
        else:
            i += 1
    
    return LexedCode(
        filename=filename,
        code_type=code_type,
        code=code,
        tokens=tokens,
    )

class ExploitFileError(Exception):
    message: str

    def __init__(self, message: str):
        self.message = message
        super().__init__(message)

class DiffMode(Enum):
    APPLY_DIFF = 1
    ASK_APPLY_DIFF = 2
    SHOW_DIFF = 3

class ExploitFileGroup:
    """Group of input exploit files to be processed."""
    code: dict[Path, LexedCode]

    def __init__(self, exploit_files: list[Path]):
        self.code = {}

        for file in exploit_files:
            # if kexploit.h is globbed in exploit_files, ignore
            if os.path.basename(file) == 'kexploit.h' or os.path.basename(file) == 'kexploit.py':
                continue

            with open(file, 'r') as f:
                exploit_code = f.read()
            
            lexed_code = lex(file, exploit_code)
            if lexed_code is None:
                continue
            self.code[file] = lexed_code
    
    def lexed_code(self) -> Iterator[LexedCode]:
        return iter(self.code.values())
    
    def get_src_kernel_name(self) -> str:
        """
        Gets the name of the kernel the exploit files were originally annotated for.
        
        If the files are not all annotated, or are annotated for different kernel names, raises a `ExploitFileError`
        """

        kernel_name = None

        for code in self.lexed_code():
            metadata = code.get_metadata()
            if metadata is None:
                raise ExploitFileError(f'{code.filename} is not annotated')
            
            if kernel_name is None:
                kernel_name = metadata.current_kernel_name
            elif kernel_name != metadata.current_kernel_name:
                raise ExploitFileError(f'{code.filename} is annotated for kernel `{metadata.current_kernel_name}`, but the other files are annotated for `{kernel_name}`')
        
        if kernel_name is None:
            raise ExploitFileError('No exploit files provided as input')
        else:
            return kernel_name
    
    def rewrite_files(self, rewrite_fn: Callable[[LexedCode], Optional[str]], diff_output: Optional[Path], diff_mode: DiffMode):
        """
        Applies `rewrite_fn` to lexed code to get new source code.
        If `rewrite_fn` is None, skips rewriting the file.

        Outputs the diff to the given file, and potentially applies it.
        """

        diff = ''
        new_file_contents = []

        for lexed_file in self.lexed_code():
            new_code = rewrite_fn(lexed_file)
            if new_code is None:
                continue

            for diff_line in unified_diff(
                lexed_file.code.split('\n'),
                new_code.split('\n'),
                fromfile=str(lexed_file.filename),
                tofile=str(lexed_file.filename),
                lineterm='',
            ):
                diff += diff_line + '\n'
            
            new_file_contents.append((lexed_file.filename, new_code))
        
        if diff_output is not None:
            with open(diff_output, 'w') as f:
                f.write(diff)
        
        def apply_diff():
            for file, code in new_file_contents:
                with open(file, 'w') as f:
                    f.write(code)
        
        if diff_mode == DiffMode.APPLY_DIFF:
            console.print_diff(diff)
            apply_diff()
        elif diff_mode == DiffMode.ASK_APPLY_DIFF:
            if console.confirm_diff(diff):
                apply_diff()
        elif diff_mode == DiffMode.SHOW_DIFF:
            console.print_diff(diff)
