from typing import Optional
from datetime import datetime
import time

from bs4 import BeautifulSoup, Tag
import requests

from .bug_db import BugMetadata, SyzkallBugDatabase

def get_tag_string(tag: Tag) -> str:
    return ' '.join(tag.find_all(string=True)).strip()

def parse_table_row(row: Tag, header=False) -> list[Tag]:
    assert row.name == 'tr'
    look_tag = 'th' if header else 'td'
    return row.find_all(look_tag)

def parse_table_row_strings(row: Tag, header=False) -> list[str]:
    return [get_tag_string(entry.find(name=True)) for entry in parse_table_row(row, header)]

def parse_table(table: Tag) -> list[dict[str, Tag]]:
    assert table.name == 'table'
    header_values = ['_'.join(value.lower().split()) for value in parse_table_row_strings(table.thead.tr, header=True)]
    
    output = []
    for row in table.tbody.find_all(name='tr'):
        row = parse_table_row(row)
        row_data = {}

        for header, row_value in zip(header_values, row):
            row_data[header] = row_value
        
        output.append(row_data)
            
    return output

# find a table based on if the name in the name in the caption element contains `name_part`
def find_table_by_name(html: BeautifulSoup, name_part: str) -> Tag:
    for caption in html.find_all('caption'):
        if name_part.lower() in get_tag_string(caption).lower():
            parent = caption.parent
            if parent.name == 'table':
                return parent

    return None

def path_to_syzkall_url(url_path: str) -> str:
    return f'https://syzkaller.appspot.com/{url_path}'

def get_syzkall_html(url_path: str) -> BeautifulSoup:
    response = requests.get(path_to_syzkall_url(url_path))
    return BeautifulSoup(response.text, 'html.parser')

# filters for bugs which are potentially exploitable
def filter_bugs(data: list[dict[str, Tag]]) -> list[dict[str, Tag]]:
    # we are only interested in bugs with a reproduction
    data = [row for row in data if get_tag_string(row['repro']) == 'C']
    
    # look for bugs that look exploitable
    # for now that just means is KASAN in the title, since that is typically memory unsafety issue
    data = [row for row in data if 'KASAN' in get_tag_string(row['title'])]
    return data

def bug_id_from_url(url_path: str) -> str:
    return url_path.split('=')[1].strip()

def download_bug_metadata(bug: dict[str, Tag]) -> Optional[BugMetadata]:
    url = bug['title'].a['href']
    bug_page = get_syzkall_html(url)
    table = parse_table(find_table_by_name(bug_page, 'crashes'))

    for c_repro_row in table:
        if get_tag_string(c_repro_row['c_repro']) != 'C':
            continue

        # asset paths are full url to google cloud bucket, not relative path like c_repro_path
        assets = c_repro_row['assets']

        disk_image_str = assets.find(string='disk image')
        disk_image_non_bootable = assets.find(string='disk image (non-bootable)')

        # use non bootable image if bootable one does not exist
        disk_image_is_bootable = True
        if disk_image_str is None and disk_image_non_bootable is not None:
            disk_image_str = disk_image_non_bootable
            disk_image_is_bootable = False

        vmlinux_str = assets.find(string='vmlinux')
        kernel_image_str = assets.find(string='kernel image')

        # assets are missing on older bug reports
        disk_image_path = None if disk_image_str is None else disk_image_str.parent['href']
        vmlinux_path = None if vmlinux_str is None else vmlinux_str.parent['href']
        kernel_image_path = None if kernel_image_str is None else kernel_image_str.parent['href']
        
        # title contains subsystems, without false positives
        subsystems = [get_tag_string(span) for span in bug['title'].find_all(name='span', attrs={'class': 'bug-label'})]

        syz_repro_a = c_repro_row['syz_repro'].find(name='a', string='syz')
        if syz_repro_a is None:
            continue

        c_repro_path = path_to_syzkall_url(c_repro_row['c_repro'].a['href'])

        # very rarely kernel commit is not an anchor and doesn't have url, so ignore
        kernel_commit_a = c_repro_row['commit'].a
        kernel_commit_url = None if kernel_commit_a is None else kernel_commit_a['href']

        # also check if config isn't proper link
        kernel_config_a = c_repro_row['config'].a
        kernel_config_url = None if kernel_config_a is None else path_to_syzkall_url(kernel_config_a['href'])

        crash_report = get_tag_string(bug_page.pre)

        crash_time_string = get_tag_string(c_repro_row['time'])
        crash_time = datetime.strptime(crash_time_string, '%Y/%m/%d %H:%M')

        return BugMetadata(
            bug_id=bug_id_from_url(url),
            description=get_tag_string(c_repro_row['title']),
            subsystems=subsystems,
            crash_time=crash_time,
            kernel_name=get_tag_string(c_repro_row['kernel']),
            kernel_url=kernel_commit_url,
            kernel_config_url=kernel_config_url,
            crash_report=crash_report,
            syz_repro_url=path_to_syzkall_url(syz_repro_a['href']),
            c_repro_url=c_repro_path,
            disk_image_url=disk_image_path,
            disk_image_is_bootable=disk_image_is_bootable,
            kernel_image_url=kernel_image_path,
            vmlinux_url=vmlinux_path,
        )
    
    return None

# kernel_name is upstream for default linux
# can also be android
def get_bugs_for_kernel(kernel_name: str) -> tuple[list[dict[str, Tag]], list[dict[str, Tag]]]:
    open_bugs_table = find_table_by_name(get_syzkall_html(kernel_name), 'open')
    fixed_bugs_table = get_syzkall_html(f'{kernel_name}/fixed').find_all(name='table')[1]

    bugs_open = filter_bugs(parse_table(open_bugs_table))
    bugs_fixed = filter_bugs(parse_table(fixed_bugs_table))

    return bugs_open, bugs_fixed

def pull_bugs(db: SyzkallBugDatabase, kernel_name: str):
    bugs_open, bugs_fixed = get_bugs_for_kernel(kernel_name)
    bugs_combined = bugs_open + bugs_fixed

    for bug in bugs_combined:
        print('Pulling:')
        print(get_tag_string(bug['title']).replace('\n', ' '))

        url = bug['title'].a['href']
        id = bug_id_from_url(url)
        if db.get_bug_metadata(id) is not None:
            print('already downloaded')
            continue

        bug_metadata = download_bug_metadata(bug)
        time.sleep(3)
        if bug_metadata is None:
            print('Failed to parse')
            continue
        db.save_bug_metadata(bug_metadata)