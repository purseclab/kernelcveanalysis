from pathlib import Path
from typing import Optional
from enum import Enum
from dataclasses import dataclass
import os

from openai import OpenAI
from pydantic import BaseModel

from .kernel_image import KernelImage, Kernel
from .parse import Literal, Token, LexedCode, ReplaceResult, ExploitFileGroup, DiffMode, is_annotated
from .tui import console
from .annotations import RopAddress, KernelAddress, SrcMetadata

SYSTEM_PROMPT = '''
You are a model which analyses exploits for the linux kernel. These exploits depend on addresses, offsets, and struct field offsets in the linux kernel to work.
You should find all these addresses, and output JSON data containing a list of these addresses, offsets, and struct field offsets?

Addresses are 64 bit absolute virtual kernel addresses, typically with many leading 0xffffff bits, and have a type of `address`.
Offsets are offsets relative to the base of the kernel executable, and have a type of `offset`.
Struct field offsets are the offset from the start of a struct to a field in the struct. They are typically much smaller then offsets relative to the kernel base, typically less than 2048.
Struct field offsets have a type of `struct_offset`.

Report addresses and offsets exactly as they appear in the code. Don't try to compute offsets from the addresses present in the code, just report the address constants in the JSON data.

For example, for the given input code:
File main.c:
```
#define A 0xffffffc010020f58
#define B_OFFSET 0x120

int main() {
    size_t c = 0xffffffc092004cd0;
    size_t d_offset = 0x34980;
    size_t z_offset = A - 0xffffffc088084000;
}
```
File exploit.h:
```
#define M_PTR 0x68
#define TIMERFD_FOPS_OFFSET 0x149828
```

The corresponding JSON data is:
```
[
    {
        "file": "main.c",
        "type": "address",
        "value": "0xffffffc010020f58"
    },
    {
        "file": "main.c",
        "type": "struct_offset",
        "value": "0x120"
    },
    {
        "file": "main.c",
        "type": "address",
        "value": "0xffffffc092004cd0"
    },
    {
        "file": "main.c",
        "type": "offset",
        "value": "0x34980"
    },
    {
        "file": "main.c",
        "type": "address",
        "value": "0xffffffc088084000"
    },
    {
        "file": "exploit.h",
        "type": "struct_offset",
        "value": "0x68"
    },
    {
        "file": "exploit.h",
        "type": "offset",
        "value": "0x149828"
    }
]
```

Be careful not to report integers constants which are used for other purposes (loop iteration, syscall arguments, etc.) as kernel addresses or offsets.

You will be given C code from several files, and should output only the JSON data.
'''
# MODEL = 'gpt-4.1-mini'
MODEL = 'gpt-5-mini'

class InitialAnnotationType(Enum):
    NONE = 0
    ADDRESS = 1
    OFFSET = 2
    VALUE = 3

class AnnotationCanidate:
    '''
    Scores how likely an integer is to be a specific type of annotation
    '''

    scores: list[float]

    def __init__(self):
        self.scores = [0.0] * 4
    
    def set_score(self, annotation: InitialAnnotationType, score: float):
        self.scores[annotation.value] = score
    
    def get_annotation_type(self) -> InitialAnnotationType:
        max_score = 0.0
        annotation_type = InitialAnnotationType.NONE

        for i, score in enumerate(self.scores):
            if score > max_score:
                max_score = score
                annotation_type = InitialAnnotationType(i)
        
        return annotation_type

class AnnotateContext:
    kernel_image: KernelImage
    files: ExploitFileGroup

    def __init__(self, files: list[Path], kernel_name: Kernel):
        # mapping from filename to file lexed file content
        self.files = ExploitFileGroup(files)
        self.kernel_image = KernelImage(kernel_name)

class KernelOffset(BaseModel):
    file: str
    # either `address`, `offset`, or `struct_offset`
    type: str
    value: str

class AnnotationData(BaseModel):
    data: list[KernelOffset]

def get_chatgpt_completions(context: AnnotateContext) -> dict[tuple[str, int], InitialAnnotationType]:
    prompt = ''
    for file in context.files.lexed_code():
        prompt += f'File: {os.path.basename(file.filename)}\n`{file.code}`\n'
    
    # TODO: use json schema
    response = OpenAI().responses.parse(
        model=MODEL,
        instructions=SYSTEM_PROMPT,
        input=prompt,
        text_format=AnnotationData,
    )

    # make type checker happy
    assert response.output_parsed is not None

    annotation_map = {}
    for offset in response.output_parsed.data:
        try:
            n = int(offset.value, 0)
        except:
            # only consider things chatgpt reports which are just integer constants for now
            continue

        match offset.type:
            case 'address':
                annotation = InitialAnnotationType.ADDRESS
            case 'offset':
                annotation = InitialAnnotationType.OFFSET
            case 'struct_offset':
                # ignore struct offsets for now
                continue
                # annotation = KEXPLOIT_STRUCT_OFFSET
            case _:
                # if invalid type ignore
                print(f'Warning: invalid offset type received from LLM: {offset.type}')
                continue

        annotation_map[(offset.file, n)] = annotation
    
    return annotation_map

MAX_SCORE = 1.0

def lerp(min_x: float, max_x: float, min_y: float, max_y: float, x: float):
    x = max(min_x, min(x, max_x))
    return ((max_y - min_y) / (max_x - min_x)) * (x - min_x) + min_y

def score_kernel_address(context: AnnotateContext, n: int) -> float:
    if context.kernel_image.is_kernel_address(n):
        base_score = 0.85

        previous_symbol = context.kernel_image.find_previous_symbol_absolute(n)
        if previous_symbol is None:
            return base_score
        
        score_loss = lerp(0, 0x40, 0.0, MAX_SCORE - base_score, n - previous_symbol.address)
        
        return MAX_SCORE - score_loss
    else:
        return 0.0

def score_kernel_offset(context: AnnotateContext, n: int) -> float:
    if context.kernel_image.is_kernel_offset(n):
        base_score = 0.1
        away_from_0_score = lerp(0, 0x8000, 0.0, 0.3, n)
        hex_digits_score = lerp(0, 4, 0.0, 0.1, len(set(hex(n)[2:])))
        decimal_digits_score = lerp(0, 4, 0.0, 0.1, len(set(str(n))))

        close_to_symbol_score = 0.0
        previous_symbol = context.kernel_image.find_previous_symbol_absolute(n)
        if previous_symbol is not None:
            close_to_symbol_score = lerp(0, 0x10, 0.4, 0.0, n - previous_symbol.address)
        
        return base_score + away_from_0_score + hex_digits_score + decimal_digits_score + close_to_symbol_score
    else:
        return 0.0

def get_annotation_type_for_integer(context: AnnotateContext, n: int) -> InitialAnnotationType:
    score = AnnotationCanidate()

    # threshhold that needs to be cleared to have a type for an annotation
    score.set_score(InitialAnnotationType.NONE, 0.7)
    score.set_score(InitialAnnotationType.ADDRESS, score_kernel_address(context, n))
    score.set_score(InitialAnnotationType.OFFSET, score_kernel_address(context, n))

    return score.get_annotation_type()

def get_non_chatgpt_annotations(context: AnnotateContext) -> dict[tuple[str, int], InitialAnnotationType]:
    annotation_map = {}

    for file in context.files.lexed_code():
        for token in file.tokens:
            match token.data:
                case Literal(int(n)):
                    annotation_type = get_annotation_type_for_integer(context, n)
                    if annotation_type != InitialAnnotationType.NONE:
                        annotation_map[(os.path.basename(file.filename), n)] = annotation_type
    
    return annotation_map

def annotate(exploit_files: list[Path], kernel: Kernel, llm_annotate: bool, diff_output: Optional[Path], diff_mode: DiffMode):
    context = AnnotateContext(exploit_files, kernel)
    metadata = SrcMetadata(
        original_kernel_name=kernel.name,
        current_kernel_name=kernel.name,
    )
    
    if llm_annotate:
        annotation_map = get_chatgpt_completions(context)
    else:
        annotation_map = get_non_chatgpt_annotations(context)

    def rewrite_file(lexed_file: LexedCode) -> Optional[str]:
        if lexed_file.is_annotated():
            console.error(f'{lexed_file.filename} is already annotated')
            return None
        
        def do_annotate(tokens: list[Token], index: int) -> ReplaceResult:
            match tokens[index].data:
                case Literal(int(value)):
                    # don't annotate if already annotated
                    if is_annotated(tokens, index):
                        return ReplaceResult.skip()

                    annotation_type = annotation_map.get((os.path.basename(lexed_file.filename), value))
                    if annotation_type is None:
                        return ReplaceResult.skip()
                    
                    is_relative = context.kernel_image.is_kernel_offset(value)
                    is_valid = is_relative or context.kernel_image.is_kernel_address(value)
                    
                    # we don't care what chatgpt says for address or offset, just do it based on kernel image
                    if (annotation_type == InitialAnnotationType.ADDRESS or annotation_type == InitialAnnotationType.OFFSET) and not is_valid:
                        # value is not address or offset
                        return ReplaceResult.skip()
                    
                    rop_gadget = None
                    if context.kernel_image.arch_info.rop_translation_supported():
                        if is_relative:
                            rop_gadget = context.kernel_image.get_rop_chain_instructions_offset(value)
                        else:
                            rop_gadget = context.kernel_image.get_rop_chain_instructions(value)
                        
                    if rop_gadget is not None:
                        annotation = RopAddress(gadget=rop_gadget, is_relative=is_relative, original_value=value)
                    else:
                        annotation = KernelAddress(address=value, is_relative=is_relative)
                    
                    return ReplaceResult.replace(1, str(annotation))

            return ReplaceResult.skip()
        
        annotated_code, errors = lexed_file.replace_tokens(do_annotate)

        annotated_code = f'{metadata}\n{annotated_code}'
        console.print_errors(errors)

        return annotated_code
    
    context.files.rewrite_files(rewrite_file, diff_output, diff_mode)